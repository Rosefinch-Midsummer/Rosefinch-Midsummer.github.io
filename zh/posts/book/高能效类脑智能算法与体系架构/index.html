<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>《高能效类脑智能：算法与体系架构》 | 天漢帝國復興錄</title><meta name=keywords content="类脑智能"><meta name=description content="《高能效类脑智能：算法与体系架构》
概论
前言
本书非常推荐阅读，虽然这个领域看起来还是个天坑。
书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。"><meta name=author content="RM"><link rel=canonical href=https://rosefinch-midsummer.github.io/zh/posts/book/%E9%AB%98%E8%83%BD%E6%95%88%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://rosefinch-midsummer.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rosefinch-midsummer.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rosefinch-midsummer.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://rosefinch-midsummer.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://rosefinch-midsummer.github.io/img/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://rosefinch-midsummer.github.io/zh/posts/book/%E9%AB%98%E8%83%BD%E6%95%88%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://rosefinch-midsummer.github.io/zh/posts/book/%E9%AB%98%E8%83%BD%E6%95%88%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"><meta property="og:site_name" content="天漢帝國復興錄"><meta property="og:title" content="《高能效类脑智能：算法与体系架构》"><meta property="og:description" content="《高能效类脑智能：算法与体系架构》 概论 前言 本书非常推荐阅读，虽然这个领域看起来还是个天坑。
书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。"><meta property="og:locale" content="zh-cn#en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-06T18:34:25+08:00"><meta property="article:modified_time" content="2025-12-06T22:54:22+08:00"><meta property="article:tag" content="类脑智能"><meta name=twitter:card content="summary"><meta name=twitter:title content="《高能效类脑智能：算法与体系架构》"><meta name=twitter:description content="《高能效类脑智能：算法与体系架构》
概论
前言
本书非常推荐阅读，虽然这个领域看起来还是个天坑。
书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"📚文章","item":"https://rosefinch-midsummer.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"📕閱讀","item":"https://rosefinch-midsummer.github.io/zh/posts/book/"},{"@type":"ListItem","position":3,"name":"《高能效类脑智能：算法与体系架构》","item":"https://rosefinch-midsummer.github.io/zh/posts/book/%E9%AB%98%E8%83%BD%E6%95%88%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"《高能效类脑智能：算法与体系架构》","name":"《高能效类脑智能：算法与体系架构》","description":"《高能效类脑智能：算法与体系架构》 概论 前言 本书非常推荐阅读，虽然这个领域看起来还是个天坑。\n书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。\n","keywords":["类脑智能"],"articleBody":"《高能效类脑智能：算法与体系架构》 概论 前言 本书非常推荐阅读，虽然这个领域看起来还是个天坑。\n书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。\n书籍简介 原作名: Learning in Energy-Efficient Neuromorphic Computing: Algorithm and Architecture Co-Design\nISBN：978-7-111-68299-8\n版印次：1-1\n作者：[中]郑楠(Nan Zheng),[美]皮纳基·马祖姆德(Pinaki Mazumder)\n出版时间：2016-01-08\n定价：99\n内容简介 本书主要关注如何构建高能效具有学习能力的脉冲型神经元网络硬件，并且提供建立具有学习能力的脉冲型神经元网络硬件协同设计、协同优化方法。完整地描述从高级算法到底层硬件实现的细节。本书同样涵盖了脉冲型神经元网络中的许多基础知识和关键点。\n本书从对脉冲型神经元网络的概述开始，讨论基于速率的人工神经网络的应用和训练，介绍实现神经网络的多种方法，如通用处理器和专用硬件，数字加速器和模拟加速器。同时展示了一个为能适应神经网络动态编程而建立的高能效加速器，验证脉冲神经网络的基础概念和流行的学习算法，简介脉冲神经网络硬件。后面的章节为读者介绍三个实现前述章节学习算法的设计案例（两个基于传统CMOS工艺，一个基于新兴的纳米工艺）。本书的结尾对脉冲型神经元网络硬件进行总结与展望。\n作者简介 郑楠 （Nan Zheng） 2011年本科毕业于上海交通大学信息工程专业，2014年和2018年分别获得美国密歇根大学电气工程硕士和博士学位。他目前是NVIDIA高级深度学习架构师，研究兴趣侧重于机器学习应用的低能耗硬件架构、算法和电路技术。\n皮纳基·马祖姆德 （Pinaki Mazumder） 美国密歇根大学电气工程与计算机科学系教授，他的研究兴趣包括对于量子MOS、自旋电子学、欺骗等离子体、共振隧穿器件等新兴技术的CMOS超大规模集成电路设计、半导体存储系统、CAD工具和电路设计。 译者简介：\n刘佩林 上海交通大学电子信息与电气工程学院教授，博士生导师。研究领域包括音频、视频、3D信号处理与智能分析，面向机器人的环境感知、人机交互、定位与导航，以及类脑计算与低功耗电路设计等。2017年起任上海交通大学类脑智能应用技术研究中心主任。\n应忍冬 上海交通大学电子信息与电气工程学院副教授，硕士生导师。研究领域包括嵌式系统、数字信号处理及VLSI实现架构、人工智能领域的机器思维原理和实现。\n薛建伟 上海交通大学电子信息与电气工程学院博士研究生。研究领域包括类脑智能、片上多核系统等。\n正文摘录 译者序 本书重点讨论如何为具有学习能力的神经网络构建节能硬件，致力于构建具有学习与执行各种任务的能力的硬件神经网络，提供协同设计和协同优化方法，并提供了从高层算法到底层实现细节的完整视图。开发硬件友好算法的目的是简化硬件实现，而特殊的硬件体系结构的提出则是为了更好地利用算法的独特功能。在本书的各章中，讨论了用于节能型神经网络加速器的算法和硬件体系结构。低功耗对于所有将功耗作为重要考虑因素的应用而言至关重要，使用耗电的GPU和将原始数据发送到可以进一步分析数据的云计算机都不是可行的选择。\n前言 简而言之，本书目前的版本有以下几个显著特点：\n包括神经形态算法硬件加速器的多层次全面评述。 涵盖架构与算法的协同设计，并采用新兴器件来极大地提升计算效率。 关注算法与硬件的协同设计，这是在神经形态计算中应用新兴器件（如传统忆阻器和扩散型忆阻器）的关键。 致谢 第1章 概述 1.1 神经网络的历史 1.2 软件中的神经网络 1.2.1 人工神经网络 1.2.2 脉冲神经网络 1.3 神经形态硬件的需求 1.4 本书的目标和大纲 参考文献 第2章 人工神经网络的基础与学习 2.1 人工神经网络的工作原理 2.1.1 推理 2.1.2 学习 2.2 基于神经网络的机器学习 2.2.1 监督学习 2.2.2 强化学习 2.2.3 无监督学习 2.2.4 案例研究：基于动作的启发式动态规划 贝尔曼方程\n演员——评论家网络\n在线学习算法\n虚拟更新技术\nGPT5-mini的解释如下所示：\n贝尔曼方程：刻画最优价值（或动作价值）与子问题之间递归关系的方程，表示某状态的价值等于该状态下采取最优行动带来的即时奖励加上按转移概率到达的下一状态折扣后续价值的期望。是动态规划与强化学习求解最优策略的基础。\n演员——评论家网络（Actor–Critic）：由两部分组成的强化学习结构：演员（Actor）输出策略参数化的动作分布，评论家（Critic）估计状态值或动作值用于评价策略；评论家提供梯度或优势估计以指导演员更新，兼顾策略学习的稳定性与样本效率。\n在线学习算法：数据逐条或小批次到达时即时更新模型的算法范式，要求逐步、低延迟地调整参数以适应流动数据或非稳环境，通常强调单次样本复杂度低、无须存储全量历史且能处理概念漂移。\n虚拟更新技术：在不实际改变环境或参数的情况下模拟、计算若干更新步骤的影响（如用模型或估计器推演未来梯度/价值变化），以评估或加速决策与学习过程，常用于样本效率提升、批内并行化或避免昂贵真实交互。\n2.3 网络拓扑 2.3.1 全连接神经网络 2.3.2 卷积神经网络 2.3.3 循环神经网络 2.4 数据集和基准 2.5 深度学习 2.5.1 前深度学习时代 2.5.2 深度学习的崛起 2.5.3 深度学习技术 性能提升技术：\n无监督预训练 丢弃（dropout） 批归一化 加速随机梯度下降过程 节能技术： 冗余去除 精度降低 2.5.4 深度神经网络示例 参考文献 第3章 硬件中的人工神经网络 学犹不及，犹恐失之。——孔子\n3.1 概述 神经网络的发展是为了模拟生物大脑的显著特征，比如它们具有模式识别和在有噪声的情况下探测运动的能力。第2章讨论了许多与人工智能神经网络（ANN）学习和推理相关的基本概念。作为一种算法，神经网络需要在特定的硬件平台上执行，然后才能部署到各种应用中。本章讨论在不同硬件平台上实现的神经网络，考虑不同平台的优缺点。一般来说，有三种类型的硬件平台可以部署神经网络算法：通用处理器、现场可编程门阵列（FPGA）和专用集成电路（ASIC）。\n3.2 通用处理器 3.3 数字加速器 3.3.1 数字ASIC实现方法 3.3.2 FPGA加速器 3.4 模拟/混合信号加速器 3.4.1 传统集成技术中的神经网络 3.4.2 基于新兴非易失性存储器的神经网络 3.4.3 光学加速器 3.5 案例研究：一种节能的自适应动态规划加速器的程序设计 3.5.1 硬件架构 3.5.2 设计示例 参考文献 第4章 脉冲神经网络的工作原理与学习 4.1 脉冲神经网络 脉冲神经网络（SNN）的灵感来自生物神经网络。对SNN的研究可以追溯到人工神经网络（ANN）广为人知很久以前。最初关于SNN的研究是为了模拟生物神经网络。近年来，在这些领域的先驱如Gerstner、Maass等的带领下，以计算为目的的使用SNN变得越来越热门。虽然ANN的早期发展受到了SNN的启发，但是SNN和ANN有很大的不同。我们在第2章和第3章中分别讨论了以下几个方面：\n（1）SNN和ANN中信息的编码方式不同。非脉冲神经元利用实数值激活来传信息，而脉冲神经元用脉冲来表示信息。\n（2）神经网络中的非脉冲神经元没有任何记忆，但脉冲神经元通常有记忆。\n（3）许多ANN（尤其是前馈ANN）产生的输出不是时间的函数，但是大多数SNN本质上是随时间变化的。\n4.1.1 常见的脉冲神经元模型 Hodgkin-Huxley模型\nLeaky Integrate-and-Fire 模型\nIzhikevich模型\n4.1.2 信息编码 4.1.3 脉冲神经元与非脉冲神经元的比较 4.2 浅层SNN的学习 4.2.1 ReSuMe 4.2.2 Tempotron 4.2.3 脉冲时间相关可塑性 4.2.4 双层神经网络中通过调制权重依赖的STDP进行学习的方法 用脉冲时间估计梯度\n4.3 深度SNN学习 4.3.1 SpikeProp 4.3.2 浅层网络栈 4.3.3 ANN的转换 4.3.4 深度SNN反向传播的研究进展 4.3.5 在多层神经网络中通过调制权重依赖的STDP进行学习的方法 参考文献 第5章 脉冲神经网络的硬件实现 5.1 对专用硬件的需求 5.1.1 地址事件表示 5.1.2 事件驱动计算 5.1.3 渐进精度推理 5.1.4 实现权重依赖的STDP学习规则的硬件注意事项 5.2 数字脉冲神经网络 5.2.1 大规模脉冲神经网络专用集成电路 5.2.2 中小型数字脉冲神经网络 5.2.3 脉冲神经网络中的硬件友好型强化学习 5.2.4 多层脉冲神经网络中的硬件友好型监督学习 5.3 模拟/混合信号脉冲神经网络 5.3.1 基本构建块 5.3.2 大规模模拟/混合信号CMOS脉冲神经网络 5.3.3 其他模拟/混合信号CMOS脉冲神经网络专用集成电路 5.3.4 基于新兴纳米技术的脉冲神经网络 5.3.5 案例研究：脉冲神经网络中基于忆阻器交叉开关的学习 参考文献 第6章 总结 6.1 展望 6.1.1 脑启发式计算 第4章中讨论的Hebbian规则和脉冲时间相关可塑性（STDP）是两个广为人知的生物学现象，长期以来人们一直认为这是大脑学习的基本机制。与此相反，长期以来人们一直批评AI界广泛采用的基于反向传播的梯度下降学习算法在生物学上是不可行的。反向传播起源于数学模型，它是解决人工神经网络（ANN）优化问题的一种高效的方法，它的诞生与生物学神经网络的学习几乎没有关系。尽管长期以来一直在争论，类脑的计算并不一定要精确地模仿生物脑的行为，但Hinton指出，STDP协议可能是在脑中进行梯度下降优化和反向传播的一种方式。这个假设在AI界和神经科学界得到了进一步发展。\n6.1.2 新兴的纳米技术 6.1.3 神经形态系统的可靠计算 6.1.4 人工神经网络和脉冲神经网络的融合 6.2 结论 参考文献 附录 术语表 附录 新闻报道中科院院士张旭：类脑智能是智能时代的新质生产力 2024-03-20 10:46:15 来源：中国证券网 作者：谭镕 记者 邓贞\n上证报中国证券网讯（谭镕 记者 邓贞）“随着智能计算系统趋近算力天花板、能耗激增、通用智能难产，人脑天然具备的低能耗、并行处理、学习性和容错性强等优势愈加凸显。”在3月19日中国金融信息中心举办的以“生成AI 重塑未来”为主题的专精特新高质量发展大讲堂上，中科院院士、广东省智能科学与技术研究院（以下简称“广东省智能院”）院长张旭分享了类脑智能的技术突破和应用图景。\n图：中科院院士、广东省智能科学与技术研究院院长张旭\n他认为，在一系列研究技术促进下，人类大脑功能联结图谱终将被成功绘制，脑机接口、类脑智能理论和类脑智能技术在未来数年内将成为脑科学和脑医学研究和拓展的重要方向。\n人工智能是新一轮科技革命和产业变革的核心驱动力，发展新质生产力的重要引擎。当前，ChatGPT、Sora等生成式AI技术不断突破，使得大模型赋能千行百业成为可能。\n“过去几年，智能算力需求快速增长。目前，算力产品的使用出现了较大的瓶颈——能耗问题。”张旭表示，人脑有860亿神经元形成神经环路和网络，能耗约20瓦，而相同大小的人工神经网络数字模拟的能耗约8兆瓦。传统计算机的信号-数据转换和高精度计算在能源和时间上产生高成本。复杂的深度学习模型耗费惊人的高训练成本，需要有替代方法。\n“不等同传统的人工智能，也不等同脑仿生，类脑智能是脑科学、神经科学启发的智能科学和技术。”张旭表示，类脑智能采用神经形态计算，通过模仿人类大脑的运作方式，让计算机软硬件实现信息高效处理，同时具有低功耗、高算力的特点。\n张旭介绍，广东省智能院是国内外首家成建制、成体系、全链条研究和开发类脑智能的科研机构，主要开展涉及感知认知神经网络、类脑智能算法与模型、类脑智能计算的基础研究和核心技术研发，同时提供类脑异构融合智能计算为主体的先进智能计算平台。\n2023年底，由广东省智能院粤港澳脑智工程中心转化的珠海横琴新近纪智能科技公司和珠海天琴芯智能科技公司共同发布了以新一代类脑计算架构（LYRArc）和处理芯片（BPU）为技术核心的绿色类脑智能计算系统。据悉，“天琴芯海”单芯片可支持2亿神经元拟态计算，为全球首颗亿级神经元规模的可编程类脑晶圆计算芯片，助力未来类脑计算机系统真正实现“体积更小，容量更大”的集成突破。同时，该团队已成功研制出基于该晶圆芯片的类脑晶圆计算机，未来有望将助力脑科学研究和全脑千亿神经元尺度类脑大模型的研发。\n“算力特别是智能算力，是人类进入智能时代的生产力，对新兴技术发展具有强大助推力。用类脑计算技术，可以实现在一些技术上的突破，来弥补现有智能计算上的一些不足。”张旭说，类脑计算机系统的推出，为解决类脑智能产业化发展的底层核心技术问题提供了新路径，有望在助力大模型训练、脑仿真、工业计算模拟、社交网络分析、金融风控分析等智能产业应用，走出智能计算的“中国新路”。\n据介绍，2023年6月，科技部公布了国家新一代人工智能公共算力开放创新平台名单，由广东省智能院建设的“新一代类脑人工智能公共算力开放创新平台”入列。该创新平台主要依托横琴先进智能计算平台进行规划和建设，重点围绕人工智能、大数据、区块链、高算力芯片、生物医药、金融工商、元宇宙等智能产业，形成类脑智能产业生态圈。\n“类脑正成为脑科学一种新的范式，借鉴脑处理的信息和学习的基本原理，发展高效能、高速、智能的新型类脑计算系统。与传统的AI相比，类脑智能计算的速度快、能耗小，逻辑分析推理能力更强，计算机体积更小，而且有可能形成对计算机架构、智能芯片、智能计算机等多方面颠覆性的创新。”张旭表示，在不久的将来，可以期待类脑智能超级计算机的算力超过人类大脑的算力，或给人类社会带来重大变革。\n本期讲堂由中国中小企业发展促进中心、上海市经济和信息化委员会指导，上海市中小企业发展服务中心、新华社中国金融信息中心主办，宁波银行上海分行、上海股权托管交易中心联合主办，上海市人工智能技术协会协办、上海市企业服务云特别支持。\n知乎文章Nature 长文综述：类脑智能与脉冲神经网络前沿 导语 在人工智能如火如荼的今天，基于人脑的“脉冲”（spiking）模拟计算框架下的脉冲神经网络（SNN）、神经形态计算（neuromorphic computing）有望在实现人工智能的同时，降低计算平台的能耗。这一跨学科领域以硅电路实现生物中的神经环路（circuit）为起点，现已发展到包括基于脉冲的编码以及事件驱动表示的算法的硬件实现。\n2019 年 11 月 28 日，普渡大学的 Kaushik Roy、 Akhilesh Jaiswal 和 Priyadarshini Panda 在 Nature 发表长文综述，概述了神经形态计算在算法和硬件方面的发展，介绍了学习和硬件框架的原理。以及神经形态计算的主要挑战以及发展前景，算法和硬件的协同设计等方面的内容。本文是全文翻译。\n编译：集智俱乐部翻译组\n来源：Nature\n原题：Towards spike-based machine intelligence with neuromorphic computing\n前言 纵观历史，创造具有类人脑能力的技术一直都是创新的源泉。从前，科学家们一直以为人脑中的信息是通过不同的通道(channels)和频率传递的，就像无线电一样。如今，科学家们认为人脑就像一台计算机。随着神经网络的发展，今天的计算机已在多个认知任务中展现出了非凡的能力，例如，AlphaGo在围棋战略游戏Go中击败了人类选手。虽然这种表现的确令人印象深刻，但一个关键问题仍然存在：这些活动涉及的计算成本有多大？\n人脑能够执行惊人的任务（例如，同时识别多个目标、推理、控制和移动），而能量消耗只有接近2瓦左右。相比之下，标准计算机仅识别1000种不同的物体就需要消耗250瓦的能量。尽管人脑尚未被探索穷尽，但从神经科学来看，人脑非凡的能力可归结于以下三个基本观察：广泛的连通性、结构和功能化的组织层次、以及时间依赖（time dependent）的神经元突触连接（图1a）。\n神经元（Neurons）是人脑的计算原始元素，它通过离散动作电位（discrete action potentials）或“脉冲”交换和传递信息。突触（synapses）是记忆和学习的基本存储元素。人脑拥有数十亿个神经元网络，通过数万亿个突触相互连接。基于脉冲的时间处理机制使得稀疏而有效的信息在人脑中传递。研究还表明，灵长类动物的视觉系统由分层级的关联区域组成，这些关联区域逐渐将视觉对象的映像转化为一种具有鲁棒性的格式，从而促进了感知能力。\n目前，最先进的人工智能总体上使用的是这种受到人脑层次结构和神经突触框架启发的神经网络。实际上，现代深度学习网络（DLNs）本质上是层级结构的人造物，就像人脑一样用多个层级去表征潜在特征，由来自输入过程中多个图层的不同潜在特征的表征，经过转换形成的（图1b）。硅晶体管硬件计算系统是这种神经网络的硬件基本。大规模计算平台的数字逻辑包含由集成在单个硅芯片上的数十亿个晶体管。这让人联想到了人脑的层级结构：各种硅基计算单元以层级方式排列，以实现高效的数据交换（图1c）。\n尽管两者在表面上有相似之处，但人脑的计算原理和硅基计算机之间存在着鲜明区别。其中包括：（1）计算机中计算（处理单元）和存储（存储单元）是分离的，不同于人脑中计算（神经元）和存储（突触）是一体的；（2）受限于二维连接的计算机硬件，人脑中大量存在的三维连通性目前无法在硅基技术上进行模拟；（3）晶体管主要为了构建确定性布尔（数字）电路开关，和人脑基于脉冲的事件驱动型随机计算不同。尽管如此，在当前的深度学习革命中，硅基计算平台（例如图像处理单元（GPU）云服务器）已成为一个重要的贡献因素。\n但是，使得“通用智能”（包括基于云服务器到边缘设备）无法实现的主要瓶颈是巨大的能耗和吞吐量需求。例如，在一个由典型的2.1Wh电池供能的嵌入式智能玻璃处理器（smart-glass processor）上运行深度网络，就会让处理器在25分钟内将电池消耗殆尽。\n在人脑的指引下，通过脉冲驱动通信从而实现了神经元-突触计算的硬件系统将可以实现节能型机器智能。神经形态计算始于20世纪80年代晶体管仿照神经元和突触的功能运作（图2），之后其迅速演化到包括事件驱动的计算本质（离散的“脉冲”人造物）。最终，在21世纪初期，这种研究努力促进了大规模神经形态芯片的出现。\n图1：生物和硅基计算的关键属性构架。a，大脑的组织原理示意图。神经元和突触与时间脉冲处理交织在一起的网络使得不同区域之间的信息能够快速高效地流动。b，一个深度卷积神经网络物体执行目标检测的图片。这些网络是多层的，并使用突触存储和神经元非线性学习广泛的数据表示。使用反向传播训练后，每层学习的特征都显示有趣的模式。第一层学习一般特征，如边缘和颜色斑点。随着我们深入网络，学习到的功能变得更具体，用对象的部分（如狗的眼睛或鼻子）代表完整的物体（如狗的脸）。这种从一般到特殊的过渡代表了视觉皮层的层次结构。c，最先进的硅计算生态系统。广义上讲，计算层次分为处理单元和内存存储。处理单元和内存层次结构的物理分离导致众所周知“内存墙瓶颈（memory well bottleneck）” 。当今的深度神经网络在强大的云服务器上训练，尽管会产生巨大的能耗，但仍可提供令人惊叹的精度。\n今天，算法设计师们正在积极探索（特别是“学习”）脉冲驱动型计算的优缺点，去推动有可扩展性、高能效的“脉冲神经网络”（spiking neural networks ，SNN）。在这种情况下，我们可以将神经形态计算领域描述为一种协同工作，它在硬件和算法域两者中权重相同，以实现脉冲型人工智能。我们首先强调了“智能”（算法）方面，包括不同的学习机制（无监督以及基于脉冲的监督，或梯度下降方案），同时突出显示了要利用基于时空事件的表征。本文讨论的重点是视觉相关的应用任务，例如图像识别和检测。然后我们将探索“计算”（硬件）方面，包括模拟计算、数字神经运动系统，它们都超越了冯·诺依曼（数字计算系统的最新架构）和芯片技术（代表了基本的场效应晶体管设备，它们是当下计算平台的基础）。最后，我们将讨论算法的硬件协同设计前景，说明算法具有用于对抗硬件漏洞的鲁棒性，可以实现能耗和精度之间的最佳平衡。\n一、算法展望：脉冲神经网络_ 脉冲神经网络 按照神经元功能，Maass开创性的论文将神经网络分为三个代际。首先，第一代被称为McCulloch–Pitt感知机，它执行阈值运算并输出数字（1、0）。基于sigmoid单元或修正线性单元（ReLU），第二代神经元单元增加了连续非线性，使其能够计算一组连续的输出值。第一代和第二代网络之间的非线性升级在扩展神经网络向复杂应用和更深度的实现方面起着关键作用。当前的DLNs在输入和输出之间具有多个隐藏层，都是基于第二代神经元。实际上，由于它们连续的神经元功能，这些模型可以支持基于梯度下降的反向传播学习，这也是目前训练深度神经网络的标准算法。\n图2：智能计算的重大发现和进展时间表（从1940年代到当代）。硬件方面，我们有从两个角度展示发现：一是对神经形态计算的启迪，或通过硬件创新实现类人脑的计算和“智能”；另一方面是对计算效率的启发，或者实现更快、更节能的布尔计算。从算法的角度来看，我们已指出这些发现是出于理解人脑的动机，受到神经科学和生物科学的驱动，并同时致力于实现人工智能，它由工程和应用科学所驱动。请注意，这张图并不是完整或全面的清单。“当前研究”并不一定意味着过去没有对这些努力进行探索；相反，我们强调了该领域正在进行和有希望研究的关键方面。\n第三代神经网络主要使用“整合放电”（integrate-and-fire）型尖峰神经元，通过脉冲交换信息（图3）。第二代和第三代神经网络之间最大的区别在于信息处理性质。第二代神经网络使用了实值计算(real-value)（例如，信号振幅），而SNN则使用信号的时间（脉冲）处理信息。脉冲本质上是二进制事件，它或是0或是1。如图3a所示，SNNs中的神经元单元只有在接收或发出尖峰信号时才处于活跃状态，因此它是事件驱动型的，因此可以使其节省能耗。若无事件发生SNNs单元则保持闲置状态，这与DLNs相反。无论实值输入和输出，DLNs所有单位都处于活跃状态。此外，SNN中的输入值为1或0，这也减少了数学上的点积运算ΣiVi×wi（图3a），减小了求和的计算量。\n针对不同的生物保真度水平下产生的脉冲代际，相关的尖峰神经元模型已被提出。例如泄漏整合放电型（LIF）（图3b）和霍奇金-赫克斯利型（Hodgkin–Huxley）。同样，针对于突触的可塑性，已有例如赫布型（Hebbian）和非赫布型]（non-Hebbian）方案。突触的可塑性即突触权重的调节（在SNNs中转化为学习）取决于突触前和突触后尖峰的相对时间（图3c）。神经形态工程师的一个主要目标是：在利用基于事件（使用基于事件的传感器）及数据驱动更新的同时，建立一个具有适当突触可塑性的脉冲神经元模型，从而实现高效的识别、推理等智能应用。\n我们认为，SNNs最大的优势在于其能够充分利用基于时空事件的信息。今天，我们有相当成熟的神经形态传感器，来记录环境实时的动态改变。这些动态感官数据可以与SNNs的时间处理能力相结合，以实现超低能耗的计算。实际上，与传统上DLNs使用的帧驱动（frame-driven）的方法相比，SNNs将时间作为附加的输入维度，以稀疏的方式记录了有价值的信息（图3），从而实现高效的SNNs框架，并通过计算视觉光流或立体视觉来实现深度感知。结合基于脉冲的学习规则，它可以产生有效的训练。机器人研究者已经证明使用基于事件的传感器进行跟踪和手势识别的优势。但是，这些应用程序大多数都使用了DLNs来执行识别。\n在此类传感器中使用SNNs主要受限于缺乏适当的训练算法，从而可以有效地利用尖峰神经元的时间信息。实际上就精度而言，在大多数学习任务中SNNs的效果仍落后于第二代的深度学习。很明显，尖峰神经元可以实现非连续的信息传递，并发出不可微分的离散脉冲（见图3），因此它们不能使用基于梯度下降型的反向传播技术，而这是传统神经网络训练的基础。\n另外，SNNs还受限于基于脉冲的数据可用性。虽然理想情况要求SNNs的输入是带有时间信息的序列，但SNNs训练算法的识别性能是在现有静态图像的数据集上进行评估的，例如CIFAR或ImageNet。然后，此类基于静态帧的数据将通过适当的编码技术（例如速率编码或次序编码，见图3d）转换为脉冲序列。虽然编码技术使我们能够评估SNNs在传统基准数据集上的性能，但我们要超越静态图像分类的任务。SNNs的最终能力应当来自于它们处理和感知瞬息万变的现实世界中的连续输入流，就像人脑轻而易举所做的那样。目前，我们既没有良好的基准数据集，也没有评估SNNs实际性能的指标。收集更多适当的基准数据集的研究，例如动态视觉传感器数据或驾驶和导航实例，便显得至关重要。\n（这里我们指的是作为DLNs的第二代连续神经网络，以区别于基于脉冲的计算。我们注意到SNNs可以在具有卷积层次结构的深度架构上实现，并同时执行尖峰神经元功能。）\n图3：SNN计算模型。a. 由输入上游神经元驱动的下游神经元组成的神经网络。上游神经尖峰Vi通过突触权重wi调节，在给点时间内产生合成电流ΣiVi×wi（相当于点积运算）。产生的合成电流会影响下游神经元的膜电位。B. LIF尖峰神经元的动力学显示。在没有脉冲的情况下，膜电位Vmem在时间常数τ中集成了传入脉冲和泄漏。当Vmem超过阈值Vthresh时，下游神经元输出脉冲。随之产生不应期，在此期间后神经元的Vmem不再受到影响。c，显示了基于实验数据的脉冲时间依赖的可塑性（STDP）公式，其中a +，a-，τ+和τ-是控制权重变化Δw的学习率和时间常数。突触权重wi根据上游神经元与下游神经元尖峰的时间差（Δt= tpost − tpre）更新。d，使用速率编码将输入图像（静态帧数据）转换为脉冲在各个时间步长上的映射。每个像素产生一个泊松脉冲序列，其激发速率与像素强度成正比。当几个时间步求和得出脉冲映射时（标记为t = 5的脉冲映射是从t = 1到t = 5的映射总和），它们开始类似于输入。因此，基于脉冲的编码既保留了输入图像的完整性，并且在时域中对数据进行了二值化。结果显示，LIF行为和随机输入尖峰的产生使SNN的内部动力学具有随机性。注意，序列编码也可以用来生成脉冲数据。\n二、在SNNs中学习算法 基于转换的方法 这种方法的思路是获得一个SNN，对给定的任务，该SNN将产生与深度神经网络相同的输入输出映射。它的基本原理是，使用权重调整（weight rescaling）和归一化方法将训练有素的DLN转换为SNN，将非线性连续输出神经元的特征和尖峰神经元的泄漏时间常数（leak time constants），不应期（refractory period）、膜阈值（membrane threshold）等功能相匹配。\n迄今为止，在图像分类的大型脉冲网络中（包括ImageNet数据集），这种方法能够产生了有竞争力的精确度。在基于转换的方法中，其优点是免除了时域中的训练负担。DLN使用了已有的深度学习框架例如Tensorflow对基于帧的数据进行训练，这些工具提供了训练中的灵活性。这种转换首先需要解析在基于事件的数据（通过对静态图像数据集进行速率编码获得）上进行训练的DLN，之后再进行简单的转换。\n但是，这种方法有其内在的局限性。例如在使用双曲线正切（tanh）或归一化指数函数（softmax）后，非线性神经元的输出值可以得正也可以得负，而脉冲神经元的速率只能是正值。因此，负值总被丢弃，导致转换后的SNNs的精度下降。转换的另一个问题是在不造成严重的性能损失的前提下获得每一层最佳。最近的研究提出了确定最佳放电率的实用解决方案，以及在DLNs的训练过程中引入其他约束（例如噪音或泄漏修正线性单元（leaky ReLUs））以更好地匹配尖脉冲神经元的放电率。今天，转换的方法可为图像识别任务提供最先进的精度，并与DLNs的分类性能相当。值得注意的是，从DLNs转换的SNNs的推理时间变得很长（约几千个时间步长），导致延迟增加、能耗增加。\n基于脉冲的方法 在基于脉冲的方法中，SNN使用时间信息进行训练，因此在整体脉冲动力学中具备明显的稀疏性和高效率优势。研究人员采用了两种主要方向：无监督学习（没有标记数据的训练），以及监督学习（有标记数据的训练）。早期监督学习成果是ReSuMe和tempotron，它们证明了在单层的SNN中，可以使用脉冲时间依赖的可塑性（STDP）的变体去进行分类。从那时起，研究工作一直致力于整合基于脉冲且类似于全局反向传播的误差梯度下降法，以便在多层SNNs中实现监督学习。大多数依赖反向传播的成果为脉冲神经元功能估计了一个近似可微的函数，从而使其能够执行梯度下降法（图4a）。SpikeProp及其相关变体已派生出通过在输出层固定一个目标脉冲序列来实现SNNs的反向传播规则。最近的成果对实值膜电位使用随机梯度下降法，是为了让正确输出神经元随机激发更多的脉冲（而不是具有精确目标的脉冲序列）。这些方法在深度卷积SNNs的小规模图像识别任务上取得了最新进展，例如美国国家标准与技术研究所（MNIST）手写数字数据库的数字分类。\n然而，尽管计算效率更高，监督学习在大型任务的精度上无法超过基于转换的方法。另一方面，受到神经科学和硬件效率为主要目标的启发，基于STDP学习规则的局部无监督SNN训练也很有意思。通过局部学习（我们将在后面的硬件讨论中看到），有机会使记忆（突触存储）和计算（神经元输出）更紧密地相结合。这种架构更像人脑，也适合节能芯片上实现。Diehl等人率先证明了完全无监督的SNN学习，其精度可与 MNIST数据库深度学习相媲美（图4b）。\n但是，将局部学习方法扩展到多层复杂任务是一个挑战。随着网络的深入，神经元的放电率会降低，我们称之为“消失的前向脉冲传播”。为了避免这种情况，多数工作用逐层的方式训练多层SNN（包括卷积SNNs）在局部的脉冲学习模式，然后进行全局学习反向传播学习，以去进行分类。这样局部和全局相结合的方法尽管很有成效，但在分类精度方面仍落后于转换的方法。此外，最近的成果显示了概念验证，即通过深度SNNs中反馈连接错误信号的随机投影确实有助于改善学习。这种基于反馈的学习方法需要进一步研究，以评估其在大规模任务上的效果。\n对二元制学习的启示 我们可以通过仅用二进制（1/0）位值，而不是需要额外的内存的16位或32位浮点值来获得超低能耗和高效的计算。实际上在算法层级，目前正在研究以概率方式学习（关于神经元何时随机突跳，权重的转换精度何时变低）获得参数较少的网络和计算操作。二元和三元的DLNs也被提出，其神经元输出和权重只取低精度值-1、0和+1，而且在大规模分类任务中表现良好。基于二进制脉冲处理模式，SNN已具有计算优势。此外，LIF神经元的神经元动力学中的随机性可以顾及外部噪声（例如，有噪声的输入或有噪声权重的硬件参数），来提高网络的鲁棒性。那么，我们是否可以用结合此SNN时间处理架构使用适当的学习方法，并将权重训练压缩为二进制，使精度损失最小，还有待研究。\n图4：尖峰网络中的全局和局部学习原理。a，在已知目标标签指导下进行全局学习，T用于分类任务。给定一个前馈网络，网络通过隐藏层单位A向前传播输入值X，并输出神经元激活值Y。结合非线性变换ƒ（Z1），使用输入的加权求和可以计算隐藏层的A，用矩阵符号表示为Z1 = W1 TX。输出以类似的方式进行计算。然后，用误差相对于权重（W1，W2）的导数E求出随后的权重更新。前向和反向传播的迭代导致学习。误差求导需要ƒ’，这要求ƒ’是连续的。因此，基于脉冲的反向传播的规则近似LIF函数可微方案。基于时间信息的处理细节没有显示在这里。b，局部STDP数字分类无监督学习。给定一个两层拓扑结构，输入层与输出层的所有神经元完全连接，通过STDP学习突触连接。根据输入层和输出层神经元的尖峰时间差异进行权重调整。输入神经元在输出之前（或之后）激发，权重值将增加（或减小）。随着迭代在多个时间步长上进行训练，权重在初始化时随机赋值，通过训练它将学习对一类输入所示的样式进行通用表示的编码（在这种情况下为“ 0”，“ 1”和“ 2”）。这里，为了进行识别，目标标签是不需要的。\n三、其他有待研究的方向 超越视觉任务 到目前为止，我们已经给出了大多数分类任务处理的办法，那么如何处理在静态图像上识别和推理以外的任务呢？SNN也可以处理序列数据，但是并没有研究论证SNN在处理NLP的能力。使用SNN做因果推断的能力又如何呢？深度学习研究员在强化学习领域作出了大量的研究[62,63]，不过使用SNN进行强化学习研究的却很少。在SNN这一领域——特别是在训练学习算法中——SNN所面临的最大挑战就是否能表现出和深度学习相当的性能。尽管深度学习已经设下了很高的竞争门槛，但是我们相信SNN会在机器人、自主控制等领域表现的更好。\n终身学习和小样本学习 深度学习模型在长期学习时会出现灾难性遗忘现象。比如，学习过任务A的神经网络在学习任务B时，它会忘记学过的任务A，只记得B。如何在动态的环境中像人一样具备长期学习的能力成为了学术界关注的热点。这固然是深度学习研究的一个新的方向，但我们应该探究给SNN增加额外的时间维度是否有助于实现持续性学习型任务。另一个类似的任务就是，利用少量数据进行学习，这也是SNN能超过深度学习的领域。SNN中的无监督学习可以与提供少量数据的监督学习相结合，只使用一小部分标记的训练数据得到高效的训练结果[ 46,50,65 ]。\n与神经科学建立联系 我们可以和神经科学的研究成果相结合，把这些抽象的结果应用到学习规则中，以此提高学习效率。例如，Masquelier等人[65]利用STDP和时间编码模拟视觉神经皮层，他们发现不同的神经元能学习到不同的特征，这一点类似于卷积层学到不同的特征。研究者把树突学习[66]和结构可塑性[67]结合起来，把树突的连接数做为一个超参数，以此为学习提供更多的可能。SNN领域的一项互补研究是LSM（liquid state machines）[68]。LSM利用的是未经训练、随机链接的递归网络框架，该网络对序列识别任务表现卓著[ 69–71]。但是在复杂的大规模任务上的表现能力仍然有待提高。\n四、硬件展望 从前文对信息处理能力和脉冲通信的描述中，我们容易假设一套具备类似能力的硬件系统。这套系统能够成为SNN的底层计算框架。受到在生物大脑中无处不在的神经元和突触的启发，设计出紧密结合在一起的计算和记忆结构；以及实现更复杂的功能——例如，使用最少的电路元件来模拟神经元与突触动力学。\n神经形态计算的出现 在20世纪80年代，晶体管发明了40年后，在生物神经系统领域，Carver Mead设想了\"更智能\"、“更高效”的硅基计算机结构[72,73]。他也表示过自己最初试图建立神经系统的尝试是“简单而愚蠢的”[74]。但是他的工作代表了计算硬件领域的一种新的范式。Mead并不在意AND、OR等布尔运算[74]。相反他利用金属氧化物硅（MOS）晶体管在亚阈值区的电气物理特性（电压-电流指数相关）来模拟指数神经元的动力学特征[72]。这样的设备-通路协同设计是神经形态计算中最有趣的领域之一。\n并行GPU的出现 和CPU这种由一个或者多个处理复杂任务的芯片组成的计算核心不同，GPU[75]是由多个可以进行并行计算的简单计算核心组成。因此能完成高并发、高吞吐的任务。在传统意义上GPU是加速图形应用程序的硬件加速器，但是现在有许多的非图形应用都受益于GPU的特性。深度学习就是一个显著的例子[6]，其实GPU不仅是深度神经网络的首选平台，也是SNN训练的平台[76,77]。虽然GPU在高扩展性上具备优势，但无法很好的用来进行基于事件驱动的脉冲计算。因此，事件驱动的“超级大脑”神经芯片就可以提供高效的解决方案[78,79]。\n“超级大脑”芯片 “超级大脑”芯片[80]的特点是整合了百万计的神经元和突触，神经元和突触提供了脉冲计算的能力[78,81–86]。Neurogrid[82]和TrueNorth[84]分别是基于混合信号模拟电路和数字电路的两种模型芯片。Neurogrid使用数字电路，因为模拟电路容易积累错误，且芯片制造过程中的错误影响也较大。设计神经网络旨在帮助科学家模拟大脑活动，通过复杂的神经元运作机制——比如离子通道的开启和关闭，以及突触特有的生物行为[82,87]。相比而言，TrueNorth作为一款神经芯片，目的是用于重要商业任务，例如使用SNN分类识别任务；而且TrueNorth是基于简化的神经科突触原型来设计的。\n以TrueNorth为例，主要特性如下[78,88]：\n异步地址事件表示（Asynchronous address event representation）：首先，异步地址事件表示不同于传统的芯片设计，在传统的芯片设计中，所有的计算都按照全局时钟进行，但是因为SNN是稀疏的，仅当脉冲产生时才要进行计算，所以异步事件驱动的计算模式更加适合进行脉冲计算[89,90]。\n芯片网络：**芯片网络（networks-on-chip，NOCs）**可以用于脉冲通信，NOC就是芯片上的路由器网络，通过时分复用技术用总线收发数据包。大规模芯片必须使用NOC，是因为在硅片加工的过程中，连接主要是二维的，在第三个维度灵活程度有限。也要注意到，尽管使用了NOC但芯片的联通程度，仍然不能和大脑中的三维连通性相比。包括TrueNorth在内的大规模数字神经芯片，比如Loihi[78]，已经展示除了SNN技术以外的应用效果。使得我们能更加接近生物仿真技术。不过，有限的连通性，NOC总线带宽的限制，和全数字方法仍然需要进一步的研究。\n超越冯·诺依曼式计算 晶体管尺寸规模的持续行缩小的现象被称之为摩尔定律[91]，摩尔定律推动了CPU和GPU以及“超级大脑”芯片的不断发展。不过近些年，随着硅基晶体管接近物理极限，这一发展速度放缓[92]。为了适应现代人类对计算能力的不断提升，研究者们设计出了一种双管齐下的方法，使得“超冯·诺依曼”、“超硅”计算模型成为了可能，冯·诺依曼模型的一大特征就是，存储单元和运算单元的分离[93]。通过系统总线传输数据。因此，数据在高速的运算单元和低速的存储单元之间的频繁传输就成为了众所周知的**“存储墙瓶颈”（memory wall bottleneck）**。这一瓶颈限制了计算的吞吐和效率[94]。\n图5：一些有代表性的“超级大脑”芯片和AER方法\nA，Neurogrid拥有65,000多个神经元和5亿个突触，而TrueNorth拥有100万神经元和2.56亿个突触。Neurogrid和TrueNorth分别使用树和网格路由拓扑两种结构。Neurogrid使用模拟混合信号设计，TrueNorth依赖数字基元。一般来说，像TrueNorth 这样的数字神经形态系统将神经元的膜电位表示为n位二进制格式。通过适当增加或减少n位字来实现神经元动力学，比如LIF行为。相比之下，模拟系统将膜电位表示为存储在电容中的电荷。通过电流从电容进出，模拟所需的神经元动力学。尽管存在电路差异，但一般来说，模拟系统和数字系统都使用事件驱动AER进行尖峰通信。事件驱动通信是实现低能耗大规模集成系统的关键。\nB，基本的 AER通信系统。每当发射端送出一个事件（一个脉冲），相应的地址就通过数据总线发送到接收端。接收端解码输入地址，并重新构造脉冲的序列。因此，每个脉冲由其位置（地址）显式编码，并在其地址发送到数据总线时隐式编码。\n减轻这一瓶颈影响的方法就是使用“近内存（near-memory）”、“内存中”计算[95,96]。近内存计算是通过在内存单元附近嵌入一个专门的处理器，由此实现内存和计算的“共存”。实际上，各种“超级大脑芯片”的分布式计算体系结构所具有的紧密放置的神经元和突触阵列就是近内存计算的表现。相比较而言，内存中计算则是把部分计算操作嵌入到内存内部或外部电路中。\n非易失性技术 非易失性技术（ non-volatile technology）[97–103]通常被用于与生物突触相比较。实际上，它们展示了生物突触的两个特征：突触效能（synaptic efficacy）和突触可塑性（ synaptic plasticity）。突触可塑性指的是根据特定的学习规则调整突触权重的能力。突触效能指的是根据输入脉冲产生输出的现象。以最简单的形式来说，意思就是，输入的脉冲信号乘以突触的权重。这表示着可编程、模拟、非易失性。从上游神经元得到的信号，相乘再求和后再作用于下游神经元的输入。后文的图片就说明了，如何使用新兴的**非易失性忆阻技术（ non-volatile memristive technology）**实现突触效率和突触可塑性[103,104]。而且，还可以通过时间驱动NOC的方式了连接开关，从而构建密集大规模的神经处理器，以实现内存中计算。\n一些已经发表过的基于忆阻技术的研究[105,106]，比如可变电阻式内存（RRAM）[107]、相变内存（PCM）[108]、 和自旋传递扭矩磁性随机读写存储器（STT-MRAM）[109]，已经在原位点积和基于 STDP 规则的突触学习中进行了探索。RRAM（氧化物基和导电桥基[107]）是电场驱动器件，依靠丝极的形成来模拟可编程电阻。RRAM容易出现因设备、因周期的变化[110,111]，这是这一技术的主要障碍。PCMS包括夹在两个电极之间的硫系材料，可以在非晶态（高电阻）和晶态（低电阻）之间转换。PCM设备有类似的编程电压和RRAM的写入速度，不过这种器件也会受到高写入电流和长时间电阻漂移的影响[108]。自旋电子器件是由两块垫片隔开的磁铁组成，依据两层的磁化方向是平行还是反平行，能呈现两种电磁状态。与RRAM和PCM相比，自旋装置显示出几乎无限的耐久性，更低的写入能量和更快的磁极反转速度[109]。然而，在自旋器件中，两个极端电阻态（ON/OFF）的比率要比在PCM和RRAM中小得多。\n另一类包含可调非易失性电阻的非易失性器件是浮栅晶体管（floating-gate transistor）。此类设备有作为突触存储器的潜力[112–114]。实际上浮栅晶体管是第一个用作非易失性突触存储器的设备[115,116]。因为，他们与MOS制造工艺的相容，比其他新型器件的生产技术更加成熟，然而，与其他非易失性技术相比，浮栅晶体管的主要缺点是耐用性低和编程电压高。\n虽然**原位计算（situ computing）**和突触学习为大规模超越冯·诺依曼分布式计算提供了诱人的前景，但有许多的挑战仍然有待克服，因设备、因周期和进程相关引起的变化，计算的近似性质容易出现错误，从而减低整体的计算效率，最终影响准确性。此外，交叉开关操作的鲁棒性受到电流潜通路、线电阻、驱动电路的源电阻和感测电阻的存在的影响[117,118]。选择器（晶体管或双端非线性装置）的非理想性、对模拟-数字转换设备的要求和有限的比特精度要求，也增加了使用非传统突触装置设计可靠计算的总体复杂性。此外，写入非易失性设备通常会消耗大量的资源。而且，此类设备的固有随机性会导致不可靠的写操作，又需要代价高昂的检验方案[119]。\n图6：使用非易失性存储设备作为突触存储器件。A，各种非易失性技术的原理图：PCM，RRAM，STT-MRAM和浮栅晶体管。这种非易失性设备已经被用作突触存储和原位神经突触计算，以及通用非神经形态存储器内加速器。B，用忆阻技术实现突触效能和突触可塑性。图中展示了以交叉方式连接的忆阻器阵列。根据欧姆定律，水平线（绿色）上的输入脉冲产生与忆阻元件电导成比例的电流。又因为基尔霍夫电流定律，与通过多个峰前神经元的电流沿着垂直线（黑色）相加。这一操作表示了具备突触功效的内存中点积运算。通常，只要神经元前和后尖峰分别在水平线和垂直线上(如在STDP)，突触可塑性就可以通过适当地施加电压脉冲来原位实现。组成忆阻器的电阻值是根据在相应的水平和垂直线上产生的电压差编程的。根据特定的器件技术，再选择要编程电压脉冲的形状和定时。请注意，浮栅晶体管是三端子器件，因此需要额外的水平线和（或）垂直线来实现交叉开关功能。该图还显示了以平铺方式连接NOC的忆阻阵列，由此可以实现高吞吐量的原位计算。\n硅（内存中）计算 除了非易失性技术之外，各种使用标准硅存储器（包括静态和动态随机存取存储器）进行内存计算的设想均在研究中。这些工作主要集中在把布尔向量计算嵌入内存数组中[120–122]。此外，混合信号模拟内存计算操作和二进制卷积操作最近被证明是可行的[123,124]。事实上，目前几乎所有主要的内存技术都在探索各种形式的内存计算，包括静态内存[125]和动态内存[126]、RRAM[127]、PCM[128]和STT-MRAM[129]。尽管这些工作大部分集中在常见的计算应用上，如加密和深度神经网络，但他们也可以轻松的一直到SNN上来。\n五、算法-硬件协同设计 混合信号模拟计算 模拟计算容易受到过程引起的变化和噪声的影响，并且由于模拟和数字转换设备的复杂性和精度要求，在面积和能耗方面就受到了很大限制。将芯片学习与紧密结合的模拟计算框架相结合，将使这类系统能够从根本上适应过程引起的变化，从而减轻对精度的影响。在过去[130,131]以及近期的可行性生物算法的研究[54]中，已经研究了以**芯片上（ on-chip）和设备上（on-device）**学习解决方案为重点的局部学习。本质上，无论是局部学习这种形式还是树突学习这种范式，我们都认为，更好的容错局部学习算法——即使是要学习额外的参数——将是推动模拟神经形态计算的关键所在。此外，芯片上学习的适应能力可在不降低目标精度的前提下，开发低成本的近似模数转换器。\n忆阻点积 作为模拟计算的一个实例，**忆阻点积（Memristive dot products）**是实现原位神经形态计算的一种有前景的方法。不幸的是，表示点积的忆阻阵列中产生的电流既有空间依赖性又有数据依赖性，这使得交叉开关电路分析成为一个非常复杂的问题。研究交叉开关电路非理想状态的影响[117,132,133]，探索减轻点积不准确影响的训练方法的研究并不多[118,134]。而且，这些工作大部分集中在深度神经网络而不是SNN中。然而，我们可以合理地假设，在这些工作中开发出的基本器件和对电路的见解也能用于SNN的实现。现有的工作需要精致的的设备-通路模拟运行，必须与训练算法紧密耦合，以减少精度损失。我们认为，基于最新设备的交叉开关阵列的理论模型，以及为点积误差建立理论边界的努力，都将引起人们的关注。这将使算法设计者无需耗时、设计迭代设备-通路-算法模拟，就能探索新的训练算法，同时也能解决硬件不一致的问题。\n随机性 随机SNN引起了人们极大的兴趣，这是因为新兴的设备本身具有随机性[135,136]。随机二进制SNN 的实现结果，大多数都集中在MNIST数字识别之类的小规模任务上[56]。这类工作的共同主题是使用随机STDP类的本地学习规则来生成权重更新。我们认为，即使在二元条件下，STDP学习中的时间维度权重更新提供了额外的带宽，是的整体朝着真确的方向前进（总体精度）。这种二元局部学习方案与基于梯度下降的大规模学习规则相结合，在利用了硬件随机性的同时，为高能效神经系统提供了有利的机会。\n混合设计方法 我们认为，基于混合方法的硬件解决方案——即在单一平台上结合各种技术的优势——是另一个需要深入研究的重要领域。这种方法可以在最近的文献中找到[137]，比如，把低精度忆阻器与高精度数字处理器结合使用。这种混合方法有许多可能的变体，包括显著驱动的计算数据分离、混合精度计算[137]、将常规硅存储器重新配置为需内存近似加速器[125]、局部同步和全局异步设计[138]、局部模拟和全局数字系统；其中新兴技术和传统技术可以同时使用，以提高精确度和效率。此外，这种混合硬件可以与基于混合脉冲学习的方法结合使用，例如局部无监督学习，然后是全局有监督反向传播算法[53]。我们认为，这种局部-全局学习方案可以用来降低硬件复杂性，同时，最大限度的减少对终端应用程序的性能影响。\n六、总 结 如今，“智能化”已经成为了我们周围所有学科的主题。在这方面，本文阐述了神经形态计算作为一种高效方式，通过硬件（计算）和算法（智能）的协同演化的方式来实现机器智能。\n我们首先讨论了脉冲神经范式的算法含义，这种范式使用事件驱动计算，而不是传统深度学习范式中的数值计算。描述了实现标准分类任务的学习规则（例如基于脉冲的梯度下降、无监督STDP和从深度学习到脉冲模型的转换方法）的优点和局限性。\n未来的算法研究应该利用基于脉冲信号的信息处理的稀疏和时间动态特性；以及可以产生实时识别的互补神经形态学数据集；硬件开发应该侧重于事件驱动的计算、内存和计算单元的协调，以及模拟神经突触的动态特征。特别引人关注的是新兴的非易失性技术，这项技术支持了原位混合信号的模拟计算。我们也讨论了包含算法-硬件协同设计的跨层优化的前景。例如，利用算法适应性（局部学习）和硬件可行性（实现随机脉冲）。\n最后，我们谈到，基于传统和新兴设备构建的基于脉冲的节能智能系统与当前无处不在的人工智能相比，二者的前景其实是相吻合的。现在是我们该交换理念的时候了，通过设备、通路、架构和算法等多学科的努力，通力合作打造一台真正节能且智能的机器。\n","wordCount":"19736","inLanguage":"zh","datePublished":"2025-12-06T18:34:25+08:00","dateModified":"2025-12-06T22:54:22+08:00","author":{"@type":"Person","name":"RM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rosefinch-midsummer.github.io/zh/posts/book/%E9%AB%98%E8%83%BD%E6%95%88%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E4%B8%8E%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"},"publisher":{"@type":"Organization","name":"天漢帝國復興錄","logo":{"@type":"ImageObject","url":"https://rosefinch-midsummer.github.io/img/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rosefinch-midsummer.github.io/zh/ accesskey=h title="天漢帝國復興錄 (Alt + H)">天漢帝國復興錄</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rosefinch-midsummer.github.io/zh/ title=🏠主頁><span>🏠主頁</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/posts title=📚文章><span>📚文章</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/search title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/archives title=⏱時間軸><span>⏱時間軸</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/categories title=🧩分類><span>🧩分類</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/tags title=🔖標簽><span>🔖標簽</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/about title=🙋🏻‍♂️關于><span>🙋🏻‍♂️關于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://rosefinch-midsummer.github.io/zh/>首頁</a>&nbsp;»&nbsp;<a href=https://rosefinch-midsummer.github.io/zh/posts/>📚文章</a>&nbsp;»&nbsp;<a href=https://rosefinch-midsummer.github.io/zh/posts/book/>📕閱讀</a></div><h1 class=post-title>《高能效类脑智能：算法与体系架构》</h1><div class=post-meta>创建: 2025-12-06 |
更新: 2025-12-06 |
字数: 19736字 |
时长: 40分钟 |
RM</div><div class=meta-item>&nbsp·&nbsp
<span id=busuanzi_container_page_pv>本文阅读量<span id=busuanzi_value_page_pv></span>次</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e9%ab%98%e8%83%bd%e6%95%88%e7%b1%bb%e8%84%91%e6%99%ba%e8%83%bd%e7%ae%97%e6%b3%95%e4%b8%8e%e4%bd%93%e7%b3%bb%e6%9e%b6%e6%9e%84 aria-label=《高能效类脑智能：算法与体系架构》>《高能效类脑智能：算法与体系架构》</a></li><li><a href=#%e6%a6%82%e8%ae%ba aria-label=概论>概论</a><ul><li><a href=#%e5%89%8d%e8%a8%80 aria-label=前言>前言</a></li><li><a href=#%e4%b9%a6%e7%b1%8d%e7%ae%80%e4%bb%8b aria-label=书籍简介>书籍简介</a></li><li><a href=#%e5%86%85%e5%ae%b9%e7%ae%80%e4%bb%8b aria-label=内容简介>内容简介</a></li><li><a href=#%e4%bd%9c%e8%80%85%e7%ae%80%e4%bb%8b aria-label=作者简介>作者简介</a></li></ul></li><li><a href=#%e6%ad%a3%e6%96%87%e6%91%98%e5%bd%95 aria-label=正文摘录>正文摘录</a><ul><li><a href=#%e8%af%91%e8%80%85%e5%ba%8f aria-label=译者序>译者序</a></li><li><a href=#%e5%89%8d%e8%a8%80-1 aria-label=前言>前言</a></li><li><a href=#%e8%87%b4%e8%b0%a2 aria-label=致谢>致谢</a></li><li><a href=#%e7%ac%ac1%e7%ab%a0-%e6%a6%82%e8%bf%b0 aria-label="第1章 概述">第1章 概述</a><ul><li><a href=#11-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%8e%86%e5%8f%b2 aria-label="1.1 神经网络的历史">1.1 神经网络的历史</a></li><li><a href=#12-%e8%bd%af%e4%bb%b6%e4%b8%ad%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="1.2 软件中的神经网络">1.2 软件中的神经网络</a><ul><li><a href=#121-%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="1.2.1 人工神经网络">1.2.1 人工神经网络</a></li><li><a href=#122-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="1.2.2 脉冲神经网络">1.2.2 脉冲神经网络</a></li></ul></li><li><a href=#13-%e7%a5%9e%e7%bb%8f%e5%bd%a2%e6%80%81%e7%a1%ac%e4%bb%b6%e7%9a%84%e9%9c%80%e6%b1%82 aria-label="1.3 神经形态硬件的需求">1.3 神经形态硬件的需求</a></li><li><a href=#14-%e6%9c%ac%e4%b9%a6%e7%9a%84%e7%9b%ae%e6%a0%87%e5%92%8c%e5%a4%a7%e7%ba%b2 aria-label="1.4 本书的目标和大纲">1.4 本书的目标和大纲</a></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label=参考文献>参考文献</a></li></ul></li><li><a href=#%e7%ac%ac2%e7%ab%a0-%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%9f%ba%e7%a1%80%e4%b8%8e%e5%ad%a6%e4%b9%a0 aria-label="第2章 人工神经网络的基础与学习">第2章 人工神经网络的基础与学习</a><ul><li><a href=#21-%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86 aria-label="2.1 人工神经网络的工作原理">2.1 人工神经网络的工作原理</a><ul><li><a href=#211-%e6%8e%a8%e7%90%86 aria-label="2.1.1 推理">2.1.1 推理</a></li><li><a href=#212-%e5%ad%a6%e4%b9%a0 aria-label="2.1.2 学习">2.1.2 学习</a></li></ul></li><li><a href=#22-%e5%9f%ba%e4%ba%8e%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0 aria-label="2.2 基于神经网络的机器学习">2.2 基于神经网络的机器学习</a><ul><li><a href=#221-%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="2.2.1 监督学习">2.2.1 监督学习</a></li><li><a href=#222-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label="2.2.2 强化学习">2.2.2 强化学习</a></li><li><a href=#223-%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="2.2.3 无监督学习">2.2.3 无监督学习</a></li><li><a href=#224-%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6%e5%9f%ba%e4%ba%8e%e5%8a%a8%e4%bd%9c%e7%9a%84%e5%90%af%e5%8f%91%e5%bc%8f%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92 aria-label="2.2.4 案例研究：基于动作的启发式动态规划">2.2.4 案例研究：基于动作的启发式动态规划</a></li></ul></li><li><a href=#23-%e7%bd%91%e7%bb%9c%e6%8b%93%e6%89%91 aria-label="2.3 网络拓扑">2.3 网络拓扑</a><ul><li><a href=#231-%e5%85%a8%e8%bf%9e%e6%8e%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="2.3.1 全连接神经网络">2.3.1 全连接神经网络</a></li><li><a href=#232-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="2.3.2 卷积神经网络">2.3.2 卷积神经网络</a></li><li><a href=#233-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="2.3.3 循环神经网络">2.3.3 循环神经网络</a></li></ul></li><li><a href=#24-%e6%95%b0%e6%8d%ae%e9%9b%86%e5%92%8c%e5%9f%ba%e5%87%86 aria-label="2.4 数据集和基准">2.4 数据集和基准</a></li><li><a href=#25-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0 aria-label="2.5 深度学习">2.5 深度学习</a><ul><li><a href=#251-%e5%89%8d%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%97%b6%e4%bb%a3 aria-label="2.5.1 前深度学习时代">2.5.1 前深度学习时代</a></li><li><a href=#252-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%b4%9b%e8%b5%b7 aria-label="2.5.2 深度学习的崛起">2.5.2 深度学习的崛起</a></li><li><a href=#253-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%8a%80%e6%9c%af aria-label="2.5.3 深度学习技术">2.5.3 深度学习技术</a></li><li><a href=#254-%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%a4%ba%e4%be%8b aria-label="2.5.4 深度神经网络示例">2.5.4 深度神经网络示例</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae-1 aria-label=参考文献>参考文献</a></li></ul></li><li><a href=#%e7%ac%ac3%e7%ab%a0-%e7%a1%ac%e4%bb%b6%e4%b8%ad%e7%9a%84%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="第3章 硬件中的人工神经网络">第3章 硬件中的人工神经网络</a><ul><li><a href=#31-%e6%a6%82%e8%bf%b0 aria-label="3.1 概述">3.1 概述</a></li><li><a href=#32-%e9%80%9a%e7%94%a8%e5%a4%84%e7%90%86%e5%99%a8 aria-label="3.2 通用处理器">3.2 通用处理器</a></li><li><a href=#33-%e6%95%b0%e5%ad%97%e5%8a%a0%e9%80%9f%e5%99%a8 aria-label="3.3 数字加速器">3.3 数字加速器</a><ul><li><a href=#331-%e6%95%b0%e5%ad%97asic%e5%ae%9e%e7%8e%b0%e6%96%b9%e6%b3%95 aria-label="3.3.1 数字ASIC实现方法">3.3.1 数字ASIC实现方法</a></li><li><a href=#332-fpga%e5%8a%a0%e9%80%9f%e5%99%a8 aria-label="3.3.2 FPGA加速器">3.3.2 FPGA加速器</a></li></ul></li><li><a href=#34-%e6%a8%a1%e6%8b%9f%e6%b7%b7%e5%90%88%e4%bf%a1%e5%8f%b7%e5%8a%a0%e9%80%9f%e5%99%a8 aria-label="3.4 模拟/混合信号加速器">3.4 模拟/混合信号加速器</a><ul><li><a href=#341-%e4%bc%a0%e7%bb%9f%e9%9b%86%e6%88%90%e6%8a%80%e6%9c%af%e4%b8%ad%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="3.4.1 传统集成技术中的神经网络">3.4.1 传统集成技术中的神经网络</a></li><li><a href=#342-%e5%9f%ba%e4%ba%8e%e6%96%b0%e5%85%b4%e9%9d%9e%e6%98%93%e5%a4%b1%e6%80%a7%e5%ad%98%e5%82%a8%e5%99%a8%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="3.4.2 基于新兴非易失性存储器的神经网络">3.4.2 基于新兴非易失性存储器的神经网络</a></li><li><a href=#343-%e5%85%89%e5%ad%a6%e5%8a%a0%e9%80%9f%e5%99%a8 aria-label="3.4.3 光学加速器">3.4.3 光学加速器</a></li></ul></li><li><a href=#35-%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6%e4%b8%80%e7%a7%8d%e8%8a%82%e8%83%bd%e7%9a%84%e8%87%aa%e9%80%82%e5%ba%94%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92%e5%8a%a0%e9%80%9f%e5%99%a8%e7%9a%84%e7%a8%8b%e5%ba%8f%e8%ae%be%e8%ae%a1 aria-label="3.5 案例研究：一种节能的自适应动态规划加速器的程序设计">3.5 案例研究：一种节能的自适应动态规划加速器的程序设计</a><ul><li><a href=#351-%e7%a1%ac%e4%bb%b6%e6%9e%b6%e6%9e%84 aria-label="3.5.1 硬件架构">3.5.1 硬件架构</a></li><li><a href=#352-%e8%ae%be%e8%ae%a1%e7%a4%ba%e4%be%8b aria-label="3.5.2 设计示例">3.5.2 设计示例</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae-2 aria-label=参考文献>参考文献</a></li></ul></li><li><a href=#%e7%ac%ac4%e7%ab%a0-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%b7%a5%e4%bd%9c%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0 aria-label="第4章 脉冲神经网络的工作原理与学习">第4章 脉冲神经网络的工作原理与学习</a><ul><li><a href=#41-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="4.1 脉冲神经网络">4.1 脉冲神经网络</a><ul><li><a href=#411-%e5%b8%b8%e8%a7%81%e7%9a%84%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e5%85%83%e6%a8%a1%e5%9e%8b aria-label="4.1.1 常见的脉冲神经元模型">4.1.1 常见的脉冲神经元模型</a></li><li><a href=#412-%e4%bf%a1%e6%81%af%e7%bc%96%e7%a0%81 aria-label="4.1.2 信息编码">4.1.2 信息编码</a></li><li><a href=#413-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e5%85%83%e4%b8%8e%e9%9d%9e%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e5%85%83%e7%9a%84%e6%af%94%e8%be%83 aria-label="4.1.3 脉冲神经元与非脉冲神经元的比较">4.1.3 脉冲神经元与非脉冲神经元的比较</a></li></ul></li><li><a href=#42-%e6%b5%85%e5%b1%82snn%e7%9a%84%e5%ad%a6%e4%b9%a0 aria-label="4.2 浅层SNN的学习">4.2 浅层SNN的学习</a><ul><li><a href=#421-resume aria-label="4.2.1 ReSuMe">4.2.1 ReSuMe</a></li><li><a href=#422-tempotron aria-label="4.2.2 Tempotron">4.2.2 Tempotron</a></li><li><a href=#423-%e8%84%89%e5%86%b2%e6%97%b6%e9%97%b4%e7%9b%b8%e5%85%b3%e5%8f%af%e5%a1%91%e6%80%a7 aria-label="4.2.3 脉冲时间相关可塑性">4.2.3 脉冲时间相关可塑性</a></li><li><a href=#424-%e5%8f%8c%e5%b1%82%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e9%80%9a%e8%bf%87%e8%b0%83%e5%88%b6%e6%9d%83%e9%87%8d%e4%be%9d%e8%b5%96%e7%9a%84stdp%e8%bf%9b%e8%a1%8c%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%96%b9%e6%b3%95 aria-label="4.2.4 双层神经网络中通过调制权重依赖的STDP进行学习的方法">4.2.4 双层神经网络中通过调制权重依赖的STDP进行学习的方法</a></li></ul></li><li><a href=#43-%e6%b7%b1%e5%ba%a6snn%e5%ad%a6%e4%b9%a0 aria-label="4.3 深度SNN学习">4.3 深度SNN学习</a><ul><li><a href=#431-spikeprop aria-label="4.3.1 SpikeProp">4.3.1 SpikeProp</a></li><li><a href=#432-%e6%b5%85%e5%b1%82%e7%bd%91%e7%bb%9c%e6%a0%88 aria-label="4.3.2 浅层网络栈">4.3.2 浅层网络栈</a></li><li><a href=#433-ann%e7%9a%84%e8%bd%ac%e6%8d%a2 aria-label="4.3.3 ANN的转换">4.3.3 ANN的转换</a></li><li><a href=#434-%e6%b7%b1%e5%ba%a6snn%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e7%a0%94%e7%a9%b6%e8%bf%9b%e5%b1%95 aria-label="4.3.4 深度SNN反向传播的研究进展">4.3.4 深度SNN反向传播的研究进展</a></li><li><a href=#435-%e5%9c%a8%e5%a4%9a%e5%b1%82%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e9%80%9a%e8%bf%87%e8%b0%83%e5%88%b6%e6%9d%83%e9%87%8d%e4%be%9d%e8%b5%96%e7%9a%84stdp%e8%bf%9b%e8%a1%8c%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%96%b9%e6%b3%95 aria-label="4.3.5 在多层神经网络中通过调制权重依赖的STDP进行学习的方法">4.3.5 在多层神经网络中通过调制权重依赖的STDP进行学习的方法</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae-3 aria-label=参考文献>参考文献</a></li></ul></li><li><a href=#%e7%ac%ac5%e7%ab%a0-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e7%a1%ac%e4%bb%b6%e5%ae%9e%e7%8e%b0 aria-label="第5章 脉冲神经网络的硬件实现">第5章 脉冲神经网络的硬件实现</a><ul><li><a href=#51-%e5%af%b9%e4%b8%93%e7%94%a8%e7%a1%ac%e4%bb%b6%e7%9a%84%e9%9c%80%e6%b1%82 aria-label="5.1 对专用硬件的需求">5.1 对专用硬件的需求</a><ul><li><a href=#511-%e5%9c%b0%e5%9d%80%e4%ba%8b%e4%bb%b6%e8%a1%a8%e7%a4%ba aria-label="5.1.1 地址事件表示">5.1.1 地址事件表示</a></li><li><a href=#512-%e4%ba%8b%e4%bb%b6%e9%a9%b1%e5%8a%a8%e8%ae%a1%e7%ae%97 aria-label="5.1.2 事件驱动计算">5.1.2 事件驱动计算</a></li><li><a href=#513-%e6%b8%90%e8%bf%9b%e7%b2%be%e5%ba%a6%e6%8e%a8%e7%90%86 aria-label="5.1.3 渐进精度推理">5.1.3 渐进精度推理</a></li><li><a href=#514-%e5%ae%9e%e7%8e%b0%e6%9d%83%e9%87%8d%e4%be%9d%e8%b5%96%e7%9a%84stdp%e5%ad%a6%e4%b9%a0%e8%a7%84%e5%88%99%e7%9a%84%e7%a1%ac%e4%bb%b6%e6%b3%a8%e6%84%8f%e4%ba%8b%e9%a1%b9 aria-label="5.1.4 实现权重依赖的STDP学习规则的硬件注意事项">5.1.4 实现权重依赖的STDP学习规则的硬件注意事项</a></li></ul></li><li><a href=#52-%e6%95%b0%e5%ad%97%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="5.2 数字脉冲神经网络">5.2 数字脉冲神经网络</a><ul><li><a href=#521-%e5%a4%a7%e8%a7%84%e6%a8%a1%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%93%e7%94%a8%e9%9b%86%e6%88%90%e7%94%b5%e8%b7%af aria-label="5.2.1 大规模脉冲神经网络专用集成电路">5.2.1 大规模脉冲神经网络专用集成电路</a></li><li><a href=#522-%e4%b8%ad%e5%b0%8f%e5%9e%8b%e6%95%b0%e5%ad%97%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="5.2.2 中小型数字脉冲神经网络">5.2.2 中小型数字脉冲神经网络</a></li><li><a href=#523-%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e7%a1%ac%e4%bb%b6%e5%8f%8b%e5%a5%bd%e5%9e%8b%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label="5.2.3 脉冲神经网络中的硬件友好型强化学习">5.2.3 脉冲神经网络中的硬件友好型强化学习</a></li><li><a href=#524-%e5%a4%9a%e5%b1%82%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e7%a1%ac%e4%bb%b6%e5%8f%8b%e5%a5%bd%e5%9e%8b%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="5.2.4 多层脉冲神经网络中的硬件友好型监督学习">5.2.4 多层脉冲神经网络中的硬件友好型监督学习</a></li></ul></li><li><a href=#53-%e6%a8%a1%e6%8b%9f%e6%b7%b7%e5%90%88%e4%bf%a1%e5%8f%b7%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="5.3 模拟/混合信号脉冲神经网络">5.3 模拟/混合信号脉冲神经网络</a><ul><li><a href=#531-%e5%9f%ba%e6%9c%ac%e6%9e%84%e5%bb%ba%e5%9d%97 aria-label="5.3.1 基本构建块">5.3.1 基本构建块</a></li><li><a href=#532-%e5%a4%a7%e8%a7%84%e6%a8%a1%e6%a8%a1%e6%8b%9f%e6%b7%b7%e5%90%88%e4%bf%a1%e5%8f%b7cmos%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="5.3.2 大规模模拟/混合信号CMOS脉冲神经网络">5.3.2 大规模模拟/混合信号CMOS脉冲神经网络</a></li><li><a href=#533-%e5%85%b6%e4%bb%96%e6%a8%a1%e6%8b%9f%e6%b7%b7%e5%90%88%e4%bf%a1%e5%8f%b7cmos%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%93%e7%94%a8%e9%9b%86%e6%88%90%e7%94%b5%e8%b7%af aria-label="5.3.3 其他模拟/混合信号CMOS脉冲神经网络专用集成电路">5.3.3 其他模拟/混合信号CMOS脉冲神经网络专用集成电路</a></li><li><a href=#534-%e5%9f%ba%e4%ba%8e%e6%96%b0%e5%85%b4%e7%ba%b3%e7%b1%b3%e6%8a%80%e6%9c%af%e7%9a%84%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="5.3.4 基于新兴纳米技术的脉冲神经网络">5.3.4 基于新兴纳米技术的脉冲神经网络</a></li><li><a href=#535-%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ad%e5%9f%ba%e4%ba%8e%e5%bf%86%e9%98%bb%e5%99%a8%e4%ba%a4%e5%8f%89%e5%bc%80%e5%85%b3%e7%9a%84%e5%ad%a6%e4%b9%a0 aria-label="5.3.5 案例研究：脉冲神经网络中基于忆阻器交叉开关的学习">5.3.5 案例研究：脉冲神经网络中基于忆阻器交叉开关的学习</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae-4 aria-label=参考文献>参考文献</a></li></ul></li><li><a href=#%e7%ac%ac6%e7%ab%a0-%e6%80%bb%e7%bb%93 aria-label="第6章 总结">第6章 总结</a><ul><li><a href=#61-%e5%b1%95%e6%9c%9b aria-label="6.1 展望">6.1 展望</a><ul><li><a href=#611-%e8%84%91%e5%90%af%e5%8f%91%e5%bc%8f%e8%ae%a1%e7%ae%97 aria-label="6.1.1 脑启发式计算">6.1.1 脑启发式计算</a></li><li><a href=#612-%e6%96%b0%e5%85%b4%e7%9a%84%e7%ba%b3%e7%b1%b3%e6%8a%80%e6%9c%af aria-label="6.1.2 新兴的纳米技术">6.1.2 新兴的纳米技术</a></li><li><a href=#613-%e7%a5%9e%e7%bb%8f%e5%bd%a2%e6%80%81%e7%b3%bb%e7%bb%9f%e7%9a%84%e5%8f%af%e9%9d%a0%e8%ae%a1%e7%ae%97 aria-label="6.1.3 神经形态系统的可靠计算">6.1.3 神经形态系统的可靠计算</a></li><li><a href=#614-%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%92%8c%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%9e%8d%e5%90%88 aria-label="6.1.4 人工神经网络和脉冲神经网络的融合">6.1.4 人工神经网络和脉冲神经网络的融合</a></li></ul></li><li><a href=#62-%e7%bb%93%e8%ae%ba aria-label="6.2 结论">6.2 结论</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae-5 aria-label=参考文献>参考文献</a></li><li><a href=#%e9%99%84%e5%bd%95 aria-label=附录>附录</a></li><li><a href=#%e6%9c%af%e8%af%ad%e8%a1%a8 aria-label=术语表>术语表</a></li></ul></li><li><a href=#%e9%99%84%e5%bd%95-1 aria-label=附录>附录</a><ul><li><a href=#%e6%96%b0%e9%97%bb%e6%8a%a5%e9%81%93%e4%b8%ad%e7%a7%91%e9%99%a2%e9%99%a2%e5%a3%ab%e5%bc%a0%e6%97%ad%e7%b1%bb%e8%84%91%e6%99%ba%e8%83%bd%e6%98%af%e6%99%ba%e8%83%bd%e6%97%b6%e4%bb%a3%e7%9a%84%e6%96%b0%e8%b4%a8%e7%94%9f%e4%ba%a7%e5%8a%9b aria-label=新闻报道中科院院士张旭：类脑智能是智能时代的新质生产力>新闻报道中科院院士张旭：类脑智能是智能时代的新质生产力</a></li><li><a href=#%e7%9f%a5%e4%b9%8e%e6%96%87%e7%ab%a0nature-%e9%95%bf%e6%96%87%e7%bb%bc%e8%bf%b0%e7%b1%bb%e8%84%91%e6%99%ba%e8%83%bd%e4%b8%8e%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%89%8d%e6%b2%bf aria-label="知乎文章Nature 长文综述：类脑智能与脉冲神经网络前沿">知乎文章Nature 长文综述：类脑智能与脉冲神经网络前沿</a><ul><li><a href=#%e5%af%bc%e8%af%ad aria-label=导语>导语</a></li><li><a href=#%e5%89%8d%e8%a8%80-2 aria-label=前言>前言</a></li><li><a href=#%e4%b8%80%e7%ae%97%e6%b3%95%e5%b1%95%e6%9c%9b%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c_ aria-label=一、算法展望：脉冲神经网络_>一、算法展望：脉冲神经网络_</a><ul><li><a href=#%e8%84%89%e5%86%b2%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=脉冲神经网络>脉冲神经网络</a></li></ul></li><li><a href=#%e4%ba%8c%e5%9c%a8snns%e4%b8%ad%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95 aria-label=二、在SNNs中学习算法>二、在SNNs中学习算法</a><ul><li><a href=#%e5%9f%ba%e4%ba%8e%e8%bd%ac%e6%8d%a2%e7%9a%84%e6%96%b9%e6%b3%95 aria-label=基于转换的方法>基于转换的方法</a></li><li><a href=#%e5%9f%ba%e4%ba%8e%e8%84%89%e5%86%b2%e7%9a%84%e6%96%b9%e6%b3%95 aria-label=基于脉冲的方法>基于脉冲的方法</a></li><li><a href=#%e5%af%b9%e4%ba%8c%e5%85%83%e5%88%b6%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%90%af%e7%a4%ba aria-label=对二元制学习的启示>对二元制学习的启示</a></li></ul></li><li><a href=#%e4%b8%89%e5%85%b6%e4%bb%96%e6%9c%89%e5%be%85%e7%a0%94%e7%a9%b6%e7%9a%84%e6%96%b9%e5%90%91 aria-label=三、其他有待研究的方向>三、其他有待研究的方向</a><ul><li><a href=#%e8%b6%85%e8%b6%8a%e8%a7%86%e8%a7%89%e4%bb%bb%e5%8a%a1 aria-label=超越视觉任务>超越视觉任务</a></li><li><a href=#%e7%bb%88%e8%ba%ab%e5%ad%a6%e4%b9%a0%e5%92%8c%e5%b0%8f%e6%a0%b7%e6%9c%ac%e5%ad%a6%e4%b9%a0 aria-label=终身学习和小样本学习>终身学习和小样本学习</a></li><li><a href=#%e4%b8%8e%e7%a5%9e%e7%bb%8f%e7%a7%91%e5%ad%a6%e5%bb%ba%e7%ab%8b%e8%81%94%e7%b3%bb aria-label=与神经科学建立联系>与神经科学建立联系</a></li></ul></li><li><a href=#%e5%9b%9b%e7%a1%ac%e4%bb%b6%e5%b1%95%e6%9c%9b aria-label=四、硬件展望>四、硬件展望</a><ul><li><a href=#%e7%a5%9e%e7%bb%8f%e5%bd%a2%e6%80%81%e8%ae%a1%e7%ae%97%e7%9a%84%e5%87%ba%e7%8e%b0 aria-label=神经形态计算的出现>神经形态计算的出现</a></li><li><a href=#%e5%b9%b6%e8%a1%8cgpu%e7%9a%84%e5%87%ba%e7%8e%b0 aria-label=并行GPU的出现>并行GPU的出现</a></li><li><a href=#%e8%b6%85%e7%ba%a7%e5%a4%a7%e8%84%91%e8%8a%af%e7%89%87 aria-label=“超级大脑”芯片>“超级大脑”芯片</a></li><li><a href=#%e8%b6%85%e8%b6%8a%e5%86%af%e8%af%ba%e4%be%9d%e6%9b%bc%e5%bc%8f%e8%ae%a1%e7%ae%97 aria-label=超越冯·诺依曼式计算>超越冯·诺依曼式计算</a></li><li><a href=#%e9%9d%9e%e6%98%93%e5%a4%b1%e6%80%a7%e6%8a%80%e6%9c%af aria-label=非易失性技术>非易失性技术</a></li><li><a href=#%e7%a1%85%e5%86%85%e5%ad%98%e4%b8%ad%e8%ae%a1%e7%ae%97 aria-label=硅（内存中）计算>硅（内存中）计算</a></li></ul></li><li><a href=#%e4%ba%94%e7%ae%97%e6%b3%95-%e7%a1%ac%e4%bb%b6%e5%8d%8f%e5%90%8c%e8%ae%be%e8%ae%a1 aria-label=五、算法-硬件协同设计>五、算法-硬件协同设计</a><ul><li><a href=#%e6%b7%b7%e5%90%88%e4%bf%a1%e5%8f%b7%e6%a8%a1%e6%8b%9f%e8%ae%a1%e7%ae%97 aria-label=混合信号模拟计算>混合信号模拟计算</a></li><li><a href=#%e5%bf%86%e9%98%bb%e7%82%b9%e7%a7%af aria-label=忆阻点积>忆阻点积</a></li><li><a href=#%e9%9a%8f%e6%9c%ba%e6%80%a7 aria-label=随机性>随机性</a></li><li><a href=#%e6%b7%b7%e5%90%88%e8%ae%be%e8%ae%a1%e6%96%b9%e6%b3%95 aria-label=混合设计方法>混合设计方法</a></li></ul></li><li><a href=#%e5%85%ad%e6%80%bb-%e7%bb%93 aria-label="六、总 结">六、总 结</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=高能效类脑智能算法与体系架构>《高能效类脑智能：算法与体系架构》<a hidden class=anchor aria-hidden=true href=#高能效类脑智能算法与体系架构>#</a></h1><h1 id=概论>概论<a hidden class=anchor aria-hidden=true href=#概论>#</a></h1><h2 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h2><p>本书非常推荐阅读，虽然这个领域看起来还是个天坑。</p><p>书中把感知机称为第一代人工神经网络，以深度学习为基础的人工神经网络为第二代，而脉冲神经网络是第三代人工神经网络。书中还给出了人工神经网络和脉冲神经网络的三点区别：编码方式、记忆和时间相关性。</p><h2 id=书籍简介>书籍简介<a hidden class=anchor aria-hidden=true href=#书籍简介>#</a></h2><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/aa1a2fb14dd5a15a037feee026a2ae21.png></p><p>原作名: Learning in Energy-Efficient Neuromorphic Computing: Algorithm and Architecture Co-Design</p><p>ISBN：978-7-111-68299-8</p><p>版印次：1-1</p><p>作者：[中]郑楠(Nan Zheng),[美]皮纳基·马祖姆德(Pinaki Mazumder)</p><p>出版时间：2016-01-08</p><p>定价：99</p><h2 id=内容简介>内容简介<a hidden class=anchor aria-hidden=true href=#内容简介>#</a></h2><p>本书主要关注如何构建高能效具有学习能力的脉冲型神经元网络硬件，并且提供建立具有学习能力的脉冲型神经元网络硬件协同设计、协同优化方法。完整地描述从高级算法到底层硬件实现的细节。本书同样涵盖了脉冲型神经元网络中的许多基础知识和关键点。</p><p>本书从对脉冲型神经元网络的概述开始，讨论基于速率的人工神经网络的应用和训练，介绍实现神经网络的多种方法，如通用处理器和专用硬件，数字加速器和模拟加速器。同时展示了一个为能适应神经网络动态编程而建立的高能效加速器，验证脉冲神经网络的基础概念和流行的学习算法，简介脉冲神经网络硬件。后面的章节为读者介绍三个实现前述章节学习算法的设计案例（两个基于传统CMOS工艺，一个基于新兴的纳米工艺）。本书的结尾对脉冲型神经元网络硬件进行总结与展望。</p><h2 id=作者简介>作者简介<a hidden class=anchor aria-hidden=true href=#作者简介>#</a></h2><p>郑楠 （Nan Zheng）  2011年本科毕业于上海交通大学信息工程专业，2014年和2018年分别获得美国密歇根大学电气工程硕士和博士学位。他目前是NVIDIA高级深度学习架构师，研究兴趣侧重于机器学习应用的低能耗硬件架构、算法和电路技术。</p><p>皮纳基·马祖姆德 （Pinaki Mazumder）  美国密歇根大学电气工程与计算机科学系教授，他的研究兴趣包括对于量子MOS、自旋电子学、欺骗等离子体、共振隧穿器件等新兴技术的CMOS超大规模集成电路设计、半导体存储系统、CAD工具和电路设计。 </p><p>译者简介：</p><p>刘佩林 上海交通大学电子信息与电气工程学院教授，博士生导师。研究领域包括音频、视频、3D信号处理与智能分析，面向机器人的环境感知、人机交互、定位与导航，以及类脑计算与低功耗电路设计等。2017年起任上海交通大学类脑智能应用技术研究中心主任。</p><p>应忍冬 上海交通大学电子信息与电气工程学院副教授，硕士生导师。研究领域包括嵌式系统、数字信号处理及VLSI实现架构、人工智能领域的机器思维原理和实现。</p><p>薛建伟 上海交通大学电子信息与电气工程学院博士研究生。研究领域包括类脑智能、片上多核系统等。</p><h1 id=正文摘录>正文摘录<a hidden class=anchor aria-hidden=true href=#正文摘录>#</a></h1><h2 id=译者序>译者序<a hidden class=anchor aria-hidden=true href=#译者序>#</a></h2><p>本书重点讨论如何为具有学习能力的神经网络构建节能硬件，致力于构建具有学习与执行各种任务的能力的硬件神经网络，提供协同设计和协同优化方法，并提供了从高层算法到底层实现细节的完整视图。开发硬件友好算法的目的是简化硬件实现，而特殊的硬件体系结构的提出则是为了更好地利用算法的独特功能。在本书的各章中，讨论了用于节能型神经网络加速器的算法和硬件体系结构。低功耗对于所有将功耗作为重要考虑因素的应用而言至关重要，使用耗电的GPU和将原始数据发送到可以进一步分析数据的云计算机都不是可行的选择。</p><h2 id=前言-1>前言<a hidden class=anchor aria-hidden=true href=#前言-1>#</a></h2><p>简而言之，本书目前的版本有以下几个显著特点：</p><ul><li>包括神经形态算法硬件加速器的多层次全面评述。</li><li>涵盖架构与算法的协同设计，并采用新兴器件来极大地提升计算效率。</li><li>关注算法与硬件的协同设计，这是在神经形态计算中应用新兴器件（如传统忆阻器和扩散型忆阻器）的关键。</li></ul><h2 id=致谢>致谢<a hidden class=anchor aria-hidden=true href=#致谢>#</a></h2><h2 id=第1章-概述>第1章 概述<a hidden class=anchor aria-hidden=true href=#第1章-概述>#</a></h2><h3 id=11-神经网络的历史>1.1 神经网络的历史<a hidden class=anchor aria-hidden=true href=#11-神经网络的历史>#</a></h3><h3 id=12-软件中的神经网络>1.2 软件中的神经网络<a hidden class=anchor aria-hidden=true href=#12-软件中的神经网络>#</a></h3><h4 id=121-人工神经网络>1.2.1 人工神经网络<a hidden class=anchor aria-hidden=true href=#121-人工神经网络>#</a></h4><h4 id=122-脉冲神经网络>1.2.2 脉冲神经网络<a hidden class=anchor aria-hidden=true href=#122-脉冲神经网络>#</a></h4><h3 id=13-神经形态硬件的需求>1.3 神经形态硬件的需求<a hidden class=anchor aria-hidden=true href=#13-神经形态硬件的需求>#</a></h3><h3 id=14-本书的目标和大纲>1.4 本书的目标和大纲<a hidden class=anchor aria-hidden=true href=#14-本书的目标和大纲>#</a></h3><h3 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h3><h2 id=第2章-人工神经网络的基础与学习>第2章 人工神经网络的基础与学习<a hidden class=anchor aria-hidden=true href=#第2章-人工神经网络的基础与学习>#</a></h2><h3 id=21-人工神经网络的工作原理>2.1 人工神经网络的工作原理<a hidden class=anchor aria-hidden=true href=#21-人工神经网络的工作原理>#</a></h3><h4 id=211-推理>2.1.1 推理<a hidden class=anchor aria-hidden=true href=#211-推理>#</a></h4><h4 id=212-学习>2.1.2 学习<a hidden class=anchor aria-hidden=true href=#212-学习>#</a></h4><h3 id=22-基于神经网络的机器学习>2.2 基于神经网络的机器学习<a hidden class=anchor aria-hidden=true href=#22-基于神经网络的机器学习>#</a></h3><h4 id=221-监督学习>2.2.1 监督学习<a hidden class=anchor aria-hidden=true href=#221-监督学习>#</a></h4><h4 id=222-强化学习>2.2.2 强化学习<a hidden class=anchor aria-hidden=true href=#222-强化学习>#</a></h4><h4 id=223-无监督学习>2.2.3 无监督学习<a hidden class=anchor aria-hidden=true href=#223-无监督学习>#</a></h4><h4 id=224-案例研究基于动作的启发式动态规划>2.2.4 案例研究：基于动作的启发式动态规划<a hidden class=anchor aria-hidden=true href=#224-案例研究基于动作的启发式动态规划>#</a></h4><p>贝尔曼方程</p><p><strong>演员——评论家网络</strong></p><p><strong>在线学习算法</strong></p><p><strong>虚拟更新技术</strong></p><p>GPT5-mini的解释如下所示：</p><ul><li><p>贝尔曼方程：刻画最优价值（或动作价值）与子问题之间递归关系的方程，表示某状态的价值等于该状态下采取最优行动带来的即时奖励加上按转移概率到达的下一状态折扣后续价值的期望。是动态规划与强化学习求解最优策略的基础。</p></li><li><p>演员——评论家网络（Actor–Critic）：由两部分组成的强化学习结构：演员（Actor）输出策略参数化的动作分布，评论家（Critic）估计状态值或动作值用于评价策略；评论家提供梯度或优势估计以指导演员更新，兼顾策略学习的稳定性与样本效率。</p></li><li><p>在线学习算法：数据逐条或小批次到达时即时更新模型的算法范式，要求逐步、低延迟地调整参数以适应流动数据或非稳环境，通常强调单次样本复杂度低、无须存储全量历史且能处理概念漂移。</p></li><li><p>虚拟更新技术：在不实际改变环境或参数的情况下模拟、计算若干更新步骤的影响（如用模型或估计器推演未来梯度/价值变化），以评估或加速决策与学习过程，常用于样本效率提升、批内并行化或避免昂贵真实交互。</p></li></ul><h3 id=23-网络拓扑>2.3 网络拓扑<a hidden class=anchor aria-hidden=true href=#23-网络拓扑>#</a></h3><h4 id=231-全连接神经网络>2.3.1 全连接神经网络<a hidden class=anchor aria-hidden=true href=#231-全连接神经网络>#</a></h4><h4 id=232-卷积神经网络>2.3.2 卷积神经网络<a hidden class=anchor aria-hidden=true href=#232-卷积神经网络>#</a></h4><h4 id=233-循环神经网络>2.3.3 循环神经网络<a hidden class=anchor aria-hidden=true href=#233-循环神经网络>#</a></h4><h3 id=24-数据集和基准>2.4 数据集和基准<a hidden class=anchor aria-hidden=true href=#24-数据集和基准>#</a></h3><h3 id=25-深度学习>2.5 深度学习<a hidden class=anchor aria-hidden=true href=#25-深度学习>#</a></h3><h4 id=251-前深度学习时代>2.5.1 前深度学习时代<a hidden class=anchor aria-hidden=true href=#251-前深度学习时代>#</a></h4><h4 id=252-深度学习的崛起>2.5.2 深度学习的崛起<a hidden class=anchor aria-hidden=true href=#252-深度学习的崛起>#</a></h4><h4 id=253-深度学习技术>2.5.3 深度学习技术<a hidden class=anchor aria-hidden=true href=#253-深度学习技术>#</a></h4><p>性能提升技术：</p><ul><li>无监督预训练</li><li>丢弃（dropout）</li><li>批归一化</li><li>加速随机梯度下降过程
节能技术：</li><li>冗余去除</li><li>精度降低</li></ul><h4 id=254-深度神经网络示例>2.5.4 深度神经网络示例<a hidden class=anchor aria-hidden=true href=#254-深度神经网络示例>#</a></h4><h3 id=参考文献-1>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献-1>#</a></h3><h2 id=第3章-硬件中的人工神经网络>第3章 硬件中的人工神经网络<a hidden class=anchor aria-hidden=true href=#第3章-硬件中的人工神经网络>#</a></h2><p>学犹不及，犹恐失之。——孔子</p><h3 id=31-概述>3.1 概述<a hidden class=anchor aria-hidden=true href=#31-概述>#</a></h3><p>神经网络的发展是为了模拟生物大脑的显著特征，比如它们具有模式识别和在有噪声的情况下探测运动的能力。第2章讨论了许多与人工智能神经网络（ANN）学习和推理相关的基本概念。作为一种算法，神经网络需要在特定的硬件平台上执行，然后才能部署到各种应用中。本章讨论在不同硬件平台上实现的神经网络，考虑不同平台的优缺点。一般来说，有三种类型的硬件平台可以部署神经网络算法：通用处理器、现场可编程门阵列（FPGA）和专用集成电路（ASIC）。</p><h3 id=32-通用处理器>3.2 通用处理器<a hidden class=anchor aria-hidden=true href=#32-通用处理器>#</a></h3><h3 id=33-数字加速器>3.3 数字加速器<a hidden class=anchor aria-hidden=true href=#33-数字加速器>#</a></h3><h4 id=331-数字asic实现方法>3.3.1 数字ASIC实现方法<a hidden class=anchor aria-hidden=true href=#331-数字asic实现方法>#</a></h4><h4 id=332-fpga加速器>3.3.2 FPGA加速器<a hidden class=anchor aria-hidden=true href=#332-fpga加速器>#</a></h4><h3 id=34-模拟混合信号加速器>3.4 模拟/混合信号加速器<a hidden class=anchor aria-hidden=true href=#34-模拟混合信号加速器>#</a></h3><h4 id=341-传统集成技术中的神经网络>3.4.1 传统集成技术中的神经网络<a hidden class=anchor aria-hidden=true href=#341-传统集成技术中的神经网络>#</a></h4><h4 id=342-基于新兴非易失性存储器的神经网络>3.4.2 基于新兴非易失性存储器的神经网络<a hidden class=anchor aria-hidden=true href=#342-基于新兴非易失性存储器的神经网络>#</a></h4><h4 id=343-光学加速器>3.4.3 光学加速器<a hidden class=anchor aria-hidden=true href=#343-光学加速器>#</a></h4><h3 id=35-案例研究一种节能的自适应动态规划加速器的程序设计>3.5 案例研究：一种节能的自适应动态规划加速器的程序设计<a hidden class=anchor aria-hidden=true href=#35-案例研究一种节能的自适应动态规划加速器的程序设计>#</a></h3><h4 id=351-硬件架构>3.5.1 硬件架构<a hidden class=anchor aria-hidden=true href=#351-硬件架构>#</a></h4><h4 id=352-设计示例>3.5.2 设计示例<a hidden class=anchor aria-hidden=true href=#352-设计示例>#</a></h4><h3 id=参考文献-2>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献-2>#</a></h3><h2 id=第4章-脉冲神经网络的工作原理与学习>第4章 脉冲神经网络的工作原理与学习<a hidden class=anchor aria-hidden=true href=#第4章-脉冲神经网络的工作原理与学习>#</a></h2><h3 id=41-脉冲神经网络>4.1 脉冲神经网络<a hidden class=anchor aria-hidden=true href=#41-脉冲神经网络>#</a></h3><p>脉冲神经网络（SNN）的灵感来自生物神经网络。对SNN的研究可以追溯到人工神经网络（ANN）广为人知很久以前。最初关于SNN的研究是为了模拟生物神经网络。近年来，在这些领域的先驱如Gerstner、Maass等的带领下，以计算为目的的使用SNN变得越来越热门。虽然ANN的早期发展受到了SNN的启发，但是SNN和ANN有很大的不同。我们在第2章和第3章中分别讨论了以下几个方面：</p><p>（1）SNN和ANN中信息的编码方式不同。非脉冲神经元利用实数值激活来传信息，而脉冲神经元用脉冲来表示信息。</p><p>（2）神经网络中的非脉冲神经元没有任何记忆，但脉冲神经元通常有记忆。</p><p>（3）许多ANN（尤其是前馈ANN）产生的输出不是时间的函数，但是大多数SNN本质上是随时间变化的。</p><h4 id=411-常见的脉冲神经元模型>4.1.1 常见的脉冲神经元模型<a hidden class=anchor aria-hidden=true href=#411-常见的脉冲神经元模型>#</a></h4><p>Hodgkin-Huxley模型</p><p>Leaky Integrate-and-Fire 模型</p><p>Izhikevich模型</p><h4 id=412-信息编码>4.1.2 信息编码<a hidden class=anchor aria-hidden=true href=#412-信息编码>#</a></h4><h4 id=413-脉冲神经元与非脉冲神经元的比较>4.1.3 脉冲神经元与非脉冲神经元的比较<a hidden class=anchor aria-hidden=true href=#413-脉冲神经元与非脉冲神经元的比较>#</a></h4><h3 id=42-浅层snn的学习>4.2 浅层SNN的学习<a hidden class=anchor aria-hidden=true href=#42-浅层snn的学习>#</a></h3><h4 id=421-resume>4.2.1 ReSuMe<a hidden class=anchor aria-hidden=true href=#421-resume>#</a></h4><h4 id=422-tempotron>4.2.2 Tempotron<a hidden class=anchor aria-hidden=true href=#422-tempotron>#</a></h4><h4 id=423-脉冲时间相关可塑性>4.2.3 脉冲时间相关可塑性<a hidden class=anchor aria-hidden=true href=#423-脉冲时间相关可塑性>#</a></h4><h4 id=424-双层神经网络中通过调制权重依赖的stdp进行学习的方法>4.2.4 双层神经网络中通过调制权重依赖的STDP进行学习的方法<a hidden class=anchor aria-hidden=true href=#424-双层神经网络中通过调制权重依赖的stdp进行学习的方法>#</a></h4><p>用脉冲时间估计梯度</p><h3 id=43-深度snn学习>4.3 深度SNN学习<a hidden class=anchor aria-hidden=true href=#43-深度snn学习>#</a></h3><h4 id=431-spikeprop>4.3.1 SpikeProp<a hidden class=anchor aria-hidden=true href=#431-spikeprop>#</a></h4><h4 id=432-浅层网络栈>4.3.2 浅层网络栈<a hidden class=anchor aria-hidden=true href=#432-浅层网络栈>#</a></h4><h4 id=433-ann的转换>4.3.3 ANN的转换<a hidden class=anchor aria-hidden=true href=#433-ann的转换>#</a></h4><h4 id=434-深度snn反向传播的研究进展>4.3.4 深度SNN反向传播的研究进展<a hidden class=anchor aria-hidden=true href=#434-深度snn反向传播的研究进展>#</a></h4><h4 id=435-在多层神经网络中通过调制权重依赖的stdp进行学习的方法>4.3.5 在多层神经网络中通过调制权重依赖的STDP进行学习的方法<a hidden class=anchor aria-hidden=true href=#435-在多层神经网络中通过调制权重依赖的stdp进行学习的方法>#</a></h4><h3 id=参考文献-3>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献-3>#</a></h3><h2 id=第5章-脉冲神经网络的硬件实现>第5章 脉冲神经网络的硬件实现<a hidden class=anchor aria-hidden=true href=#第5章-脉冲神经网络的硬件实现>#</a></h2><h3 id=51-对专用硬件的需求>5.1 对专用硬件的需求<a hidden class=anchor aria-hidden=true href=#51-对专用硬件的需求>#</a></h3><h4 id=511-地址事件表示>5.1.1 地址事件表示<a hidden class=anchor aria-hidden=true href=#511-地址事件表示>#</a></h4><h4 id=512-事件驱动计算>5.1.2 事件驱动计算<a hidden class=anchor aria-hidden=true href=#512-事件驱动计算>#</a></h4><h4 id=513-渐进精度推理>5.1.3 渐进精度推理<a hidden class=anchor aria-hidden=true href=#513-渐进精度推理>#</a></h4><h4 id=514-实现权重依赖的stdp学习规则的硬件注意事项>5.1.4 实现权重依赖的STDP学习规则的硬件注意事项<a hidden class=anchor aria-hidden=true href=#514-实现权重依赖的stdp学习规则的硬件注意事项>#</a></h4><h3 id=52-数字脉冲神经网络>5.2 数字脉冲神经网络<a hidden class=anchor aria-hidden=true href=#52-数字脉冲神经网络>#</a></h3><h4 id=521-大规模脉冲神经网络专用集成电路>5.2.1 大规模脉冲神经网络专用集成电路<a hidden class=anchor aria-hidden=true href=#521-大规模脉冲神经网络专用集成电路>#</a></h4><h4 id=522-中小型数字脉冲神经网络>5.2.2 中小型数字脉冲神经网络<a hidden class=anchor aria-hidden=true href=#522-中小型数字脉冲神经网络>#</a></h4><h4 id=523-脉冲神经网络中的硬件友好型强化学习>5.2.3 脉冲神经网络中的硬件友好型强化学习<a hidden class=anchor aria-hidden=true href=#523-脉冲神经网络中的硬件友好型强化学习>#</a></h4><h4 id=524-多层脉冲神经网络中的硬件友好型监督学习>5.2.4 多层脉冲神经网络中的硬件友好型监督学习<a hidden class=anchor aria-hidden=true href=#524-多层脉冲神经网络中的硬件友好型监督学习>#</a></h4><h3 id=53-模拟混合信号脉冲神经网络>5.3 模拟/混合信号脉冲神经网络<a hidden class=anchor aria-hidden=true href=#53-模拟混合信号脉冲神经网络>#</a></h3><h4 id=531-基本构建块>5.3.1 基本构建块<a hidden class=anchor aria-hidden=true href=#531-基本构建块>#</a></h4><h4 id=532-大规模模拟混合信号cmos脉冲神经网络>5.3.2 大规模模拟/混合信号CMOS脉冲神经网络<a hidden class=anchor aria-hidden=true href=#532-大规模模拟混合信号cmos脉冲神经网络>#</a></h4><h4 id=533-其他模拟混合信号cmos脉冲神经网络专用集成电路>5.3.3 其他模拟/混合信号CMOS脉冲神经网络专用集成电路<a hidden class=anchor aria-hidden=true href=#533-其他模拟混合信号cmos脉冲神经网络专用集成电路>#</a></h4><h4 id=534-基于新兴纳米技术的脉冲神经网络>5.3.4 基于新兴纳米技术的脉冲神经网络<a hidden class=anchor aria-hidden=true href=#534-基于新兴纳米技术的脉冲神经网络>#</a></h4><h4 id=535-案例研究脉冲神经网络中基于忆阻器交叉开关的学习>5.3.5 案例研究：脉冲神经网络中基于忆阻器交叉开关的学习<a hidden class=anchor aria-hidden=true href=#535-案例研究脉冲神经网络中基于忆阻器交叉开关的学习>#</a></h4><h3 id=参考文献-4>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献-4>#</a></h3><h2 id=第6章-总结>第6章 总结<a hidden class=anchor aria-hidden=true href=#第6章-总结>#</a></h2><h3 id=61-展望>6.1 展望<a hidden class=anchor aria-hidden=true href=#61-展望>#</a></h3><h4 id=611-脑启发式计算>6.1.1 脑启发式计算<a hidden class=anchor aria-hidden=true href=#611-脑启发式计算>#</a></h4><p>第4章中讨论的Hebbian规则和脉冲时间相关可塑性（STDP）是两个广为人知的生物学现象，长期以来人们一直认为这是大脑学习的基本机制。与此相反，长期以来人们一直批评AI界广泛采用的基于反向传播的梯度下降学习算法在生物学上是不可行的。反向传播起源于数学模型，它是解决人工神经网络（ANN）优化问题的一种高效的方法，它的诞生与生物学神经网络的学习几乎没有关系。尽管长期以来一直在争论，类脑的计算并不一定要精确地模仿生物脑的行为，但Hinton指出，STDP协议可能是在脑中进行梯度下降优化和反向传播的一种方式。这个假设在AI界和神经科学界得到了进一步发展。</p><h4 id=612-新兴的纳米技术>6.1.2 新兴的纳米技术<a hidden class=anchor aria-hidden=true href=#612-新兴的纳米技术>#</a></h4><h4 id=613-神经形态系统的可靠计算>6.1.3 神经形态系统的可靠计算<a hidden class=anchor aria-hidden=true href=#613-神经形态系统的可靠计算>#</a></h4><h4 id=614-人工神经网络和脉冲神经网络的融合>6.1.4 人工神经网络和脉冲神经网络的融合<a hidden class=anchor aria-hidden=true href=#614-人工神经网络和脉冲神经网络的融合>#</a></h4><h3 id=62-结论>6.2 结论<a hidden class=anchor aria-hidden=true href=#62-结论>#</a></h3><h2 id=参考文献-5>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献-5>#</a></h2><h2 id=附录>附录<a hidden class=anchor aria-hidden=true href=#附录>#</a></h2><h2 id=术语表>术语表<a hidden class=anchor aria-hidden=true href=#术语表>#</a></h2><h1 id=附录-1>附录<a hidden class=anchor aria-hidden=true href=#附录-1>#</a></h1><h2 id=新闻报道中科院院士张旭类脑智能是智能时代的新质生产力>新闻报道<a href=https://news.cnstock.com/industry,rdjj-202403-5207288.htm>中科院院士张旭：类脑智能是智能时代的新质生产力</a><a hidden class=anchor aria-hidden=true href=#新闻报道中科院院士张旭类脑智能是智能时代的新质生产力>#</a></h2><p>2024-03-20 10:46:15 </p><p>来源：<a href=http://www.cnstock.com/>中国证券网</a> </p><p>作者：谭镕 记者 邓贞</p><p>上证报中国证券网讯（谭镕 记者 邓贞）“随着智能计算系统趋近算力天花板、能耗激增、通用智能难产，人脑天然具备的低能耗、并行处理、学习性和容错性强等优势愈加凸显。”在3月19日中国金融信息中心举办的以“生成AI 重塑未来”为主题的专精特新高质量发展大讲堂上，中科院院士、广东省智能科学与技术研究院（以下简称“广东省智能院”）院长张旭分享了类脑智能的技术突破和应用图景。</p><p><img loading=lazy src=https://www.cnstock.com/image/202403/20/20240320100936909.png></p><p>图：中科院院士、广东省智能科学与技术研究院院长张旭</p><p>他认为，在一系列研究技术促进下，人类大脑功能联结图谱终将被成功绘制，脑机接口、类脑智能理论和类脑智能技术在未来数年内将成为脑科学和脑医学研究和拓展的重要方向。</p><p>人工智能是新一轮科技革命和产业变革的核心驱动力，发展新质生产力的重要引擎。当前，ChatGPT、Sora等生成式AI技术不断突破，使得大模型赋能千行百业成为可能。</p><p>“过去几年，智能算力需求快速增长。目前，算力产品的使用出现了较大的瓶颈——能耗问题。”张旭表示，人脑有860亿神经元形成神经环路和网络，能耗约20瓦，而相同大小的人工神经网络数字模拟的能耗约8兆瓦。传统计算机的信号-数据转换和高精度计算在能源和时间上产生高成本。复杂的深度学习模型耗费惊人的高训练成本，需要有替代方法。</p><p>“不等同传统的人工智能，也不等同脑仿生，类脑智能是脑科学、神经科学启发的智能科学和技术。”张旭表示，类脑智能采用神经形态计算，通过模仿人类大脑的运作方式，让计算机软硬件实现信息高效处理，同时具有低功耗、高算力的特点。</p><p>张旭介绍，广东省智能院是国内外首家成建制、成体系、全链条研究和开发类脑智能的科研机构，主要开展涉及感知认知神经网络、类脑智能算法与模型、类脑智能计算的基础研究和核心技术研发，同时提供类脑异构融合智能计算为主体的先进智能计算平台。</p><p>2023年底，由广东省智能院粤港澳脑智工程中心转化的珠海横琴新近纪智能科技公司和珠海天琴芯智能科技公司共同发布了以新一代类脑计算架构（LYRArc）和处理芯片（BPU）为技术核心的绿色类脑智能计算系统。据悉，“天琴芯海”单芯片可支持2亿神经元拟态计算，为全球首颗亿级神经元规模的可编程类脑晶圆计算芯片，助力未来类脑计算机系统真正实现“体积更小，容量更大”的集成突破。同时，该团队已成功研制出基于该晶圆芯片的类脑晶圆计算机，未来有望将助力脑科学研究和全脑千亿神经元尺度类脑大模型的研发。</p><p>“算力特别是智能算力，是人类进入智能时代的生产力，对新兴技术发展具有强大助推力。用类脑计算技术，可以实现在一些技术上的突破，来弥补现有智能计算上的一些不足。”张旭说，类脑计算机系统的推出，为解决类脑智能产业化发展的底层核心技术问题提供了新路径，有望在助力大模型训练、脑仿真、工业计算模拟、社交网络分析、金融风控分析等智能产业应用，走出智能计算的“中国新路”。</p><p>据介绍，2023年6月，科技部公布了国家新一代人工智能公共算力开放创新平台名单，由广东省智能院建设的“新一代类脑人工智能公共算力开放创新平台”入列。该创新平台主要依托横琴先进智能计算平台进行规划和建设，重点围绕人工智能、大数据、区块链、高算力芯片、生物医药、金融工商、元宇宙等智能产业，形成类脑智能产业生态圈。</p><p>“类脑正成为脑科学一种新的范式，借鉴脑处理的信息和学习的基本原理，发展高效能、高速、智能的新型类脑计算系统。与传统的AI相比，类脑智能计算的速度快、能耗小，逻辑分析推理能力更强，计算机体积更小，而且有可能形成对计算机架构、智能芯片、智能计算机等多方面颠覆性的创新。”张旭表示，在不久的将来，可以期待类脑智能超级计算机的算力超过人类大脑的算力，或给人类社会带来重大变革。</p><p>本期讲堂由中国中小企业发展促进中心、上海市经济和信息化委员会指导，上海市中小企业发展服务中心、新华社中国金融信息中心主办，宁波银行上海分行、上海股权托管交易中心联合主办，上海市人工智能技术协会协办、上海市企业服务云特别支持。</p><h2 id=知乎文章nature-长文综述类脑智能与脉冲神经网络前沿>知乎文章<a href=https://zhuanlan.zhihu.com/p/94556277>Nature 长文综述：类脑智能与脉冲神经网络前沿</a><a hidden class=anchor aria-hidden=true href=#知乎文章nature-长文综述类脑智能与脉冲神经网络前沿>#</a></h2><h3 id=导语><strong>导语</strong><a hidden class=anchor aria-hidden=true href=#导语>#</a></h3><p>在人工智能如火如荼的今天，基于人脑的“脉冲”（spiking）模拟计算框架下的脉冲神经网络（SNN）、神经形态计算（neuromorphic computing）有望在实现人工智能的同时，降低计算平台的能耗。这一跨学科领域以硅电路实现生物中的神经环路（circuit）为起点，现已发展到包括基于脉冲的编码以及事件驱动表示的算法的硬件实现。</p><p>2019 年 11 月 28 日，普渡大学的 Kaushik Roy、 Akhilesh Jaiswal 和 Priyadarshini Panda 在 Nature 发表长文综述，概述了神经形态计算在算法和硬件方面的发展，介绍了学习和硬件框架的原理。以及神经形态计算的主要挑战以及发展前景，算法和硬件的协同设计等方面的内容。本文是全文翻译。</p><blockquote><p>编译：集智俱乐部翻译组<br>来源：Nature<br>原题：Towards spike-based machine intelligence with neuromorphic computing</p></blockquote><h3 id=前言-2><strong>前言</strong><a hidden class=anchor aria-hidden=true href=#前言-2>#</a></h3><p>纵观历史，创造具有类人脑能力的技术一直都是创新的源泉。从前，科学家们一直以为人脑中的信息是通过不同的通道(channels)和频率传递的，就像无线电一样。如今，科学家们认为人脑就像一台计算机。随着神经网络的发展，今天的计算机已在多个认知任务中展现出了非凡的能力，例如，AlphaGo在围棋战略游戏Go中击败了人类选手。虽然这种表现的确令人印象深刻，但一个关键问题仍然存在：这些活动涉及的计算成本有多大？</p><p>人脑能够执行惊人的任务（例如，同时识别多个目标、推理、控制和移动），而能量消耗只有接近2瓦左右。相比之下，标准计算机仅识别1000种不同的物体就需要消耗250瓦的能量。尽管人脑尚未被探索穷尽，但从神经科学来看，人脑非凡的能力可归结于以下三个基本观察：广泛的连通性、结构和功能化的组织层次、以及时间依赖（time dependent）的神经元突触连接（图1a）。</p><p>神经元（Neurons）是人脑的计算原始元素，它通过离散动作电位（discrete action potentials）或“脉冲”交换和传递信息。突触（synapses）是记忆和学习的基本存储元素。人脑拥有数十亿个神经元网络，通过数万亿个突触相互连接。基于脉冲的时间处理机制使得稀疏而有效的信息在人脑中传递。研究还表明，灵长类动物的视觉系统由分层级的关联区域组成，这些关联区域逐渐将视觉对象的映像转化为一种具有鲁棒性的格式，从而促进了感知能力。</p><p>目前，最先进的人工智能总体上使用的是这种受到人脑层次结构和神经突触框架启发的神经网络。实际上，现代深度学习网络（DLNs）本质上是层级结构的人造物，就像人脑一样用多个层级去表征潜在特征，由来自输入过程中多个图层的不同潜在特征的表征，经过转换形成的（图1b）。硅晶体管硬件计算系统是这种神经网络的硬件基本。大规模计算平台的数字逻辑包含由集成在单个硅芯片上的数十亿个晶体管。这让人联想到了人脑的层级结构：各种硅基计算单元以层级方式排列，以实现高效的数据交换（图1c）。</p><p>尽管两者在表面上有相似之处，但人脑的计算原理和硅基计算机之间存在着鲜明区别。其中包括：（1）计算机中计算（处理单元）和存储（存储单元）是分离的，不同于人脑中计算（神经元）和存储（突触）是一体的；（2）受限于二维连接的计算机硬件，人脑中大量存在的三维连通性目前无法在硅基技术上进行模拟；（3）晶体管主要为了构建确定性布尔（数字）电路开关，和人脑基于脉冲的事件驱动型随机计算不同。尽管如此，在当前的深度学习革命中，硅基计算平台（例如图像处理单元（GPU）云服务器）已成为一个重要的贡献因素。</p><p>但是，使得“通用智能”（包括基于云服务器到边缘设备）无法实现的主要瓶颈是巨大的能耗和吞吐量需求。例如，在一个由典型的2.1Wh电池供能的嵌入式智能玻璃处理器（smart-glass processor）上运行深度网络，就会让处理器在25分钟内将电池消耗殆尽。</p><p>在人脑的指引下，通过脉冲驱动通信从而实现了神经元-突触计算的硬件系统将可以实现节能型机器智能。神经形态计算始于20世纪80年代晶体管仿照神经元和突触的功能运作（图2），之后其迅速演化到包括事件驱动的计算本质（离散的“脉冲”人造物）。最终，在21世纪初期，这种研究努力促进了大规模神经形态芯片的出现。</p><p><img loading=lazy src=https://pic1.zhimg.com/v2-91568fa6a8b9dfd74fd01506ddbd7208_1440w.jpg></p><p><strong>图1：生物和硅基计算的关键属性构架。a，大脑的组织原理示意图。神经元和突触与时间脉冲处理交织在一起的网络使得不同区域之间的信息能够快速高效地流动。b，一个深度卷积神经网络物体执行目标检测的图片。这些网络是多层的，并使用突触存储和神经元非线性学习广泛的数据表示。使用反向传播训练后，每层学习的特征都显示有趣的模式。第一层学习一般特征，如边缘和颜色斑点。随着我们深入网络，学习到的功能变得更具体，用对象的部分（如狗的眼睛或鼻子）代表完整的物体（如狗的脸）。这种从一般到特殊的过渡代表了视觉皮层的层次结构。c，最先进的硅计算生态系统。广义上讲，计算层次分为处理单元和内存存储。处理单元和内存层次结构的物理分离导致众所周知“内存墙瓶颈（memory well bottleneck）” 。当今的深度神经网络在强大的云服务器上训练，尽管会产生巨大的能耗，但仍可提供令人惊叹的精度。</strong></p><p>今天，算法设计师们正在积极探索（特别是“学习”）脉冲驱动型计算的优缺点，去推动有可扩展性、高能效的“脉冲神经网络”（spiking neural networks ，SNN）。在这种情况下，我们可以将神经形态计算领域描述为一种协同工作，它在硬件和算法域两者中权重相同，以实现脉冲型人工智能。我们首先强调了“智能”（算法）方面，包括不同的学习机制（无监督以及基于脉冲的监督，或梯度下降方案），同时突出显示了要利用基于时空事件的表征。本文讨论的重点是视觉相关的应用任务，例如图像识别和检测。然后我们将探索“计算”（硬件）方面，包括模拟计算、数字神经运动系统，它们都超越了冯·诺依曼（数字计算系统的最新架构）和芯片技术（代表了基本的场效应晶体管设备，它们是当下计算平台的基础）。最后，我们将讨论算法的硬件协同设计前景，说明算法具有用于对抗硬件漏洞的鲁棒性，可以实现能耗和精度之间的最佳平衡。</p><h3 id=一算法展望脉冲神经网络_><strong>一、算法展望：脉冲神经网络_</strong><a hidden class=anchor aria-hidden=true href=#一算法展望脉冲神经网络_>#</a></h3><h4 id=脉冲神经网络><strong>脉冲神经网络</strong><a hidden class=anchor aria-hidden=true href=#脉冲神经网络>#</a></h4><p>按照神经元功能，Maass开创性的论文将神经网络分为三个代际。首先，第一代被称为McCulloch–Pitt感知机，它执行阈值运算并输出数字（1、0）。基于sigmoid单元或修正线性单元（ReLU），第二代神经元单元增加了连续非线性，使其能够计算一组连续的输出值。第一代和第二代网络之间的非线性升级在扩展神经网络向复杂应用和更深度的实现方面起着关键作用。当前的DLNs在输入和输出之间具有多个隐藏层，都是基于第二代神经元。实际上，由于它们连续的神经元功能，这些模型可以支持基于梯度下降的反向传播学习，这也是目前训练深度神经网络的标准算法。</p><p><img loading=lazy src=https://picx.zhimg.com/v2-0123d5ad7f267256f380e6f5482d161b_1440w.jpg></p><p><strong>图2：智能计算的重大发现和进展时间表（从1940年代到当代）。硬件方面，我们有从两个角度展示发现：一是对神经形态计算的启迪，或通过硬件创新实现类人脑的计算和“智能”；另一方面是对计算效率的启发，或者实现更快、更节能的布尔计算。从算法的角度来看，我们已指出这些发现是出于理解人脑的动机，受到神经科学和生物科学的驱动，并同时致力于实现人工智能，它由工程和应用科学所驱动。请注意，这张图并不是完整或全面的清单。“当前研究”并不一定意味着过去没有对这些努力进行探索；相反，我们强调了该领域正在进行和有希望研究的关键方面。</strong></p><p>第三代神经网络主要使用“整合放电”（integrate-and-fire）型尖峰神经元，通过脉冲交换信息（图3）。第二代和第三代神经网络之间最大的区别在于信息处理性质。第二代神经网络使用了实值计算(real-value)（例如，信号振幅），而SNN则使用信号的时间（脉冲）处理信息。脉冲本质上是二进制事件，它或是0或是1。如图3a所示，SNNs中的神经元单元只有在接收或发出尖峰信号时才处于活跃状态，因此它是事件驱动型的，因此可以使其节省能耗。若无事件发生SNNs单元则保持闲置状态，这与DLNs相反。无论实值输入和输出，DLNs所有单位都处于活跃状态。此外，SNN中的输入值为1或0，这也减少了数学上的点积运算ΣiVi×wi（图3a），减小了求和的计算量。</p><p>针对不同的生物保真度水平下产生的脉冲代际，相关的尖峰神经元模型已被提出。例如泄漏整合放电型（LIF）（图3b）和霍奇金-赫克斯利型（Hodgkin–Huxley）。同样，针对于突触的可塑性，已有例如赫布型（Hebbian）和非赫布型]（non-Hebbian）方案。突触的可塑性即突触权重的调节（在SNNs中转化为学习）取决于突触前和突触后尖峰的相对时间（图3c）。神经形态工程师的一个主要目标是：在利用基于事件（使用基于事件的传感器）及数据驱动更新的同时，建立一个具有适当突触可塑性的脉冲神经元模型，从而实现高效的识别、推理等智能应用。</p><p>我们认为，SNNs最大的优势在于其能够充分利用基于时空事件的信息。今天，我们有相当成熟的神经形态传感器，来记录环境实时的动态改变。这些动态感官数据可以与SNNs的时间处理能力相结合，以实现超低能耗的计算。实际上，与传统上DLNs使用的帧驱动（frame-driven）的方法相比，SNNs将时间作为附加的输入维度，以稀疏的方式记录了有价值的信息（图3），从而实现高效的SNNs框架，并通过计算视觉光流或立体视觉来实现深度感知。结合基于脉冲的学习规则，它可以产生有效的训练。机器人研究者已经证明使用基于事件的传感器进行跟踪和手势识别的优势。但是，这些应用程序大多数都使用了DLNs来执行识别。</p><p>在此类传感器中使用SNNs主要受限于缺乏适当的训练算法，从而可以有效地利用尖峰神经元的时间信息。实际上就精度而言，在大多数学习任务中SNNs的效果仍落后于第二代的深度学习。很明显，尖峰神经元可以实现非连续的信息传递，并发出不可微分的离散脉冲（见图3），因此它们不能使用基于梯度下降型的反向传播技术，而这是传统神经网络训练的基础。</p><p>另外，SNNs还受限于基于脉冲的数据可用性。虽然理想情况要求SNNs的输入是带有时间信息的序列，但SNNs训练算法的识别性能是在现有静态图像的数据集上进行评估的，例如CIFAR或ImageNet。然后，此类基于静态帧的数据将通过适当的编码技术（例如速率编码或次序编码，见图3d）转换为脉冲序列。虽然编码技术使我们能够评估SNNs在传统基准数据集上的性能，但我们要超越静态图像分类的任务。SNNs的最终能力应当来自于它们处理和感知瞬息万变的现实世界中的连续输入流，就像人脑轻而易举所做的那样。目前，我们既没有良好的基准数据集，也没有评估SNNs实际性能的指标。收集更多适当的基准数据集的研究，例如动态视觉传感器数据或驾驶和导航实例，便显得至关重要。</p><p>（这里我们指的是作为DLNs的第二代连续神经网络，以区别于基于脉冲的计算。我们注意到SNNs可以在具有卷积层次结构的深度架构上实现，并同时执行尖峰神经元功能。）</p><p><img loading=lazy src=https://pic3.zhimg.com/v2-28f0af0eac71426e65e918be2f689fa2_1440w.jpg></p><p><img loading=lazy src=https://pic1.zhimg.com/v2-5a15aab006a20c7e73db1315302f8d90_1440w.jpg></p><p><img loading=lazy src=https://pic1.zhimg.com/v2-cb1a2733f2caf6133411a27cacf830c4_1440w.jpg></p><p><strong>图3：SNN计算模型。a. 由输入上游神经元驱动的下游神经元组成的神经网络。上游神经尖峰Vi通过突触权重wi调节，在给点时间内产生合成电流ΣiVi×wi（相当于点积运算）。产生的合成电流会影响下游神经元的膜电位。B. LIF尖峰神经元的动力学显示。在没有脉冲的情况下，膜电位Vmem在时间常数τ中集成了传入脉冲和泄漏。当Vmem超过阈值Vthresh时，下游神经元输出脉冲。随之产生不应期，在此期间后神经元的Vmem不再受到影响。c，显示了基于实验数据的脉冲时间依赖的可塑性（STDP）公式，其中a +，a-，τ+和τ-是控制权重变化Δw的学习率和时间常数。突触权重wi根据上游神经元与下游神经元尖峰的时间差（Δt= tpost − tpre）更新。d，使用速率编码将输入图像（静态帧数据）转换为脉冲在各个时间步长上的映射。每个像素产生一个泊松脉冲序列，其激发速率与像素强度成正比。当几个时间步求和得出脉冲映射时（标记为t = 5的脉冲映射是从t = 1到t = 5的映射总和），它们开始类似于输入。因此，基于脉冲的编码既保留了输入图像的完整性，并且在时域中对数据进行了二值化。结果显示，LIF行为和随机输入尖峰的产生使SNN的内部动力学具有随机性。注意，序列编码也可以用来生成脉冲数据。</strong></p><h3 id=二在snns中学习算法><strong>二、在SNNs中学习算法</strong><a hidden class=anchor aria-hidden=true href=#二在snns中学习算法>#</a></h3><h4 id=基于转换的方法><strong>基于转换的方法</strong><a hidden class=anchor aria-hidden=true href=#基于转换的方法>#</a></h4><p>这种方法的思路是获得一个SNN，对给定的任务，该SNN将产生与深度神经网络相同的输入输出映射。它的基本原理是，使用权重调整（weight rescaling）和归一化方法将训练有素的DLN转换为SNN，将非线性连续输出神经元的特征和尖峰神经元的泄漏时间常数（leak time constants），不应期（refractory period）、膜阈值（membrane threshold）等功能相匹配。</p><p>迄今为止，在图像分类的大型脉冲网络中（包括ImageNet数据集），这种方法能够产生了有竞争力的精确度。在基于转换的方法中，其优点是免除了时域中的训练负担。DLN使用了已有的深度学习框架例如Tensorflow对基于帧的数据进行训练，这些工具提供了训练中的灵活性。这种转换首先需要解析在基于事件的数据（通过对静态图像数据集进行速率编码获得）上进行训练的DLN，之后再进行简单的转换。</p><p>但是，这种方法有其内在的局限性。例如在使用双曲线正切（tanh）或归一化指数函数（softmax）后，非线性神经元的输出值可以得正也可以得负，而脉冲神经元的速率只能是正值。因此，负值总被丢弃，导致转换后的SNNs的精度下降。转换的另一个问题是在不造成严重的性能损失的前提下获得每一层最佳。最近的研究提出了确定最佳放电率的实用解决方案，以及在DLNs的训练过程中引入其他约束（例如噪音或泄漏修正线性单元（leaky ReLUs））以更好地匹配尖脉冲神经元的放电率。今天，转换的方法可为图像识别任务提供最先进的精度，并与DLNs的分类性能相当。值得注意的是，从DLNs转换的SNNs的推理时间变得很长（约几千个时间步长），导致延迟增加、能耗增加。</p><h4 id=基于脉冲的方法><strong>基于脉冲的方法</strong><a hidden class=anchor aria-hidden=true href=#基于脉冲的方法>#</a></h4><p>在基于脉冲的方法中，SNN使用时间信息进行训练，因此在整体脉冲动力学中具备明显的稀疏性和高效率优势。研究人员采用了两种主要方向：无监督学习（没有标记数据的训练），以及监督学习（有标记数据的训练）。早期监督学习成果是ReSuMe和tempotron，它们证明了在单层的SNN中，可以使用脉冲时间依赖的可塑性（STDP）的变体去进行分类。从那时起，研究工作一直致力于整合基于脉冲且类似于全局反向传播的误差梯度下降法，以便在多层SNNs中实现监督学习。大多数依赖反向传播的成果为脉冲神经元功能估计了一个近似可微的函数，从而使其能够执行梯度下降法（图4a）。SpikeProp及其相关变体已派生出通过在输出层固定一个目标脉冲序列来实现SNNs的反向传播规则。最近的成果对实值膜电位使用随机梯度下降法，是为了让正确输出神经元随机激发更多的脉冲（而不是具有精确目标的脉冲序列）。这些方法在深度卷积SNNs的小规模图像识别任务上取得了最新进展，例如美国国家标准与技术研究所（MNIST）手写数字数据库的数字分类。</p><p>然而，尽管计算效率更高，监督学习在大型任务的精度上无法超过基于转换的方法。另一方面，受到神经科学和硬件效率为主要目标的启发，基于STDP学习规则的局部无监督SNN训练也很有意思。通过局部学习（我们将在后面的硬件讨论中看到），有机会使记忆（突触存储）和计算（神经元输出）更紧密地相结合。这种架构更像人脑，也适合节能芯片上实现。Diehl等人率先证明了完全无监督的SNN学习，其精度可与 MNIST数据库深度学习相媲美（图4b）。</p><p>但是，将局部学习方法扩展到多层复杂任务是一个挑战。随着网络的深入，神经元的放电率会降低，我们称之为“消失的前向脉冲传播”。为了避免这种情况，多数工作用逐层的方式训练多层SNN（包括卷积SNNs）在局部的脉冲学习模式，然后进行全局学习反向传播学习，以去进行分类。这样局部和全局相结合的方法尽管很有成效，但在分类精度方面仍落后于转换的方法。此外，最近的成果显示了概念验证，即通过深度SNNs中反馈连接错误信号的随机投影确实有助于改善学习。这种基于反馈的学习方法需要进一步研究，以评估其在大规模任务上的效果。</p><h4 id=对二元制学习的启示><strong>对二元制学习的启示</strong><a hidden class=anchor aria-hidden=true href=#对二元制学习的启示>#</a></h4><p>我们可以通过仅用二进制（1/0）位值，而不是需要额外的内存的16位或32位浮点值来获得超低能耗和高效的计算。实际上在算法层级，目前正在研究以概率方式学习（关于神经元何时随机突跳，权重的转换精度何时变低）获得参数较少的网络和计算操作。二元和三元的DLNs也被提出，其神经元输出和权重只取低精度值-1、0和+1，而且在大规模分类任务中表现良好。基于二进制脉冲处理模式，SNN已具有计算优势。此外，LIF神经元的神经元动力学中的随机性可以顾及外部噪声（例如，有噪声的输入或有噪声权重的硬件参数），来提高网络的鲁棒性。那么，我们是否可以用结合此SNN时间处理架构使用适当的学习方法，并将权重训练压缩为二进制，使精度损失最小，还有待研究。</p><p><img loading=lazy src=https://pic3.zhimg.com/v2-7915a9f7beebf43c1bf57d6a0e241d56_1440w.jpg></p><p><strong>图4：尖峰网络中的全局和局部学习原理。a，在已知目标标签指导下进行全局学习，T用于分类任务。给定一个前馈网络，网络通过隐藏层单位A向前传播输入值X，并输出神经元激活值Y。结合非线性变换ƒ（Z1），使用输入的加权求和可以计算隐藏层的A，用矩阵符号表示为Z1 = W1 TX。输出以类似的方式进行计算。然后，用误差相对于权重（W1，W2）的导数E求出随后的权重更新。前向和反向传播的迭代导致学习。误差求导需要ƒ&rsquo;，这要求ƒ&rsquo;是连续的。因此，基于脉冲的反向传播的规则近似LIF函数可微方案。基于时间信息的处理细节没有显示在这里。b，局部STDP数字分类无监督学习。给定一个两层拓扑结构，输入层与输出层的所有神经元完全连接，通过STDP学习突触连接。根据输入层和输出层神经元的尖峰时间差异进行权重调整。输入神经元在输出之前（或之后）激发，权重值将增加（或减小）。随着迭代在多个时间步长上进行训练，权重在初始化时随机赋值，通过训练它将学习对一类输入所示的样式进行通用表示的编码（在这种情况下为“ 0”，“ 1”和“ 2”）。这里，为了进行识别，目标标签是不需要的。</strong></p><h3 id=三其他有待研究的方向><strong>三、其他有待研究的方向</strong><a hidden class=anchor aria-hidden=true href=#三其他有待研究的方向>#</a></h3><h4 id=超越视觉任务><strong>超越视觉任务</strong><a hidden class=anchor aria-hidden=true href=#超越视觉任务>#</a></h4><p>到目前为止，我们已经给出了大多数分类任务处理的办法，那么如何处理在静态图像上识别和推理以外的任务呢？SNN也可以处理序列数据，但是并没有研究论证SNN在处理NLP的能力。使用SNN做因果推断的能力又如何呢？深度学习研究员在强化学习领域作出了大量的研究[62,63]，不过使用SNN进行强化学习研究的却很少。在SNN这一领域——特别是在训练学习算法中——SNN所面临的最大挑战就是否能表现出和深度学习相当的性能。尽管深度学习已经设下了很高的竞争门槛，但是我们相信SNN会在机器人、自主控制等领域表现的更好。</p><h4 id=终身学习和小样本学习><strong>终身学习和小样本学习</strong><a hidden class=anchor aria-hidden=true href=#终身学习和小样本学习>#</a></h4><p>深度学习模型在长期学习时会出现灾难性遗忘现象。比如，学习过任务A的神经网络在学习任务B时，它会忘记学过的任务A，只记得B。如何在动态的环境中像人一样具备长期学习的能力成为了学术界关注的热点。这固然是深度学习研究的一个新的方向，但我们应该探究给SNN增加额外的时间维度是否有助于实现持续性学习型任务。另一个类似的任务就是，利用少量数据进行学习，这也是SNN能超过深度学习的领域。SNN中的无监督学习可以与提供少量数据的监督学习相结合，只使用一小部分标记的训练数据得到高效的训练结果[ 46,50,65 ]。</p><h4 id=与神经科学建立联系><strong>与神经科学建立联系</strong><a hidden class=anchor aria-hidden=true href=#与神经科学建立联系>#</a></h4><p>我们可以和神经科学的研究成果相结合，把这些抽象的结果应用到学习规则中，以此提高学习效率。例如，Masquelier等人[65]利用STDP和时间编码模拟视觉神经皮层，他们发现不同的神经元能学习到不同的特征，这一点类似于卷积层学到不同的特征。研究者把树突学习[66]和结构可塑性[67]结合起来，把树突的连接数做为一个超参数，以此为学习提供更多的可能。SNN领域的一项互补研究是<a href="https://zhida.zhihu.com/search?content_id=109142176&amp;content_type=Article&amp;match_order=1&amp;q=LSM&amp;zhida_source=entity">LSM</a>（liquid state machines）[68]。LSM利用的是未经训练、随机链接的递归网络框架，该网络对序列识别任务表现卓著[ 69–71]。但是在复杂的大规模任务上的表现能力仍然有待提高。</p><h3 id=四硬件展望><strong>四、硬件展望</strong><a hidden class=anchor aria-hidden=true href=#四硬件展望>#</a></h3><p>从前文对信息处理能力和脉冲通信的描述中，我们容易假设一套具备类似能力的硬件系统。这套系统能够成为SNN的底层计算框架。受到在生物大脑中无处不在的神经元和突触的启发，设计出紧密结合在一起的计算和记忆结构；以及实现更复杂的功能——例如，使用最少的电路元件来模拟神经元与突触动力学。</p><h4 id=神经形态计算的出现><strong>神经形态计算的出现</strong><a hidden class=anchor aria-hidden=true href=#神经形态计算的出现>#</a></h4><p>在20世纪80年代，晶体管发明了40年后，在生物神经系统领域，Carver Mead设想了"更智能"、“更高效”的硅基计算机结构[72,73]。他也表示过自己最初试图建立神经系统的尝试是“简单而愚蠢的”[74]。但是他的工作代表了计算硬件领域的一种新的范式。Mead并不在意AND、OR等布尔运算[74]。相反他利用金属氧化物硅（MOS）晶体管在亚阈值区的电气物理特性（电压-电流指数相关）来模拟指数神经元的动力学特征[72]。这样的设备-通路协同设计是神经形态计算中最有趣的领域之一。</p><h4 id=并行gpu的出现><strong>并行GPU的出现</strong><a hidden class=anchor aria-hidden=true href=#并行gpu的出现>#</a></h4><p>和CPU这种由一个或者多个处理复杂任务的芯片组成的计算核心不同，GPU[75]是由多个可以进行并行计算的简单计算核心组成。因此能完成高并发、高吞吐的任务。在传统意义上GPU是加速图形应用程序的硬件加速器，但是现在有许多的非图形应用都受益于GPU的特性。深度学习就是一个显著的例子[6]，其实GPU不仅是深度神经网络的首选平台，也是SNN训练的平台[76,77]。虽然GPU在高扩展性上具备优势，但无法很好的用来进行基于事件驱动的脉冲计算。因此，事件驱动的“超级大脑”神经芯片就可以提供高效的解决方案[78,79]。</p><h4 id=超级大脑芯片><strong>“超级大脑”芯片</strong><a hidden class=anchor aria-hidden=true href=#超级大脑芯片>#</a></h4><p>“超级大脑”芯片[80]的特点是整合了百万计的神经元和突触，神经元和突触提供了脉冲计算的能力[78,81–86]。Neurogrid[82]和TrueNorth[84]分别是基于混合信号模拟电路和数字电路的两种模型芯片。Neurogrid使用数字电路，因为模拟电路容易积累错误，且芯片制造过程中的错误影响也较大。设计神经网络旨在帮助科学家模拟大脑活动，通过复杂的神经元运作机制——比如离子通道的开启和关闭，以及突触特有的生物行为[82,87]。相比而言，TrueNorth作为一款神经芯片，目的是用于重要商业任务，例如使用SNN分类识别任务；而且TrueNorth是基于简化的神经科突触原型来设计的。</p><p>以TrueNorth为例，主要特性如下[78,88]：</p><p>异步地址事件表示（Asynchronous address event representation）：首先，异步地址事件表示不同于传统的芯片设计，在传统的芯片设计中，所有的计算都按照全局时钟进行，但是因为SNN是稀疏的，仅当脉冲产生时才要进行计算，所以异步事件驱动的计算模式更加适合进行脉冲计算[89,90]。</p><p>芯片网络：**芯片网络（networks-on-chip，NOCs）**可以用于脉冲通信，NOC就是芯片上的路由器网络，通过时分复用技术用总线收发数据包。大规模芯片必须使用NOC，是因为在硅片加工的过程中，连接主要是二维的，在第三个维度灵活程度有限。也要注意到，尽管使用了NOC但芯片的联通程度，仍然不能和大脑中的三维连通性相比。包括TrueNorth在内的大规模数字神经芯片，比如Loihi[78]，已经展示除了SNN技术以外的应用效果。使得我们能更加接近生物仿真技术。不过，有限的连通性，NOC总线带宽的限制，和全数字方法仍然需要进一步的研究。</p><h4 id=超越冯诺依曼式计算><strong>超越冯·诺依曼式计算</strong><a hidden class=anchor aria-hidden=true href=#超越冯诺依曼式计算>#</a></h4><p>晶体管尺寸规模的持续行缩小的现象被称之为摩尔定律[91]，摩尔定律推动了CPU和GPU以及“超级大脑”芯片的不断发展。不过近些年，随着硅基晶体管接近物理极限，这一发展速度放缓[92]。为了适应现代人类对计算能力的不断提升，研究者们设计出了一种双管齐下的方法，使得“超冯·诺依曼”、“超硅”计算模型成为了可能，冯·诺依曼模型的一大特征就是，存储单元和运算单元的分离[93]。通过系统总线传输数据。因此，数据在高速的运算单元和低速的存储单元之间的频繁传输就成为了众所周知的**“存储墙瓶颈”（memory wall bottleneck）**。这一瓶颈限制了计算的吞吐和效率[94]。</p><p><img loading=lazy src=https://pic3.zhimg.com/v2-66227fc2a4ed3d7a4cf5750321814a86_1440w.jpg></p><p><strong>图5：一些有代表性的“超级大脑”芯片和AER方法</strong></p><p><strong>A，Neurogrid拥有65,000多个神经元和5亿个突触，而TrueNorth拥有100万神经元和2.56亿个突触。Neurogrid和TrueNorth分别使用树和网格路由拓扑两种结构。Neurogrid使用模拟混合信号设计，TrueNorth依赖数字基元。一般来说，像TrueNorth 这样的数字神经形态系统将神经元的膜电位表示为n位二进制格式。通过适当增加或减少n位字来实现神经元动力学，比如LIF行为。相比之下，模拟系统将膜电位表示为存储在电容中的电荷。通过电流从电容进出，模拟所需的神经元动力学。尽管存在电路差异，但一般来说，模拟系统和数字系统都使用事件驱动AER进行尖峰通信。事件驱动通信是实现低能耗大规模集成系统的关键。</strong></p><p><strong>B，基本的 AER通信系统。每当发射端送出一个事件（一个脉冲），相应的地址就通过数据总线发送到接收端。接收端解码输入地址，并重新构造脉冲的序列。因此，每个脉冲由其位置（地址）显式编码，并在其地址发送到数据总线时隐式编码。</strong></p><p>减轻这一瓶颈影响的方法就是使用“近内存（near-memory）”、“内存中”计算[95,96]。近内存计算是通过在内存单元附近嵌入一个专门的处理器，由此实现内存和计算的“共存”。实际上，各种“超级大脑芯片”的分布式计算体系结构所具有的紧密放置的神经元和突触阵列就是近内存计算的表现。相比较而言，内存中计算则是把部分计算操作嵌入到内存内部或外部电路中。</p><h4 id=非易失性技术><strong>非易失性技术</strong><a hidden class=anchor aria-hidden=true href=#非易失性技术>#</a></h4><p><strong>非易失性技术（ non-volatile technology）</strong>[97–103]通常被用于与生物突触相比较。实际上，它们展示了生物突触的两个特征：<strong>突触效能（synaptic efficacy）<strong>和</strong>突触可塑性（ synaptic plasticity）</strong>。突触可塑性指的是根据特定的学习规则调整突触权重的能力。突触效能指的是根据输入脉冲产生输出的现象。以最简单的形式来说，意思就是，输入的脉冲信号乘以突触的权重。这表示着可编程、模拟、非易失性。从上游神经元得到的信号，相乘再求和后再作用于下游神经元的输入。后文的图片就说明了，如何使用新兴的**非易失性忆阻技术（ non-volatile memristive technology）**实现突触效率和突触可塑性[103,104]。而且，还可以通过时间驱动NOC的方式了连接开关，从而构建密集大规模的神经处理器，以实现内存中计算。</p><p>一些已经发表过的基于忆阻技术的研究[105,106]，比如<strong>可变电阻式内存（RRAM）[107]</strong>、<strong>相变内存（PCM）</strong>[108]、 和<strong>自旋传递扭矩磁性随机读写存储器（STT-MRAM）</strong>[109]，已经在原位点积和基于 STDP 规则的突触学习中进行了探索。RRAM（氧化物基和导电桥基[107]）是电场驱动器件，依靠丝极的形成来模拟可编程电阻。RRAM容易出现因设备、因周期的变化[110,111]，这是这一技术的主要障碍。PCMS包括夹在两个电极之间的硫系材料，可以在非晶态（高电阻）和晶态（低电阻）之间转换。PCM设备有类似的编程电压和RRAM的写入速度，不过这种器件也会受到高写入电流和长时间电阻漂移的影响[108]。自旋电子器件是由两块垫片隔开的磁铁组成，依据两层的磁化方向是平行还是反平行，能呈现两种电磁状态。与RRAM和PCM相比，自旋装置显示出几乎无限的耐久性，更低的写入能量和更快的磁极反转速度[109]。然而，在自旋器件中，两个极端电阻态（ON/OFF）的比率要比在PCM和RRAM中小得多。</p><p>另一类包含可调非易失性电阻的非易失性器件是<strong>浮栅晶体管（floating-gate transistor）</strong>。此类设备有作为突触存储器的潜力[112–114]。实际上浮栅晶体管是第一个用作非易失性突触存储器的设备[115,116]。因为，他们与MOS制造工艺的相容，比其他新型器件的生产技术更加成熟，然而，与其他非易失性技术相比，浮栅晶体管的主要缺点是耐用性低和编程电压高。</p><p>虽然**原位计算（situ computing）**和突触学习为大规模超越冯·诺依曼分布式计算提供了诱人的前景，但有许多的挑战仍然有待克服，因设备、因周期和进程相关引起的变化，计算的近似性质容易出现错误，从而减低整体的计算效率，最终影响准确性。此外，交叉开关操作的鲁棒性受到电流潜通路、线电阻、驱动电路的源电阻和感测电阻的存在的影响[117,118]。选择器（晶体管或双端非线性装置）的非理想性、对模拟-数字转换设备的要求和有限的比特精度要求，也增加了使用非传统突触装置设计可靠计算的总体复杂性。此外，写入非易失性设备通常会消耗大量的资源。而且，此类设备的固有随机性会导致不可靠的写操作，又需要代价高昂的检验方案[119]。</p><p><img loading=lazy src=https://pic2.zhimg.com/v2-ec81993c84d5dd17340111c987a2cde5_1440w.jpg></p><p><strong>图6：使用非易失性存储设备作为突触存储器件。A，各种非易失性技术的原理图：PCM，RRAM，STT-MRAM和浮栅晶体管。这种非易失性设备已经被用作突触存储和原位神经突触计算，以及通用非神经形态存储器内加速器。B，用忆阻技术实现突触效能和突触可塑性。图中展示了以交叉方式连接的忆阻器阵列。根据欧姆定律，水平线（绿色）上的输入脉冲产生与忆阻元件电导成比例的电流。又因为基尔霍夫电流定律，与通过多个峰前神经元的电流沿着垂直线（黑色）相加。这一操作表示了具备突触功效的内存中点积运算。通常，只要神经元前和后尖峰分别在水平线和垂直线上(如在STDP)，突触可塑性就可以通过适当地施加电压脉冲来原位实现。组成忆阻器的电阻值是根据在相应的水平和垂直线上产生的电压差编程的。根据特定的器件技术，再选择要编程电压脉冲的形状和定时。请注意，浮栅晶体管是三端子器件，因此需要额外的水平线和（或）垂直线来实现交叉开关功能。该图还显示了以平铺方式连接NOC的忆阻阵列，由此可以实现高吞吐量的原位计算。</strong></p><h4 id=硅内存中计算><strong>硅（内存中）计算</strong><a hidden class=anchor aria-hidden=true href=#硅内存中计算>#</a></h4><p>除了非易失性技术之外，各种使用标准硅存储器（包括静态和动态随机存取存储器）进行内存计算的设想均在研究中。这些工作主要集中在把布尔向量计算嵌入内存数组中[120–122]。此外，混合信号模拟内存计算操作和二进制卷积操作最近被证明是可行的[123,124]。事实上，目前几乎所有主要的内存技术都在探索各种形式的内存计算，包括静态内存[125]和动态内存[126]、RRAM[127]、PCM[128]和STT-MRAM[129]。尽管这些工作大部分集中在常见的计算应用上，如加密和深度神经网络，但他们也可以轻松的一直到SNN上来。</p><h3 id=五算法-硬件协同设计><strong>五、算法-硬件协同设计</strong><a hidden class=anchor aria-hidden=true href=#五算法-硬件协同设计>#</a></h3><h4 id=混合信号模拟计算><strong>混合信号模拟计算</strong><a hidden class=anchor aria-hidden=true href=#混合信号模拟计算>#</a></h4><p>模拟计算容易受到过程引起的变化和噪声的影响，并且由于模拟和数字转换设备的复杂性和精度要求，在面积和能耗方面就受到了很大限制。将芯片学习与紧密结合的模拟计算框架相结合，将使这类系统能够从根本上适应过程引起的变化，从而减轻对精度的影响。在过去[130,131]以及近期的可行性生物算法的研究[54]中，已经研究了以**芯片上（ on-chip）<strong>和</strong>设备上（on-device）**学习解决方案为重点的局部学习。本质上，无论是局部学习这种形式还是树突学习这种范式，我们都认为，更好的容错局部学习算法——即使是要学习额外的参数——将是推动模拟神经形态计算的关键所在。此外，芯片上学习的适应能力可在不降低目标精度的前提下，开发低成本的近似模数转换器。</p><h4 id=忆阻点积><strong>忆阻点积</strong><a hidden class=anchor aria-hidden=true href=#忆阻点积>#</a></h4><p>作为模拟计算的一个实例，**忆阻点积（Memristive dot products）**是实现原位神经形态计算的一种有前景的方法。不幸的是，表示点积的忆阻阵列中产生的电流既有空间依赖性又有数据依赖性，这使得交叉开关电路分析成为一个非常复杂的问题。研究交叉开关电路非理想状态的影响[117,132,133]，探索减轻点积不准确影响的训练方法的研究并不多[118,134]。而且，这些工作大部分集中在深度神经网络而不是SNN中。然而，我们可以合理地假设，在这些工作中开发出的基本器件和对电路的见解也能用于SNN的实现。现有的工作需要精致的的设备-通路模拟运行，必须与训练算法紧密耦合，以减少精度损失。我们认为，基于最新设备的交叉开关阵列的理论模型，以及为点积误差建立理论边界的努力，都将引起人们的关注。这将使算法设计者无需耗时、设计迭代设备-通路-算法模拟，就能探索新的训练算法，同时也能解决硬件不一致的问题。</p><h4 id=随机性><strong>随机性</strong><a hidden class=anchor aria-hidden=true href=#随机性>#</a></h4><p>随机SNN引起了人们极大的兴趣，这是因为新兴的设备本身具有随机性[135,136]。随机二进制SNN 的实现结果，大多数都集中在MNIST数字识别之类的小规模任务上[56]。这类工作的共同主题是使用随机STDP类的本地学习规则来生成权重更新。我们认为，即使在二元条件下，STDP学习中的时间维度权重更新提供了额外的带宽，是的整体朝着真确的方向前进（总体精度）。这种二元局部学习方案与基于梯度下降的大规模学习规则相结合，在利用了硬件随机性的同时，为高能效神经系统提供了有利的机会。</p><h4 id=混合设计方法><strong>混合设计方法</strong><a hidden class=anchor aria-hidden=true href=#混合设计方法>#</a></h4><p>我们认为，基于混合方法的硬件解决方案——即在单一平台上结合各种技术的优势——是另一个需要深入研究的重要领域。这种方法可以在最近的文献中找到[137]，比如，把低精度忆阻器与高精度数字处理器结合使用。这种混合方法有许多可能的变体，包括显著驱动的计算数据分离、混合精度计算[137]、将常规硅存储器重新配置为需内存近似加速器[125]、局部同步和全局异步设计[138]、局部模拟和全局数字系统；其中新兴技术和传统技术可以同时使用，以提高精确度和效率。此外，这种混合硬件可以与基于混合脉冲学习的方法结合使用，例如局部无监督学习，然后是全局有监督反向传播算法[53]。我们认为，这种局部-全局学习方案可以用来降低硬件复杂性，同时，最大限度的减少对终端应用程序的性能影响。</p><h3 id=六总-结><strong>六、总 结</strong><a hidden class=anchor aria-hidden=true href=#六总-结>#</a></h3><p>如今，“智能化”已经成为了我们周围所有学科的主题。在这方面，本文阐述了神经形态计算作为一种高效方式，通过硬件（计算）和算法（智能）的协同演化的方式来实现机器智能。</p><p>我们首先讨论了脉冲神经范式的算法含义，这种范式使用事件驱动计算，而不是传统深度学习范式中的数值计算。描述了实现标准分类任务的学习规则（例如基于脉冲的梯度下降、无监督STDP和从深度学习到脉冲模型的转换方法）的优点和局限性。</p><p>未来的算法研究应该利用基于脉冲信号的信息处理的稀疏和时间动态特性；以及可以产生实时识别的互补神经形态学数据集；硬件开发应该侧重于事件驱动的计算、内存和计算单元的协调，以及模拟神经突触的动态特征。特别引人关注的是新兴的非易失性技术，这项技术支持了原位混合信号的模拟计算。我们也讨论了包含算法-硬件协同设计的跨层优化的前景。例如，利用算法适应性（局部学习）和硬件可行性（实现随机脉冲）。</p><p>最后，我们谈到，基于传统和新兴设备构建的基于脉冲的节能智能系统与当前无处不在的人工智能相比，二者的前景其实是相吻合的。现在是我们该交换理念的时候了，通过设备、通路、架构和算法等多学科的努力，通力合作打造一台真正节能且智能的机器。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://rosefinch-midsummer.github.io/zh/tags/%E7%B1%BB%E8%84%91%E6%99%BA%E8%83%BD/>类脑智能</a></li></ul><nav class=paginav><a class=prev href=https://rosefinch-midsummer.github.io/zh/posts/course/harvardcs50-this-is-cs50x/><span class=title>« 上一頁</span><br><span>Harvard《CS50: This is CS50x》</span>
</a><a class=next href=https://rosefinch-midsummer.github.io/zh/posts/pastime/%E6%B8%B8%E6%88%8F%E8%B5%84%E6%BA%90%E6%90%9C%E9%9B%86%E5%99%A8/><span class=title>下一頁 »</span><br><span>游戏资源搜集器</span></a></nav></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>评论</span><br></div><div id=tcomment></div><script src=https://utteranc.es/client.js repo=Rosefinch-Midsummer/comments_of_blog issue-term=title theme=github-light crossorigin=anonymous async></script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.body.className.includes("dark")?"github-light":"photon-dark",t={type:"set-theme",theme:e},n=document.querySelector(".utterances-frame");n.contentWindow.postMessage(t,"https://utteranc.es")})</script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://rosefinch-midsummer.github.io/zh/>天漢帝國復興錄</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>本站总访问量<span id=busuanzi_value_site_pv></span>次
</span><span id=busuanzi_container_site_uv>本站访客数<span id=busuanzi_value_site_uv></span>人次</span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>