<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã | Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ</title><meta name=keywords content="MIT,AI"><meta name=description content="MIT How to AI(Almost) Everything
ÂâçË®Ä
Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ
ËØæÁ®ãËµÑÊñôÈìæÊé•
Course Description
Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more."><meta name=author content="RM"><link rel=canonical href=https://rosefinch-midsummer.github.io/zh/posts/course/mit%E5%A6%82%E4%BD%95ai%E5%87%A0%E4%B9%8E%E6%89%80%E6%9C%89%E4%BA%8B%E7%89%A9/><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://rosefinch-midsummer.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rosefinch-midsummer.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rosefinch-midsummer.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://rosefinch-midsummer.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://rosefinch-midsummer.github.io/img/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://rosefinch-midsummer.github.io/zh/posts/course/mit%E5%A6%82%E4%BD%95ai%E5%87%A0%E4%B9%8E%E6%89%80%E6%9C%89%E4%BA%8B%E7%89%A9/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://rosefinch-midsummer.github.io/zh/posts/course/mit%E5%A6%82%E4%BD%95ai%E5%87%A0%E4%B9%8E%E6%89%80%E6%9C%89%E4%BA%8B%E7%89%A9/"><meta property="og:site_name" content="Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ"><meta property="og:title" content="MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã"><meta property="og:description" content="MIT How to AI(Almost) Everything ÂâçË®Ä Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ
ËØæÁ®ãËµÑÊñôÈìæÊé•
Course Description Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more."><meta property="og:locale" content="zh-cn#en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-29T18:34:25+08:00"><meta property="article:modified_time" content="2025-11-29T21:54:22+08:00"><meta property="article:tag" content="MIT"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã"><meta name=twitter:description content="MIT How to AI(Almost) Everything
ÂâçË®Ä
Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ
ËØæÁ®ãËµÑÊñôÈìæÊé•
Course Description
Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"üìöÊñáÁ´†","item":"https://rosefinch-midsummer.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"üè´Ë™≤Á®ã","item":"https://rosefinch-midsummer.github.io/zh/posts/course/"},{"@type":"ListItem","position":3,"name":"MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã","item":"https://rosefinch-midsummer.github.io/zh/posts/course/mit%E5%A6%82%E4%BD%95ai%E5%87%A0%E4%B9%8E%E6%89%80%E6%9C%89%E4%BA%8B%E7%89%A9/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã","name":"MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã","description":"MIT How to AI(Almost) Everything ÂâçË®Ä Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ\nËØæÁ®ãËµÑÊñôÈìæÊé•\nCourse Description Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more.\n","keywords":["MIT","AI"],"articleBody":"MIT How to AI(Almost) Everything ÂâçË®Ä Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ\nËØæÁ®ãËµÑÊñôÈìæÊé•\nCourse Description Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more.\nThrough lectures, readings, discussions, and a significant research component, this course will develop critical thinking skills and intuitions when applying AI to new data modalities, knowledge of recent technical achievements in AI, and a deeper understanding of the AI research process.\nCourse Info Instructor Prof. Paul Liang Departments Media Arts and Sciences Content Introduction Multisensory intelligence: Creating human-AI symbiosis across scales and sensory mediums to enhance productivity, creativity, and wellbeing.\nCourse Overview AI for new modalities: data, modeling, evaluation, deployment Multimodal AI: connecting multiple different data sources Learning Objectives Study recent technical achievements in AI research Improve critical and creative thinking skills Understand future research challenges in AI Explore and implement new research ideas in AI Preferred Pre-requisites Some knowledge of programming (ideally in Python) Some basic understanding of modern AI capabilities \u0026 limitations Bring external (non-AI) domain knowledge about your problem Bonus: worked on AI for some modality Research Projects on New Modalities Motivation: Many tasks of real-world impact go beyond image and text.\nChallenges:\nAI with non-deep-learning effective modalities (e.g., tabular, time-series) Multimodal deep learning + time-series analysis + tabular models AI for physiological sensing, IoT sensing in cities, climate and environment sensing Smell, taste, art, music, tangible and embodied systems Potential models and dataset to start with\nBrain EEG Signal: https://arxiv.org/abs/2306.16934 Speech: https://arxiv.org/pdf/2310.02050.pdf Facial Motion: https://arxiv.org/abs/2308.10897 Tactile: https://arxiv.org/pdf/2204.00117.pdf Research Projects on AI Reasoning Motivation: Robust, reliable, interpretable reasoning in (multimodal) LLMs.\nChallenges:\nFine-grained and compositional reasoning Neuro-symbolic reasoning Emergent reasoning in foundation models Potential models and dataset to start with\nCan LLMs actually reason and plan? Code for VQA: CodeVQA: https://arxiv.org/pdf/2306.05392.pdf, VisProg: https://prior.allenai.org/projects/visprog, Viper: https://viper.cs.columbia.edu/ Cola: https://openreview.net/pdf?id=kdHpWogtX6Y NLVR2: https://arxiv.org/abs/1811.00491 Reference games: https://mcgill-nlp.github.io/imagecode/, https://github.com/AlabNII/onecommon, https://dmg-photobook.github.io/ Research Projects on Interactive Agents Motivation: Grounding AI models in the web, computer, or other virtual worlds to help humans with digital tasks.\nChallenges:\nWeb visual understanding is quite different from natural image understanding Instructions and language grounded in web images, tools, APIs Asking for human clarification, human-in-the-loop Search over environment and planning Potential models and dataset to start with\nWebArena: https://arxiv.org/pdf/2307.13854.pdf AgentBench: https://arxiv.org/pdf/2308.03688.pdf ToolFormer: https://arxiv.org/abs/2302.04761 SeeAct: https://osu-nlp-group.github.io/SeeAct/ Research Projects on Embodied and Tangible AI Motivation: Building tangible and embodied AI systems that help humans in physical tasks.\nChallenges:\nPerception, reasoning, and interaction Connecting sensing and actuation Efficient models that can run on hardware Understanding influence of actions on the world (world model) Potential models and dataset to start with\nVirtual Home: http://virtual-home.org/paper/virtualhome.pdf Habitat 3.0 https://ai.meta.com/static-resource/habitat3 RoboThor: https://ai2thor.allenai.org/robothor LangSuite-E: https://github.com/bigai-nlco/langsuite Language models and world models: https://arxiv.org/pdf/2305.10626.pdf Research Projects on Socially Intelligent AI Motivation: Building AI that can understand and interact with humans in social situations.\nChallenges:\nSocial interaction, reasoning, and commonsense. Building social relationships over months and years. Theory-of-Mind and multi-party social interactions. Potential models and dataset to start with\nMultimodal WereWolf: https://persuasion-deductiongame.socialai-data.org/ Ego4D: https://arxiv.org/abs/2110.07058 MMToM-QA: https://openreview.net/pdf?id=jbLM1yvxaL 11866 Artificial Social Intelligence: https://cmu-multicomp-lab.github.io/asi-course/spring2023/ Research Projects on Human-AI Interaction Motivation: What is the right medium for human-AI interaction? How can we really trust AI? How do we enable collaboration and synergy?\nChallenges:\nModeling and conveying model uncertainty ‚Äì text input uncertainty, visual uncertainty, multimodal uncertainty? cross-modal interaction uncertainty? Asking for human clarification, human-in-the-loop, types of human feedback and ways to learn from human feedback through all modalities. New mediums to interact with AI. New tasks beyond imitating humans, leading to collaboration. Potential models and dataset to start with\nMMHal-Bench: https://arxiv.org/pdf/2309.14525.pdf aligning multimodal LLMs HACL: https://arxiv.org/pdf/2312.06968.pdf hallucination + LLM Research Projects on Ethics and Safety Motivation: Large AI models are can emit unsafe text content, generate or retrieve biased images.\nChallenges:\nTaxonomizing types of biases: text, vision, audio, generation, etc. Tracing biases to pretraining data, seeing how bias can be amplified during training, fine-tuning. New ways of mitigating biases and aligning to human preferences. Potential models and dataset to start with\nMany works on fairness in LLMs -\u003e how to extend to multimodal? Mitigating bias in text generation, image-captioning, image generation Introduction to AI and AI research Introduction to AI and AI research Generating ideas, reading and writing papers, AI experimentation How Do We Get Research Ideas?\nTurn a concrete understanding of existing research‚Äôs failings to a higher-level experimental question. ‚Ä¢ Bottom-up discovery of research ideas ‚Ä¢ Great tool for incremental progress, but may preclude larger leaps\nMove from a higher-level question to a lower-level concrete testing of that question.\n‚Ä¢ Top-down design of research ideas ‚Ä¢ Favors bigger ideas, but can be disconnected from reality\nBeware ‚ÄúDoes X Make Y Better?‚Äù ‚ÄúYes‚Äù\nThe above question/hypothesis is natural, but indirect\nIf the answer is ‚Äúno‚Äù after your experiments, how do you tell what‚Äôs going wrong? Usually you have an intuition about why X will make Y better (not just random)\nCan you think of other research questions/ hypotheses that confirm/falsify these assumptions\nHow to do Literature Review and Read a Paper?\nGoogle scholar Papers with code, Github, Huggingface Recent conference proceedings Blog posts Survey papers, tutorials, courses Testing Research Ideas\nGather and process dataset, visualize data, gather labels, do data splits. Implement the most simple pipeline and get it working. -\u003e Pipeline = data loading + basic model + eval function + loss/visualization/deployment Change one component of the model at a time, repeat x10 (top-down or bottom-up). Find what works best, and exploit. Scale up experiments, repeat across multiple datasets. Careful ablation studies. Qualitative comparisons and visualizations. Repeat until successful. How to Write a Paper\nPrepare a 15min talk (with figures, examples, tables, etc.) Convert the talk into a paper. More resources\nhttps://github.com/pliang279/awesome-phd-advice https://github.com/jbhuang0604/awesome-tips https://www.cs197.seas.harvard.edu/ https://medium.com/spotprobe/the-hexagon-of-ideas-02e5b770d75e Module 1: Foundations of AI Data, structure, and information Lecture Outline:\nVision, language, audio, sensing, set, graph modalities Modality profile Types of data and labels Common learning objectives and generalization Most of AI is about learning abstractions, or representations, from data.\nModality Profile:\nElement representations: Discrete, continuous, granularity Element distributions: Density, frequency Structure: Temporal, spatial, latent, explicit Information: Abstraction, entropy Noise: Uncertainty, noise, missing data Relevance: Task, context dependence Summary: How To Data\nDecide how much data to collect, and how much to label (costs and time) Clean data: normalize/standardize, find noisy data, anomaly/outlier detection Visualize data: plot, dimensionality reduction (PCA, t-sne), cluster analysis Decide on evaluation metric (proxy + real, quantitative and qualitative) Choose model class and learning algorithm (more next lecture) Huggingface Tutorial ‚ÄúHuggingface‚Äù is a set of multiple packages transformers: Provides API to initialize large pretrained models datasets: Provides easy way to download datasets Not from Huggingface but often used together bitsandbytes: Provides functions to quantize large models flash-attn: Allows the model to run faster with less memory Some terms to keep in mind LoRA: Adapter to train large models with less memory Bfloat16: Robust half precision representation often used to save memory The Recipe Become one with the Data Set up end-to-end skeleton and get dumb baselines Overfit to diagnose errors Regularize for better generalization ‚óè Add more real data ‚Äì the best way to reduce overfitting\n‚óè Use data augmentation and pretraining\n‚óè Reduce input dimensions and model size\n‚óè Techniques: Dropout, weight decay, early stopping Tune hyperparameters ‚óè Prefer random search over grid search\n‚óè Use Bayesian optimization tools when available\n‚óè Don‚Äôt overcomplicate ‚Äì start with simple models Squeeze out final improvements How to Design ML Models for New Data Look at the data first For simple, low dimensional data, start with simple models (SVM, Random Forest, Shallow MLP/CNN) For vision/language data, try pretrained model Start simple, then add complexity. Simple ones can be used as baselines. How to Debug Your Model Look at the data first. Is the input data \u0026 label correct? Ensure no data leakage; Look at the outputs. Is model only predicting one label? Label imbalance: Data Augmentation; loss scaling Look at the training loss Loss is nan: Inspect weights and inputs for NaN values. Make sure weights are initialized. LLM: Use bfloat16 instead of float16. Loss not changing: Model underfitting. Increase learning rate; decrease weight decay; Add more complexity; Use better optimizer*. Look at Loss (Continued) Loss highly varied/increasing: Decrease learning rate; Gradient Clipping; Use better Optimizers Look at train vs val accuracy (or any other metrics) Train ¬ª Val: Model overfitting. More weight decay, reduce model complexity, data augmentation, get more data Train ‚âà Val ‚âà 100%: Check for data leakage Personal tip: I recommend trying second order optimizers from packages like Heavyball\nCommon model architectures Lecture Outline\nA unifying paradigm of model architectures Temporal sequence models Spatial convolution models Models for sets and graphs Summary: How To Model\nDecide how much data to collect, and how much to label (costs and time) Clean data: normalize/standardize, find noisy data, anomaly/outlier detection Visualize data: plot, dimensionality reduction (PCA, t-sne), cluster analysis Decide on evaluation metric (proxy + real, quantitative and qualitative) Choose modeling paradigm - domain-specific vs general-purpose Figure out base elements and their representation Figure out data invariances \u0026 equivariances (+other parts of modality profile) Iterate between data collection, model design, model training, hyperparameter tuning etc. until satisfied. Discussion 1: Learning and generalization Learning the Bitter Lesson Unifying Grokking and Double Descent Generalization in Neural Networks Textbooks are all you Need A Conceptual Pipeline for Machine Learning Module 2: Foundations of multimodal AI Multimodal connections and alignment Modality refers to the way in which something expressed or perceived.\nA research-oriented definition‚Ä¶ Multimodal is the science of heterogeneous and interconnected(Connected + Interacting) data.\nHeterogeneous Modalities: Information in different modalities shows diverse qualities, structures, \u0026 representations.\nChallenge 1: Representation\nDefinition: Learning representations that reflect cross-modal interactions between individual elements, across different modalities This is a core building block for most multimodal modeling problems!\nChallenge 2: Alignment\nDefinition: Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure.\nSub-challenges:\nDiscrete connections: Explicit alignment (e.g., grounding) Contextualized representation: Implicit alignment + representation Continuous alignment: Granularity of individual elements Challenge 2a: Discrete Alignment\nDefinition: Identify and model connections between elements of multiple modalities\nChallenge 2b: Continuous Alignment\nDefinition: Model alignment between modalities with continuous signals and no explicit elements\nChallenge 3: Reasoning\nDefinition: Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure.\nChallenge 4: Generation\nDefinition: Learning a generative process to produce raw modalities that reflects cross-m\nChallenge 5: Transference\nDefinition: Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resources.\nChallenge 6: Quantification\nDefinition: Empirical and theoretical study to better understand heterogeneity, cross-modal interactions, and the multimodal learning process.\nDiscussion 2: Modern AI architectures Scaling Laws for Generative Mixed-Modal Models Not All Tokens Are What You Need for Pretraining PaLI: A Jointly-Scaled Multilingual Language-Image Model The Evolution of Multimodal Model Architectures An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale A ConvNet for the 2020s Inductive Representation Learning on Large Graphs Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs Multimodal interactions and fusion Visual-and-Language Transformer (ViLT) (‚âà BERT + ViT)\nALBEF: Align Before Fusion 32 (‚âà BERT + ViT + CLIP-ish)\nDiscussion 3: Multimodal alignment The Platonic Representation Hypothesis What Makes for Good Views for Contrastive Learning? Understanding the Emergence of Multimodal Representation Alignment Does equivariance matter at scale? Learning Transferable Visual Models From Natural Language Supervision? Emerging Properties in Self-Supervised Vision Transformers Foundations \u0026 trends in multimodal machine learning - Principles, challenges, and open questions Cross-modal transfer Transference\nDefinition: Transfer knowledge between modalities, usually to help the primary modality which may be noisy or with limited resources\nPart 1: Transfer via Pretrained Models\nDefinition: Transferring knowledge from large-scale pretrained models to downstream tasks involving\nPart 2: Co-learning\nDefinition: Transferring information from secondary to primary modality by sharing representation spaces between both modalities.\nCo-learning via Alignment\nDefinition: Transferring information from secondary to primary modality by sharing representation spaces between modalities\nRepresentation alignment: word embedding space for zero-shot visual classification\nCo-learning via Translation\nDefinition: Transferring information from secondary to primary modality by using the secondary modality as a generation target.\nPart 3: Model Induction\nModel Induction ùë¶1 ùë¶2 Definition: Keeping individual unimodal models separate but inducing common behavior across separate models.\nDiscussion 4: Multimodal interactions Multimodal interaction: A review Quantifying \u0026 Modeling Multimodal Interactions: An Information Decomposition Framework Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think! Kosmos-2: Grounding Multimodal Large Language Models to the World Chameleon: Mixed-modal early-fusion foundation models MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts Module 3: Large models and modern AI Pre-training, scaling, fine-tuning LLMs Today‚Äôs Agenda\nHistory of LLMs, RNNs vs Transformer Pretraining of LLMs Types of Architecture Instruction Finetuning \u0026 Preference Tuning Efficient Training Practical Tips Scaling Laws:\nBigger model allows models to reach a better performance given sufficient compute Over training models getting popular nowadays Limitations of RL + Reward Modeling\nHuman preferences are unreliable Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth This can result in making up facts + hallucinations Reward Model doesn‚Äôt always reflect humans‚Äô preferences \u0026 may have unintended behaviors Efficient Training: LoRA / Efficient low rank adaptation\nTraining the whole model takes a lot of compute and GPU memory Solution: Freeze the model, train a small adapter that updates with a low-rank decomposition Efficient Training: Mixture of Experts\nTrain multiple parallel networks (experts) simultaneously During each forward pass, only activate k experts Saves compute \u0026 GPU memory Deepseek R1: 671B, only 37B activated, performance on par with OpenAI o1-mini Efficient Inference: Quantization\nRange Clipping Scale \u0026 Shift Convert to lower bits Calibration Practical Tips: How to Instruction Finetune an LLM\nData Preparation: Convert your data to conversation format Choosing a good starting point Secure compute \u0026 Finetune the model Evaluation \u0026 Deployment Large multimodal models Today‚Äôs lecture\nMultimodal foundation models and pre-training Adapting LLMs into multimodal LLMs From text to multimodal generation Latest directions: natively multimodal, multimodal MoE, real-world modalities Part 1: Multimodal foundation model representations of text, video, audio\nPart 2: Adapting large language models for multimodal text generation\nPart 3: Enabling text and image generation\nVisual-and-Language Transformer (ViLT)\nPre-training datasets\nLargest dataset is DataComp. It has 12.8 billion image-text pairs. Recent efforts shifted more towards filtering for high quality multimodal data. Examples include DFN (2B), COYO (600M), and Obelics (141M)\nNative Multimodal Models\nBackground Non-native VLMs: Image encoder paired with frozen trained LLM. The image encoder can either be frozen or trained. Most VLMs now use this structure. Native Multimodal Modals: LLMs Trained from scratch with multimodal input Late fusion: Image patches -\u003e Image Encoder -\u003e Linear -\u003e LLM. Early fusion: Image patches -\u003e Linear -\u003e LLM (No image encoder!) Scaling Laws for Native Multimodal Models\nEarly fusion models hold small advantage on small scales. On larger scales, both architectures perform similarly. (We don‚Äôt actually need image encoders!) NMMs scale similarly to unimodal LLMs, with slightly varying scaling exponents depending on the target data type and training mixture Sparse structure like MOE significantly benefits NMMs at the same inference cost In an MOE structure, Modality-aware design (having separate image/text experts) performs worse than modality-agnostic design (unified experts for both image/text tokens) Discussion 5: Large language models LoRA: Low-Rank Adaptation of Large Language Models Gated Linear Attention Transformers with Hardware-Efficient Training Unintended Impacts of LLM Alignment on Global Representation A Visual Guide to Quantization Scaling Instruction-Finetuned Language Models Modern generative AI Todays Lecture\nWhat are Generative Models? Current State of the Art (Flow Matching) Conditional Generation Architectures Tips to Train these Models ### Discussion 6: Large multimodal models\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference ModaVerse: Efficiently Transforming Modalities with LLMs Spider: Any-to-Many Multimodal LLM SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites NExT-GPT: Any-to-Any Multimodal LLM Learning to rebalance multi-modal optimization by adaptively masking subnetworks Module 4: Interactive AI Discussion 7: Generative AI Large Language Diffusion Models Compositional Generative Modeling: A Single Model is Not All You Need Flow Matching for Generative Modeling Flow Matching Guide and Code FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-to-Motion Generation MusFlow: Multimodal Music Generation via Conditional Flow Matching Unraveling the Connections Between Flow Matching and Diffusion Probabilistic Models Exploring Diffusion and Flow Matching Under Generator Matching Multi-step reasoning Today‚Äôs lecture\nMultimodal reasoning: Solving hard problems by breaking them down into step-by-step reasoning steps in multiple modalities AI agents Human-AI interaction Ethics and safety Models: Multimodal fusion and generation\nData: Hard challenges + human reasoning steps\nTraining: Reinforcement learning for emergent reasoning\nHuman: Trustworthy, safe, controllable\nMultimodal Reasoning\nPart 1: Multimodal foundation model representations of text, video, audio\nPart 2: Adapting large language models for multimodal text generation\nPart 3: Enabling text and image generation\nPart 4: Human-AI interaction\nInteractive and embodied AI Today‚Äôs lecture\nBasics of reinforcement learning Modern RL for LLM alignment and reasoning Interactive LLM agents Tips and Training for Reinforcement Learning\nSanity Check with Fixed Policy Monitor KL Divergence (in PPO-like algorithms) Plot Entropy Over Time Use Greedy Rollouts for Evaluation Debug Value Function Separately: Visualize predicted vs. actual return Gradient Norm Clipping is Crucial Check Advantage Distribution Train on a Frozen Replay Buffer Use Curriculum Learning: Gradually increase task difficulty or reward sparsity Watch for Mode Collapse in MoE or Multi-Head Policies Human-AI interaction and safety Interactive Agents\nMultisensory agents for the web and digital automation\nExample task: Purchase a set of earphones with at least 4.5 stars in rating and ship it to me.\nEmbodied Agents\nGenerate precise robotics control directly via trained vision language models.\nHuman-AI interaction\nWhat medium(s) is most intuitive for human-AI interaction? especially beyond language prompting 1 What new technical challenges in AI have to be solved for human-AI interaction? quantification What new opportunities arise when integrating AI with the human experience? productivity, creativity, wellbeing Quantification\nDefinition: Empirical and theoretical studies to better understand model shortcomings and predict and control model behavior.\nQuantification - Safety\nEasy to generate biased and dangerous content with language models!\nBut there exist ways to ‚Äòjailbreak‚Äô the safety measures in aligned LLMs\nReadings Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions\nMultimodal Machine Learning: A Survey and Taxonomy\nRepresentation Learning: A Review and New Perspectives\nMachine learning: Trends, Perspectives, and Prospects\nRepresentation Learning: A Review and New Perspectives\nFoundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges\nA Recipe for Training Neural Networks\nFine-tuning a Code LLM on Custom Code on a single GPU\nMAS.S60 Pytorch Introduction\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAttention Is All You Need\nNeural Machine Translation by Jointly Learning to Align and Translate\nDeep Sets\nGraph Attention Networks\nLearning the Bitter Lesson\nUnifying Grokking and Double Descent\nGeneralization in Neural Networks\nTextbooks are all you Need\nA Conceptual Pipeline for Machine Learning\nFoundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions\nWhat Makes for Good Views for Contrastive Learning?\nCharacterization and classification of semantic image-text relations\nWhen and why vision-language models behave like bags-of-words, and what to do about it?\nScaling Laws for Generative Mixed-Modal Models\nNot All Tokens Are What You Need for Pretraining\nPaLI: A Jointly-Scaled Multilingual Language-Image Model\nThe Evolution of Multimodal Model Architectures\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nA ConvNet for the 2020s\nInductive Representation Learning on Large Graphs\nJanossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs\nTen Myths of Multimodal Interaction\nMultimodal interaction: A review\nQuantifying \u0026 Modeling Multimodal Interactions: An Information Decomposition Framework\nDoes my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!\nThe Platonic Representation Hypothesis\nWhat Makes for Good Views for Contrastive Learning?\nUnderstanding the Emergence of Multimodal Representation Alignment\nDoes equivariance matter at scale?\nLearning Transferable Visual Models From Natural Language Supervision?\nEmerging Properties in Self-Supervised Vision Transformers\nFoundations \u0026 trends in multimodal machine learning - Principles, challenges, and open questions\nLLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day\nDreamLLM: Synergistic Multimodal Comprehension and Creation\nPaLM-E: An Embodied Multimodal Language Model\nMultimodal interaction: A review\nQuantifying \u0026 Modeling Multimodal Interactions: An Information Decomposition Framework\nDoes my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!\nKosmos-2: Grounding Multimodal Large Language Models to the World\nChameleon: Mixed-modal early-fusion foundation models\nMM1: Methods, Analysis and Insights from Multimodal LLM Pre-training\nMoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts\nTraining Compute-Optimal Large Language Models\nSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks\nLoRA: Low-Rank Adaptation of Large Language Models\nA Visual Guide to Mixture of Experts (MoE)\nA Visual Guide to Quantization\nImproved Baselines with Visual Instruction Tuning\nQuantifying \u0026 Modeling Multimodal Interactions: An Information Decomposition Framework\nMultimodal Transformer for Unaligned Multimodal Language Sequences\nMasked Autoencoders Are Scalable Vision Learners\nScaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models\nTransfer between Modalities with MetaQueries\nLoRA: Low-Rank Adaptation of Large Language Models\nGated Linear Attention Transformers with Hardware-Efficient Training\nUnintended Impacts of LLM Alignment on Global Representation\nA Visual Guide to Quantization\nScaling Instruction-Finetuned Language Models\nScalable Diffusion Models with Transformers\nFlow Matching in Latent Space\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis\nMovie Gen: A Cast of Media Foundation Models\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\nCobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference\nModaVerse: Efficiently Transforming Modalities with LLMs\nSpider: Any-to-Many Multimodal LLM\nSPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models\nHow Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites\nNExT-GPT: Any-to-Any Multimodal LLM\nLearning to rebalance multi-modal optimization by adaptively masking subnetworks\nLarge Language Diffusion Models\nCompositional Generative Modeling: A Single Model is Not All You Need\nFlow Matching for Generative Modeling\nFlow Matching Guide and Code\nFlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-to-Motion Generation\nMusFlow: Multimodal Music Generation via Conditional Flow Matching\nUnraveling the Connections Between Flow Matching and Diffusion Probabilistic Models\nExploring Diffusion and Flow Matching Under Generator Matching\nDeep reinforcement learning from human preferences\nDeepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning\nFaulty reward functions in the wild\nDirect preference optimization: Your language model is secretly a reward model\nInteractive Sketchpad: A Multimodal Tutoring System for Collaborative, Visual Problem-Solving\nVideoWebArena: Evaluating Multimodal Agents on Video Understanding Web Tasks\nOpenVLA: An Open-Source Vision-Language-Action Model\nArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs\nGuidelines for Human-AI Interaction\n","wordCount":"3995","inLanguage":"zh","datePublished":"2025-11-29T18:34:25+08:00","dateModified":"2025-11-29T21:54:22+08:00","author":{"@type":"Person","name":"RM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rosefinch-midsummer.github.io/zh/posts/course/mit%E5%A6%82%E4%BD%95ai%E5%87%A0%E4%B9%8E%E6%89%80%E6%9C%89%E4%BA%8B%E7%89%A9/"},"publisher":{"@type":"Organization","name":"Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ","logo":{"@type":"ImageObject","url":"https://rosefinch-midsummer.github.io/img/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rosefinch-midsummer.github.io/zh/ accesskey=h title="Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ (Alt + H)">Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rosefinch-midsummer.github.io/zh/ title=üè†‰∏ªÈ†Å><span>üè†‰∏ªÈ†Å</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/posts title=üìöÊñáÁ´†><span>üìöÊñáÁ´†</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/search title="üîçÊêúÁ¥¢ (Alt + /)" accesskey=/><span>üîçÊêúÁ¥¢</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/archives title=‚è±ÊôÇÈñìËª∏><span>‚è±ÊôÇÈñìËª∏</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/categories title=üß©ÂàÜÈ°û><span>üß©ÂàÜÈ°û</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/tags title=üîñÊ®ôÁ∞Ω><span>üîñÊ®ôÁ∞Ω</span></a></li><li><a href=https://rosefinch-midsummer.github.io/zh/about title=üôãüèª‚Äç‚ôÇÔ∏èÈóú‰∫é><span>üôãüèª‚Äç‚ôÇÔ∏èÈóú‰∫é</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://rosefinch-midsummer.github.io/zh/>È¶ñÈ†Å</a>&nbsp;¬ª&nbsp;<a href=https://rosefinch-midsummer.github.io/zh/posts/>üìöÊñáÁ´†</a>&nbsp;¬ª&nbsp;<a href=https://rosefinch-midsummer.github.io/zh/posts/course/>üè´Ë™≤Á®ã</a></div><h1 class=post-title>MIT„ÄäÂ¶Ç‰ΩïAIÔºàÂá†‰πéÔºâÊâÄÊúâ‰∫ãÁâ©„Äã</h1><div class=post-meta>ÂàõÂª∫: 2025-11-29 |
Êõ¥Êñ∞: 2025-11-29 |
Â≠óÊï∞: 3995Â≠ó |
Êó∂Èïø: 8ÂàÜÈíü |
RM</div><div class=meta-item>&nbsp¬∑&nbsp
<span id=busuanzi_container_page_pv>Êú¨ÊñáÈòÖËØªÈáè<span id=busuanzi_value_page_pv></span>Ê¨°</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>ÁõÆÈåÑ</span></summary><div class=inner><ul><li><a href=#mit-how-to-aialmost-everything aria-label="MIT How to AI(Almost) Everything">MIT How to AI(Almost) Everything</a><ul><li><a href=#%e5%89%8d%e8%a8%80 aria-label=ÂâçË®Ä>ÂâçË®Ä</a></li><li><a href=#course-description aria-label="Course Description">Course Description</a></li><li><a href=#course-info aria-label="Course Info">Course Info</a><ul><ul><ul><li><a href=#instructor aria-label=Instructor>Instructor</a></li><li><a href=#departments aria-label=Departments>Departments</a></li></ul></ul></ul></li></ul></li><li><a href=#content aria-label=Content>Content</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a><ul><li><a href=#course-overview aria-label="Course Overview">Course Overview</a></li><li><a href=#learning-objectives aria-label="Learning Objectives">Learning Objectives</a></li><li><a href=#preferred-pre-requisites aria-label="Preferred Pre-requisites">Preferred Pre-requisites</a></li><li><a href=#research-projects-on-new-modalities aria-label="Research Projects on New Modalities">Research Projects on New Modalities</a></li><li><a href=#research-projects-on-ai-reasoning aria-label="Research Projects on AI Reasoning">Research Projects on AI Reasoning</a></li><li><a href=#research-projects-on-interactive-agents aria-label="Research Projects on Interactive Agents">Research Projects on Interactive Agents</a></li><li><a href=#research-projects-on-embodied-and-tangible-ai aria-label="Research Projects on Embodied and Tangible AI">Research Projects on Embodied and Tangible AI</a></li><li><a href=#research-projects-on-socially-intelligent-ai aria-label="Research Projects on Socially Intelligent AI">Research Projects on Socially Intelligent AI</a></li><li><a href=#research-projects-on-human-ai-interaction aria-label="Research Projects on Human-AI Interaction">Research Projects on Human-AI Interaction</a></li><li><a href=#research-projects-on-ethics-and-safety aria-label="Research Projects on Ethics and Safety">Research Projects on Ethics and Safety</a></li><li><a href=#introduction-to-ai-and-ai-research aria-label="Introduction to AI and AI research">Introduction to AI and AI research</a></li></ul></li><li><a href=#module-1-foundations-of-ai aria-label="Module 1: Foundations of AI">Module 1: Foundations of AI</a><ul><li><a href=#data-structure-and-information aria-label="Data, structure, and information">Data, structure, and information</a><ul><li><a href=#huggingface-tutorial aria-label="Huggingface Tutorial">Huggingface Tutorial</a></li><li><a href=#the-recipe aria-label="The Recipe">The Recipe</a></li><li><a href=#how-to-design-ml-models-for-new-data aria-label="How to Design ML Models for New Data">How to Design ML Models for New Data</a></li><li><a href=#how-to-debug-your-model aria-label="How to Debug Your Model">How to Debug Your Model</a></li></ul></li><li><a href=#common-model-architectures aria-label="Common model architectures">Common model architectures</a></li><li><a href=#discussion-1-learning-and-generalization aria-label="Discussion 1: Learning and generalization">Discussion 1: Learning and generalization</a></li></ul></li><li><a href=#module-2-foundations-of-multimodal-ai aria-label="Module 2: Foundations of multimodal AI">Module 2: Foundations of multimodal AI</a><ul><li><a href=#multimodal-connections-and-alignment aria-label="Multimodal connections and alignment">Multimodal connections and alignment</a></li><li><a href=#discussion-2-modern-ai-architectures aria-label="Discussion 2: Modern AI architectures">Discussion 2: Modern AI architectures</a></li><li><a href=#multimodal-interactions-and-fusion aria-label="Multimodal interactions and fusion">Multimodal interactions and fusion</a></li><li><a href=#discussion-3-multimodal-alignment aria-label="Discussion 3: Multimodal alignment">Discussion 3: Multimodal alignment</a></li><li><a href=#cross-modal-transfer aria-label="Cross-modal transfer">Cross-modal transfer</a></li><li><a href=#discussion-4-multimodal-interactions aria-label="Discussion 4: Multimodal interactions">Discussion 4: Multimodal interactions</a></li></ul></li><li><a href=#module-3-large-models-and-modern-ai aria-label="Module 3: Large models and modern AI">Module 3: Large models and modern AI</a><ul><li><a href=#pre-training-scaling-fine-tuning-llms aria-label="Pre-training, scaling, fine-tuning LLMs">Pre-training, scaling, fine-tuning LLMs</a></li><li><a href=#large-multimodal-models aria-label="Large multimodal models">Large multimodal models</a></li><li><a href=#discussion-5-large-language-models aria-label="Discussion 5: Large language models">Discussion 5: Large language models</a></li><li><a href=#modern-generative-ai aria-label="Modern generative AI">Modern generative AI</a></li></ul></li><li><a href=#module-4-interactive-ai aria-label="Module 4: Interactive AI">Module 4: Interactive AI</a><ul><li><a href=#discussion-7-generative-ai aria-label="Discussion 7: Generative AI">Discussion 7: Generative AI</a></li><li><a href=#multi-step-reasoning aria-label="Multi-step reasoning">Multi-step reasoning</a></li><li><a href=#interactive-and-embodied-ai aria-label="Interactive and embodied AI">Interactive and embodied AI</a></li><li><a href=#human-ai-interaction-and-safety aria-label="Human-AI interaction and safety">Human-AI interaction and safety</a></li></ul></li><li><a href=#readings aria-label=Readings>Readings</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=mit-how-to-aialmost-everything>MIT <em>How to AI(Almost) Everything</em><a hidden class=anchor aria-hidden=true href=#mit-how-to-aialmost-everything>#</a></h1><h2 id=ÂâçË®Ä>ÂâçË®Ä<a hidden class=anchor aria-hidden=true href=#ÂâçË®Ä>#</a></h2><p>Êú¨Èó®ËØæÁ®ã‰∏ªË¶ÅËÆ≤ÁöÑÊòØÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºåËÄå‰∏çÊòØAIÂ∫îÁî®ÊñπÂºè„ÄÇ</p><p><a href=https://mit-mi.github.io/how2ai-course/spring2025/schedule/>ËØæÁ®ãËµÑÊñôÈìæÊé•</a></p><h2 id=course-description>Course Description<a hidden class=anchor aria-hidden=true href=#course-description>#</a></h2><p>Artificial Intelligence (AI) holds great promise as a technology to enhance digital productivity, physical interactions, overall well-being, and the human experience. To enable the true impact of AI, these systems will need to be grounded in real-world data modalities, from language-only systems to vision, audio, sensors, medical data, music, art, smell, and taste. This course will introduce the basic principles of AI (focusing on modern deep learning and foundation models) and how we can apply AI to novel real-world data modalities. In addition, we will introduce the principles of multimodal AI that can process many modalities at once, such as connecting language and multimedia, music and art, sensing and actuation, and more.</p><p>Through lectures, readings, discussions, and a significant research component, this course will develop critical thinking skills and intuitions when applying AI to new data modalities, knowledge of recent technical achievements in AI, and a deeper understanding of the AI research process.</p><h2 id=course-info>Course Info<a hidden class=anchor aria-hidden=true href=#course-info>#</a></h2><h5 id=instructor>Instructor<a hidden class=anchor aria-hidden=true href=#instructor>#</a></h5><ul><li><a href="https://ocw.mit.edu/search/?q=Prof.+Paul+Liang">Prof. Paul Liang</a></li></ul><h5 id=departments>Departments<a hidden class=anchor aria-hidden=true href=#departments>#</a></h5><ul><li><a href="https://ocw.mit.edu/search/?d=Media+Arts+and+Sciences">Media Arts and Sciences</a></li></ul><h1 id=content>Content<a hidden class=anchor aria-hidden=true href=#content>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Multisensory intelligence: Creating human-AI symbiosis across scales and sensory mediums to enhance productivity, creativity, and wellbeing.</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/6efcb9a36aa9717f5385c1ae0f9d7441.png></p><h3 id=course-overview>Course Overview<a hidden class=anchor aria-hidden=true href=#course-overview>#</a></h3><ol><li>AI for new modalities: data, modeling, evaluation, deployment</li><li>Multimodal AI: connecting multiple different data sources</li></ol><h3 id=learning-objectives>Learning Objectives<a hidden class=anchor aria-hidden=true href=#learning-objectives>#</a></h3><ol><li>Study recent technical achievements in AI research</li><li>Improve critical and creative thinking skills</li><li>Understand future research challenges in AI</li><li>Explore and implement new research ideas in AI</li></ol><h3 id=preferred-pre-requisites>Preferred Pre-requisites<a hidden class=anchor aria-hidden=true href=#preferred-pre-requisites>#</a></h3><ol><li>Some knowledge of programming (ideally in Python)</li><li>Some basic understanding of modern AI capabilities & limitations</li><li>Bring external (non-AI) domain knowledge about your problem</li><li>Bonus: worked on AI for some modality</li></ol><h3 id=research-projects-on-new-modalities>Research Projects on New Modalities<a hidden class=anchor aria-hidden=true href=#research-projects-on-new-modalities>#</a></h3><p>Motivation: Many tasks of real-world impact go beyond image and text.</p><p>Challenges:</p><ul><li>AI with non-deep-learning effective modalities (e.g., tabular, time-series)</li><li>Multimodal deep learning + time-series analysis + tabular models</li><li>AI for physiological sensing, IoT sensing in cities, climate and environment sensing</li><li>Smell, taste, art, music, tangible and embodied systems</li></ul><p>Potential models and dataset to start with</p><ul><li>Brain EEG Signal: <a href=https://arxiv.org/abs/2306.16934>https://arxiv.org/abs/2306.16934</a></li><li>Speech: <a href=https://arxiv.org/pdf/2310.02050.pdf>https://arxiv.org/pdf/2310.02050.pdf</a></li><li>Facial Motion: <a href=https://arxiv.org/abs/2308.10897>https://arxiv.org/abs/2308.10897</a></li><li>Tactile: <a href=https://arxiv.org/pdf/2204.00117.pdf>https://arxiv.org/pdf/2204.00117.pdf</a></li></ul><h3 id=research-projects-on-ai-reasoning>Research Projects on AI Reasoning<a hidden class=anchor aria-hidden=true href=#research-projects-on-ai-reasoning>#</a></h3><p>Motivation: Robust, reliable, interpretable reasoning in (multimodal) LLMs.</p><p>Challenges:</p><ul><li>Fine-grained and compositional reasoning</li><li>Neuro-symbolic reasoning</li><li>Emergent reasoning in foundation models</li></ul><p>Potential models and dataset to start with</p><ul><li>Can LLMs actually reason and plan?</li><li>Code for VQA: CodeVQA: <a href=https://arxiv.org/pdf/2306.05392.pdf>https://arxiv.org/pdf/2306.05392.pdf</a>, VisProg:
<a href=https://prior.allenai.org/projects/visprog>https://prior.allenai.org/projects/visprog</a>, Viper: <a href=https://viper.cs.columbia.edu/>https://viper.cs.columbia.edu/</a></li><li>Cola: <a href="https://openreview.net/pdf?id=kdHpWogtX6Y">https://openreview.net/pdf?id=kdHpWogtX6Y</a></li><li>NLVR2: <a href=https://arxiv.org/abs/1811.00491>https://arxiv.org/abs/1811.00491</a></li><li>Reference games: <a href=https://mcgill-nlp.github.io/imagecode/>https://mcgill-nlp.github.io/imagecode/</a>, <a href=https://github.com/AlabNII/onecommon>https://github.com/AlabNII/onecommon</a>, <a href=https://dmg-photobook.github.io/>https://dmg-photobook.github.io/</a></li></ul><h3 id=research-projects-on-interactive-agents>Research Projects on Interactive Agents<a hidden class=anchor aria-hidden=true href=#research-projects-on-interactive-agents>#</a></h3><p>Motivation: Grounding AI models in the web, computer, or other virtual worlds to help humans with digital tasks.</p><p>Challenges:</p><ul><li>Web visual understanding is quite different from natural image understanding</li><li>Instructions and language grounded in web images, tools, APIs</li><li>Asking for human clarification, human-in-the-loop</li><li>Search over environment and planning</li></ul><p>Potential models and dataset to start with</p><ul><li>WebArena: <a href=https://arxiv.org/pdf/2307.13854.pdf>https://arxiv.org/pdf/2307.13854.pdf</a></li><li>AgentBench: <a href=https://arxiv.org/pdf/2308.03688.pdf>https://arxiv.org/pdf/2308.03688.pdf</a></li><li>ToolFormer: <a href=https://arxiv.org/abs/2302.04761>https://arxiv.org/abs/2302.04761</a></li><li>SeeAct: <a href=https://osu-nlp-group.github.io/SeeAct/>https://osu-nlp-group.github.io/SeeAct/</a></li></ul><h3 id=research-projects-on-embodied-and-tangible-ai>Research Projects on Embodied and Tangible AI<a hidden class=anchor aria-hidden=true href=#research-projects-on-embodied-and-tangible-ai>#</a></h3><p>Motivation: Building tangible and embodied AI systems that help humans in physical tasks.</p><p>Challenges:</p><ul><li>Perception, reasoning, and interaction</li><li>Connecting sensing and actuation</li><li>Efficient models that can run on hardware</li><li>Understanding influence of actions on the world (world model)</li></ul><p>Potential models and dataset to start with</p><ul><li>Virtual Home: <a href=http://virtual-home.org/paper/virtualhome.pdf>http://virtual-home.org/paper/virtualhome.pdf</a></li><li>Habitat 3.0 <a href=https://ai.meta.com/static-resource/habitat3>https://ai.meta.com/static-resource/habitat3</a></li><li>RoboThor: <a href=https://ai2thor.allenai.org/robothor>https://ai2thor.allenai.org/robothor</a></li><li>LangSuite-E: <a href=https://github.com/bigai-nlco/langsuite>https://github.com/bigai-nlco/langsuite</a></li><li>Language models and world models: <a href=https://arxiv.org/pdf/2305.10626.pdf>https://arxiv.org/pdf/2305.10626.pdf</a></li></ul><h3 id=research-projects-on-socially-intelligent-ai>Research Projects on Socially Intelligent AI<a hidden class=anchor aria-hidden=true href=#research-projects-on-socially-intelligent-ai>#</a></h3><p>Motivation: Building AI that can understand and interact with humans in social situations.</p><p>Challenges:</p><ul><li>Social interaction, reasoning, and commonsense.</li><li>Building social relationships over months and years.</li><li>Theory-of-Mind and multi-party social interactions.</li></ul><p>Potential models and dataset to start with</p><ul><li>Multimodal WereWolf: <a href=https://persuasion-deductiongame.socialai-data.org/>https://persuasion-deductiongame.socialai-data.org/</a></li><li>Ego4D: <a href=https://arxiv.org/abs/2110.07058>https://arxiv.org/abs/2110.07058</a></li><li>MMToM-QA: <a href="https://openreview.net/pdf?id=jbLM1yvxaL">https://openreview.net/pdf?id=jbLM1yvxaL</a></li><li>11866 Artificial Social Intelligence: <a href=https://cmu-multicomp-lab.github.io/asi-course/spring2023/>https://cmu-multicomp-lab.github.io/asi-course/spring2023/</a></li></ul><h3 id=research-projects-on-human-ai-interaction>Research Projects on Human-AI Interaction<a hidden class=anchor aria-hidden=true href=#research-projects-on-human-ai-interaction>#</a></h3><p>Motivation: What is the right medium for human-AI interaction? How can we really trust AI? How do we enable collaboration and synergy?</p><p>Challenges:</p><ul><li>Modeling and conveying model uncertainty ‚Äì text input uncertainty, visual uncertainty,
multimodal uncertainty? cross-modal interaction uncertainty?</li><li>Asking for human clarification, human-in-the-loop, types of human feedback and ways to learn from human feedback through all modalities.</li><li>New mediums to interact with AI. New tasks beyond imitating humans, leading to collaboration.</li></ul><p>Potential models and dataset to start with</p><ul><li>MMHal-Bench: <a href=https://arxiv.org/pdf/2309.14525.pdf>https://arxiv.org/pdf/2309.14525.pdf</a> aligning multimodal LLMs</li><li>HACL: <a href=https://arxiv.org/pdf/2312.06968.pdf>https://arxiv.org/pdf/2312.06968.pdf</a> hallucination + LLM</li></ul><h3 id=research-projects-on-ethics-and-safety>Research Projects on Ethics and Safety<a hidden class=anchor aria-hidden=true href=#research-projects-on-ethics-and-safety>#</a></h3><p>Motivation: Large AI models are can emit unsafe text content, generate or retrieve biased images.</p><p>Challenges:</p><ul><li>Taxonomizing types of biases: text, vision, audio, generation, etc.</li><li>Tracing biases to pretraining data, seeing how bias can be amplified during training, fine-tuning.</li><li>New ways of mitigating biases and aligning to human preferences.</li></ul><p>Potential models and dataset to start with</p><ul><li>Many works on fairness in LLMs -> how to extend to multimodal?</li><li>Mitigating bias in text generation, image-captioning, image generation</li></ul><h3 id=introduction-to-ai-and-ai-research>Introduction to AI and AI research<a hidden class=anchor aria-hidden=true href=#introduction-to-ai-and-ai-research>#</a></h3><ul><li>Introduction to AI and AI research</li><li>Generating ideas, reading and writing papers, AI experimentation</li></ul><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/83653b7e2311d55d157814aea71a24bc.png></p><p>How Do We Get Research Ideas?</p><p>Turn a concrete understanding of existing research&rsquo;s failings to a higher-level experimental question.
‚Ä¢ Bottom-up discovery of research ideas
‚Ä¢ Great tool for incremental progress, but may preclude larger leaps</p><p>Move from a higher-level question to a lower-level concrete testing of that question.</p><p>‚Ä¢ Top-down design of research ideas
‚Ä¢ Favors bigger ideas, but can be disconnected from reality</p><p>Beware &ldquo;Does X Make Y Better?&rdquo; &ldquo;Yes&rdquo;</p><p>The above question/hypothesis is natural, but indirect</p><ul><li>If the answer is &ldquo;no&rdquo; after your experiments, how do you tell what&rsquo;s going wrong?</li></ul><p>Usually you have an intuition about why X will make Y better (not just random)</p><p>Can you think of other research questions/ hypotheses that confirm/falsify these assumptions</p><p>How to do Literature Review and Read a Paper?</p><ol><li>Google scholar</li><li>Papers with code, Github, Huggingface</li><li>Recent conference proceedings</li><li>Blog posts</li><li>Survey papers, tutorials, courses</li></ol><p>Testing Research Ideas</p><ol><li>Gather and process dataset, visualize data, gather labels, do data splits.</li><li>Implement the most simple pipeline and get it working. -> Pipeline = data loading + basic model + eval function + loss/visualization/deployment</li><li>Change one component of the model at a time, repeat x10 (top-down or bottom-up).</li><li>Find what works best, and exploit.</li><li>Scale up experiments, repeat across multiple datasets.</li><li>Careful ablation studies.</li><li>Qualitative comparisons and visualizations.</li><li>Repeat until successful.</li></ol><p>How to Write a Paper</p><ol><li>Prepare a 15min talk (with figures, examples, tables, etc.)</li><li>Convert the talk into a paper.</li></ol><p>More resources</p><ul><li><a href=https://github.com/pliang279/awesome-phd-advice>https://github.com/pliang279/awesome-phd-advice</a></li><li><a href=https://github.com/jbhuang0604/awesome-tips>https://github.com/jbhuang0604/awesome-tips</a></li><li><a href=https://www.cs197.seas.harvard.edu/>https://www.cs197.seas.harvard.edu/</a></li><li><a href=https://medium.com/spotprobe/the-hexagon-of-ideas-02e5b770d75e>https://medium.com/spotprobe/the-hexagon-of-ideas-02e5b770d75e</a></li></ul><h2 id=module-1-foundations-of-ai>Module 1: Foundations of AI<a hidden class=anchor aria-hidden=true href=#module-1-foundations-of-ai>#</a></h2><h3 id=data-structure-and-information>Data, structure, and information<a hidden class=anchor aria-hidden=true href=#data-structure-and-information>#</a></h3><p>Lecture Outline:</p><ol><li>Vision, language, audio, sensing, set, graph modalities</li><li>Modality profile</li><li>Types of data and labels</li><li>Common learning objectives and generalization</li></ol><p>Most of AI is about learning abstractions, or representations, from data.</p><p>Modality Profile:</p><ol><li>Element representations: Discrete, continuous, granularity</li><li>Element distributions: Density, frequency</li><li>Structure: Temporal, spatial, latent, explicit</li><li>Information: Abstraction, entropy</li><li>Noise: Uncertainty, noise, missing data</li><li>Relevance: Task, context dependence</li></ol><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/c7fd5d109e0541537796199d9b51db7b.png></p><p>Summary: How To Data</p><ol><li>Decide how much data to collect, and how much to label (costs and time)</li><li>Clean data: normalize/standardize, find noisy data, anomaly/outlier detection</li><li>Visualize data: plot, dimensionality reduction (PCA, t-sne), cluster analysis</li><li>Decide on evaluation metric (proxy + real, quantitative and qualitative)</li><li>Choose model class and learning algorithm (more next lecture)</li></ol><h4 id=huggingface-tutorial>Huggingface Tutorial<a hidden class=anchor aria-hidden=true href=#huggingface-tutorial>#</a></h4><ul><li>‚ÄúHuggingface‚Äù is a set of multiple packages<ul><li>transformers: Provides API to initialize large pretrained models</li><li>datasets: Provides easy way to download datasets</li></ul></li><li>Not from Huggingface but often used together<ul><li>bitsandbytes: Provides functions to quantize large models</li><li>flash-attn: Allows the model to run faster with less memory</li></ul></li><li>Some terms to keep in mind<ul><li>LoRA: Adapter to train large models with less memory</li><li>Bfloat16: Robust half precision representation often used to save memory</li></ul></li></ul><h4 id=the-recipe>The Recipe<a hidden class=anchor aria-hidden=true href=#the-recipe>#</a></h4><ul><li>Become one with the Data</li><li>Set up end-to-end skeleton and get dumb baselines</li><li>Overfit to diagnose errors</li><li>Regularize for better generalization
‚óè Add more real data ‚Äì the best way to reduce overfitting<br>‚óè Use data augmentation and pretraining<br>‚óè Reduce input dimensions and model size<br>‚óè Techniques: Dropout, weight decay, early stopping</li><li>Tune hyperparameters
‚óè Prefer random search over grid search<br>‚óè Use Bayesian optimization tools when available<br>‚óè Don‚Äôt overcomplicate ‚Äì start with simple models</li><li>Squeeze out final improvements</li></ul><h4 id=how-to-design-ml-models-for-new-data>How to Design ML Models for New Data<a hidden class=anchor aria-hidden=true href=#how-to-design-ml-models-for-new-data>#</a></h4><ul><li>Look at the data first</li><li>For simple, low dimensional data, start with simple models (SVM, Random Forest, Shallow MLP/CNN)</li><li>For vision/language data, try pretrained model</li><li>Start simple, then add complexity. Simple ones can be used as baselines.</li></ul><h4 id=how-to-debug-your-model>How to Debug Your Model<a hidden class=anchor aria-hidden=true href=#how-to-debug-your-model>#</a></h4><ul><li>Look at the data first. Is the input data & label correct?<ul><li>Ensure no data leakage;</li></ul></li><li>Look at the outputs. Is model only predicting one label?<ul><li>Label imbalance: Data Augmentation; loss scaling</li></ul></li><li>Look at the training loss<ul><li>Loss is nan: Inspect weights and inputs for NaN values. Make sure weights are initialized. LLM: Use bfloat16 instead of float16.</li><li>Loss not changing: Model underfitting. Increase learning rate; decrease weight decay; Add more complexity; Use better optimizer*.</li></ul></li><li>Look at Loss (Continued)<ul><li>Loss highly varied/increasing: Decrease learning rate; Gradient Clipping; Use better Optimizers</li></ul></li><li>Look at train vs val accuracy (or any other metrics)<ul><li>Train &#187; Val: Model overfitting. More weight decay, reduce model complexity, data augmentation, get more data</li><li>Train ‚âà Val ‚âà 100%: Check for data leakage</li></ul></li></ul><p>Personal tip: I recommend trying second order optimizers from packages like Heavyball</p><h3 id=common-model-architectures>Common model architectures<a hidden class=anchor aria-hidden=true href=#common-model-architectures>#</a></h3><p>Lecture Outline</p><ol><li>A unifying paradigm of model architectures</li><li>Temporal sequence models</li><li>Spatial convolution models</li><li>Models for sets and graphs</li></ol><p>Summary: How To Model</p><ol><li>Decide how much data to collect, and how much to label (costs and time)</li><li>Clean data: normalize/standardize, find noisy data, anomaly/outlier detection</li><li>Visualize data: plot, dimensionality reduction (PCA, t-sne), cluster analysis</li><li>Decide on evaluation metric (proxy + real, quantitative and qualitative)</li><li>Choose modeling paradigm - domain-specific vs general-purpose</li><li>Figure out base elements and their representation</li><li>Figure out data invariances & equivariances (+other parts of modality profile)</li><li>Iterate between data collection, model design, model training, hyperparameter tuning etc. until satisfied.</li></ol><h3 id=discussion-1-learning-and-generalization><strong>Discussion 1: Learning and generalization</strong><a hidden class=anchor aria-hidden=true href=#discussion-1-learning-and-generalization>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2410.09649>Learning the Bitter Lesson</a></li><li><a href=https://arxiv.org/pdf/2303.06173>Unifying Grokking and Double Descent</a></li><li><a href=https://arxiv.org/pdf/2209.01610>Generalization in Neural Networks</a></li><li><a href=https://arxiv.org/pdf/2306.11644>Textbooks are all you Need</a></li><li><a href=https://arxiv.org/pdf/2207.07528>A Conceptual Pipeline for Machine Learning</a></li></ul><h2 id=module-2-foundations-of-multimodal-ai>Module 2: Foundations of multimodal AI<a hidden class=anchor aria-hidden=true href=#module-2-foundations-of-multimodal-ai>#</a></h2><h3 id=multimodal-connections-and-alignment>Multimodal connections and alignment<a hidden class=anchor aria-hidden=true href=#multimodal-connections-and-alignment>#</a></h3><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/5b5cd7675f62426f70dfe7985f9155ff.png></p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/ac168ca938d2cdf2ea8590f72482cb39.png></p><p>Modality refers to the way in which something expressed or perceived.</p><p>A research-oriented definition‚Ä¶ Multimodal is the science of heterogeneous and interconnected(Connected + Interacting) data.</p><p>Heterogeneous Modalities: Information in different modalities shows diverse qualities, structures, & representations.</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/5b27d830a96c3a294f9b2e139e53f893.png></p><p>Challenge 1: Representation</p><p>Definition: Learning representations that reflect cross-modal interactions between individual elements, across different modalities This is a core building block for most multimodal modeling problems!</p><p>Challenge 2: Alignment</p><p>Definition: Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure.</p><p>Sub-challenges:</p><ul><li>Discrete connections: Explicit alignment (e.g., grounding)</li><li>Contextualized representation: Implicit alignment + representation</li><li>Continuous alignment: Granularity of individual elements</li></ul><p>Challenge 2a: Discrete Alignment</p><p>Definition: Identify and model connections between elements of multiple modalities</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/207230008dd6a8c59c35717e86cea6bd.png></p><p>Challenge 2b: Continuous Alignment</p><p>Definition: Model alignment between modalities with continuous signals and no explicit elements</p><p>Challenge 3: Reasoning</p><p>Definition: Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure.</p><p>Challenge 4: Generation</p><p>Definition: Learning a generative process to produce raw modalities that reflects cross-m</p><p>Challenge 5: Transference</p><p>Definition: Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resources.</p><p>Challenge 6: Quantification</p><p>Definition: Empirical and theoretical study to better understand heterogeneity, cross-modal interactions, and the multimodal learning process.</p><h3 id=discussion-2-modern-ai-architectures><strong>Discussion 2: Modern AI architectures</strong><a hidden class=anchor aria-hidden=true href=#discussion-2-modern-ai-architectures>#</a></h3><ul><li><a href=https://arxiv.org/abs/2301.03728>Scaling Laws for Generative Mixed-Modal Models</a></li><li><a href=https://arxiv.org/abs/2404.07965>Not All Tokens Are What You Need for Pretraining</a></li><li><a href=https://arxiv.org/abs/2209.06794>PaLI: A Jointly-Scaled Multilingual Language-Image Model</a></li><li><a href=https://arxiv.org/abs/2405.17927>The Evolution of Multimodal Model Architectures</a></li><li><a href=https://arxiv.org/abs/2010.11929>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li><li><a href=https://arxiv.org/abs/2201.03545>A ConvNet for the 2020s</a></li><li><a href=https://arxiv.org/abs/1706.02216>Inductive Representation Learning on Large Graphs</a></li><li><a href=https://arxiv.org/abs/1811.01900>Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs</a></li></ul><h3 id=multimodal-interactions-and-fusion>Multimodal interactions and fusion<a hidden class=anchor aria-hidden=true href=#multimodal-interactions-and-fusion>#</a></h3><p>Visual-and-Language Transformer (ViLT) (‚âà BERT + ViT)</p><p>ALBEF: Align Before Fusion 32 (‚âà BERT + ViT + CLIP-ish)</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/6e48bfc64f296152fa3f7439320bf0be.png></p><h3 id=discussion-3-multimodal-alignment><strong>Discussion 3: Multimodal alignment</strong><a hidden class=anchor aria-hidden=true href=#discussion-3-multimodal-alignment>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2405.07987>The Platonic Representation Hypothesis</a></li><li><a href=https://arxiv.org/abs/2005.10243>What Makes for Good Views for Contrastive Learning?</a></li><li><a href=https://arxiv.org/pdf/2502.16282>Understanding the Emergence of Multimodal Representation Alignment</a></li><li><a href=https://arxiv.org/abs/2410.23179>Does equivariance matter at scale?</a></li><li><a href=https://arxiv.org/pdf/2103.00020>Learning Transferable Visual Models From Natural Language Supervision?</a></li><li><a href=https://arxiv.org/pdf/2104.14294>Emerging Properties in Self-Supervised Vision Transformers</a></li><li><a href=https://arxiv.org/abs/2209.03430>Foundations & trends in multimodal machine learning - Principles, challenges, and open questions</a></li></ul><h3 id=cross-modal-transfer>Cross-modal transfer<a hidden class=anchor aria-hidden=true href=#cross-modal-transfer>#</a></h3><p>Transference</p><p>Definition: Transfer knowledge between modalities, usually to help the primary modality which may be noisy or with limited resources</p><p>Part 1: Transfer via Pretrained Models</p><p>Definition: Transferring knowledge from large-scale pretrained models to downstream tasks involving</p><p>Part 2: Co-learning</p><p>Definition: Transferring information from secondary to primary modality by sharing representation spaces between both modalities.</p><p>Co-learning via Alignment</p><p>Definition: Transferring information from secondary to primary modality by sharing representation spaces between modalities</p><p>Representation alignment: word embedding space for zero-shot visual classification</p><p>Co-learning via Translation</p><p>Definition: Transferring information from secondary to primary modality by using the secondary modality as a generation target.</p><p>Part 3: Model Induction</p><p>Model Induction ùë¶1 ùë¶2 Definition: Keeping individual unimodal models separate but inducing common behavior across separate models.</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/a93c027b0a8f19412a197d2f0249652c.png></p><h3 id=discussion-4-multimodal-interactions><strong>Discussion 4: Multimodal interactions</strong><a hidden class=anchor aria-hidden=true href=#discussion-4-multimodal-interactions>#</a></h3><ul><li><a href=https://www.sciencedirect.com/science/article/pii/S0167865513002584>Multimodal interaction: A review</a></li><li><a href=https://arxiv.org/abs/2302.12247>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</a></li><li><a href=https://aclanthology.org/2020.emnlp-main.62/>Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!</a></li><li><a href=https://arxiv.org/abs/2306.14824>Kosmos-2: Grounding Multimodal Large Language Models to the World</a></li><li><a href=https://arxiv.org/abs/2405.09818>Chameleon: Mixed-modal early-fusion foundation models</a></li><li><a href=https://link.springer.com/chapter/10.1007/978-3-031-73397-0_18>MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training</a></li><li><a href=https://arxiv.org/pdf/2407.21770>MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</a></li></ul><h2 id=module-3-large-models-and-modern-ai>Module 3: Large models and modern AI<a hidden class=anchor aria-hidden=true href=#module-3-large-models-and-modern-ai>#</a></h2><h3 id=pre-training-scaling-fine-tuning-llms>Pre-training, scaling, fine-tuning LLMs<a hidden class=anchor aria-hidden=true href=#pre-training-scaling-fine-tuning-llms>#</a></h3><p>Today‚Äôs Agenda</p><ul><li>History of LLMs, RNNs vs Transformer</li><li>Pretraining of LLMs</li><li>Types of Architecture</li><li>Instruction Finetuning & Preference Tuning</li><li>Efficient Training</li><li>Practical Tips</li></ul><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/125d02839e5bf7ef07a5d4f36b2b0ac5.png></p><p>Scaling Laws:</p><ul><li>Bigger model allows models to reach a better performance given sufficient compute</li><li>Over training models getting popular nowadays</li></ul><p>Limitations of RL + Reward Modeling</p><ul><li>Human preferences are unreliable<ul><li>Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth</li><li>This can result in making up facts + hallucinations</li></ul></li><li>Reward Model doesn‚Äôt always reflect humans‚Äô preferences & may have unintended behaviors</li></ul><p>Efficient Training: LoRA / Efficient low rank adaptation</p><ul><li>Training the whole model takes a lot of compute and GPU memory</li><li>Solution: Freeze the model, train a small adapter that updates with a low-rank decomposition</li></ul><p>Efficient Training: Mixture of Experts</p><ul><li>Train multiple parallel networks (experts) simultaneously</li><li>During each forward pass, only activate k experts</li><li>Saves compute & GPU memory</li><li>Deepseek R1: 671B, only 37B activated, performance on par with OpenAI o1-mini</li></ul><p>Efficient Inference: Quantization</p><ul><li>Range Clipping</li><li>Scale & Shift</li><li>Convert to lower bits</li><li>Calibration</li></ul><p>Practical Tips: How to Instruction Finetune an LLM</p><ol><li>Data Preparation: Convert your data to conversation format</li><li>Choosing a good starting point</li><li>Secure compute & Finetune the model</li><li>Evaluation & Deployment</li></ol><h3 id=large-multimodal-models>Large multimodal models<a hidden class=anchor aria-hidden=true href=#large-multimodal-models>#</a></h3><p>Today‚Äôs lecture</p><ol><li>Multimodal foundation models and pre-training</li><li>Adapting LLMs into multimodal LLMs</li><li>From text to multimodal generation</li><li>Latest directions: natively multimodal, multimodal MoE, real-world modalities</li></ol><p>Part 1: Multimodal foundation model representations of text, video, audio</p><p>Part 2: Adapting large language models for multimodal text generation</p><p>Part 3: Enabling text and image generation</p><p>Visual-and-Language Transformer (ViLT)</p><p>Pre-training datasets</p><ul><li>Largest dataset is DataComp. It has 12.8 billion image-text pairs.</li><li>Recent efforts shifted more towards filtering for high quality multimodal data.</li></ul><p>Examples include DFN (2B), COYO (600M), and Obelics (141M)</p><p>Native Multimodal Models</p><ul><li>Background<ul><li>Non-native VLMs: Image encoder paired with frozen trained LLM. The image encoder can either be frozen or trained. Most VLMs now use this structure.</li><li>Native Multimodal Modals: LLMs Trained from scratch with multimodal input<ul><li>Late fusion: Image patches -> Image Encoder -> Linear -> LLM.</li><li>Early fusion: Image patches -> Linear -> LLM (No image encoder!)</li></ul></li></ul></li></ul><p>Scaling Laws for Native Multimodal Models</p><ul><li>Early fusion models hold small advantage on small scales.</li><li>On larger scales, both architectures perform similarly. (We don‚Äôt actually need image encoders!)</li><li>NMMs scale similarly to unimodal LLMs, with slightly varying scaling exponents depending on the target data type and training mixture</li><li>Sparse structure like MOE significantly benefits NMMs at the same inference cost</li><li>In an MOE structure, Modality-aware design (having separate image/text experts) performs worse than modality-agnostic design (unified experts for both image/text tokens)</li></ul><h3 id=discussion-5-large-language-models><strong>Discussion 5: Large language models</strong><a hidden class=anchor aria-hidden=true href=#discussion-5-large-language-models>#</a></h3><ul><li><a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><a href=https://arxiv.org/abs/2312.06635>Gated Linear Attention Transformers with Hardware-Efficient Training</a></li><li><a href=https://arxiv.org/abs/2402.15018>Unintended Impacts of LLM Alignment on Global Representation</a></li><li><a href=https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization>A Visual Guide to Quantization</a></li><li><a href=https://arxiv.org/abs/2106.09685>Scaling Instruction-Finetuned Language Models</a></li></ul><h3 id=modern-generative-ai>Modern generative AI<a hidden class=anchor aria-hidden=true href=#modern-generative-ai>#</a></h3><p>Todays Lecture</p><ol><li>What are Generative Models?</li><li>Current State of the Art (Flow Matching)</li><li>Conditional Generation</li><li>Architectures</li><li>Tips to Train these Models</li></ol><p>¬†### <strong>Discussion 6: Large multimodal models</strong></p><ul><li><a href=https://arxiv.org/pdf/2303.16199>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></li><li><a href=https://arxiv.org/pdf/2403.14520>Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference</a></li><li><a href=https://arxiv.org/abs/2401.06395>ModaVerse: Efficiently Transforming Modalities with LLMs</a></li><li><a href=https://arxiv.org/pdf/2411.09439>Spider: Any-to-Many Multimodal LLM</a></li><li><a href=https://arxiv.org/pdf/2402.05935>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2404.16821>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</a></li><li><a href=https://arxiv.org/pdf/2309.05519>NExT-GPT: Any-to-Any Multimodal LLM</a></li><li><a href=https://arxiv.org/pdf/2404.08347>Learning to rebalance multi-modal optimization by adaptively masking subnetworks</a></li></ul><h2 id=module-4-interactive-ai>Module 4: Interactive AI<a hidden class=anchor aria-hidden=true href=#module-4-interactive-ai>#</a></h2><h3 id=discussion-7-generative-ai><strong>Discussion 7: Generative AI</strong><a hidden class=anchor aria-hidden=true href=#discussion-7-generative-ai>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2502.09992>Large Language Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2402.01103>Compositional Generative Modeling: A Single Model is Not All You Need</a></li><li><a href=https://arxiv.org/abs/2210.02747>Flow Matching for Generative Modeling</a></li><li><a href=https://arxiv.org/abs/2412.06264>Flow Matching Guide and Code</a></li><li><a href=https://arxiv.org/abs/2504.01338>FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-to-Motion Generation</a></li><li><a href=https://arxiv.org/abs/2504.13535>MusFlow: Multimodal Music Generation via Conditional Flow Matching</a></li><li><a href=https://arxiv.org/abs/2311.07625>Unraveling the Connections Between Flow Matching and Diffusion Probabilistic Models</a></li><li><a href=https://arxiv.org/abs/2412.11024>Exploring Diffusion and Flow Matching Under Generator Matching</a></li></ul><h3 id=multi-step-reasoning>Multi-step reasoning<a hidden class=anchor aria-hidden=true href=#multi-step-reasoning>#</a></h3><p>Today‚Äôs lecture</p><ol><li>Multimodal reasoning: Solving hard problems by breaking them down into step-by-step reasoning steps in multiple modalities</li><li>AI agents</li><li>Human-AI interaction</li><li>Ethics and safety</li></ol><p>Models: Multimodal fusion and generation<br>Data: Hard challenges + human reasoning steps<br>Training: Reinforcement learning for emergent reasoning<br>Human: Trustworthy, safe, controllable</p><p>Multimodal Reasoning</p><p>Part 1: Multimodal foundation model representations of text, video, audio</p><p>Part 2: Adapting large language models for multimodal text generation</p><p>Part 3: Enabling text and image generation</p><p>Part 4: Human-AI interaction</p><h3 id=interactive-and-embodied-ai>Interactive and embodied AI<a hidden class=anchor aria-hidden=true href=#interactive-and-embodied-ai>#</a></h3><p>Today‚Äôs lecture</p><ol><li>Basics of reinforcement learning</li><li>Modern RL for LLM alignment and reasoning</li><li>Interactive LLM agents</li></ol><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/c7e007c0be2df306bfa71746afbfdd2b.png></p><p>Tips and Training for Reinforcement Learning</p><ol><li>Sanity Check with Fixed Policy</li><li>Monitor KL Divergence (in PPO-like algorithms)</li><li>Plot Entropy Over Time</li><li>Use Greedy Rollouts for Evaluation</li><li>Debug Value Function Separately: Visualize predicted vs. actual return</li><li>Gradient Norm Clipping is Crucial</li><li>Check Advantage Distribution</li><li>Train on a Frozen Replay Buffer</li><li>Use Curriculum Learning: Gradually increase task difficulty or reward sparsity</li><li>Watch for Mode Collapse in MoE or Multi-Head Policies</li></ol><h3 id=human-ai-interaction-and-safety>Human-AI interaction and safety<a hidden class=anchor aria-hidden=true href=#human-ai-interaction-and-safety>#</a></h3><p>Interactive Agents</p><p>Multisensory agents for the web and digital automation</p><p>Example task: Purchase a set of earphones with at least 4.5 stars in rating and ship it to me.</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/964f2f63b3842e0b7084a8a55bea5e84.png></p><p>Embodied Agents</p><p>Generate precise robotics control directly via trained vision language models.</p><p>Human-AI interaction</p><ol><li>What medium(s) is most intuitive for human-AI interaction?<ul><li>especially beyond language prompting 1</li></ul></li><li>What new technical challenges in AI have to be solved for human-AI interaction?<ul><li>quantification</li></ul></li><li>What new opportunities arise when integrating AI with the human experience?<ul><li>productivity, creativity, wellbeing</li></ul></li></ol><p>Quantification</p><p>Definition: Empirical and theoretical studies to better understand model
shortcomings and predict and control model behavior.</p><p>Quantification - Safety</p><p>Easy to generate biased and dangerous content with language models!</p><p>But there exist ways to ‚Äòjailbreak‚Äô the safety measures in aligned LLMs</p><p><img loading=lazy src=https://setsailtowardstianhan.ip-ddns.com/blog/86da7377b1b2cb82abeb80edaa652400.png></p><h2 id=readings>Readings<a hidden class=anchor aria-hidden=true href=#readings>#</a></h2><ul><li><p><a href=https://arxiv.org/abs/2209.03430>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</a></p></li><li><p><a href=https://arxiv.org/abs/1705.09406>Multimodal Machine Learning: A Survey and Taxonomy</a></p></li><li><p><a href=https://arxiv.org/abs/1206.5538>Representation Learning: A Review and New Perspectives</a></p></li><li><p><a href=https://www.science.org/doi/abs/10.1126/science.aaa8415>Machine learning: Trends, Perspectives, and Prospects</a></p></li><li><p><a href=https://arxiv.org/abs/1206.5538>Representation Learning: A Review and New Perspectives</a></p></li><li><p><a href=https://arxiv.org/abs/2209.03430>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</a></p></li><li><p><a href=https://arxiv.org/abs/2104.13478>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</a></p></li><li><p><a href=https://karpathy.github.io/2019/04/25/recipe/>A Recipe for Training Neural Networks</a></p></li><li><p><a href="https://colab.research.google.com/drive/1EDsjYRrAiujUew0GRJ_hyVoNMsSDnlnx?usp=sharing">Fine-tuning a Code LLM on Custom Code on a single GPU</a></p></li><li><p><a href="https://colab.research.google.com/drive/1SoTu6gvYcLNDqPwNPTWmsSYF-l-UfHPx?usp=sharing">MAS.S60 Pytorch Introduction</a></p></li><li><p><a href=https://arxiv.org/abs/2104.13478>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</a></p></li><li><p><a href=https://arxiv.org/abs/2010.11929>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p></li><li><p><a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a></p></li><li><p><a href=https://arxiv.org/abs/1409.0473>Neural Machine Translation by Jointly Learning to Align and Translate</a></p></li><li><p><a href=https://arxiv.org/abs/1703.06114>Deep Sets</a></p></li><li><p><a href=https://arxiv.org/abs/1710.10903>Graph Attention Networks</a></p></li><li><p><a href=https://arxiv.org/pdf/2410.09649>Learning the Bitter Lesson</a></p></li><li><p><a href=https://arxiv.org/pdf/2303.06173>Unifying Grokking and Double Descent</a></p></li><li><p><a href=https://arxiv.org/pdf/2209.01610>Generalization in Neural Networks</a></p></li><li><p><a href=https://arxiv.org/pdf/2306.11644>Textbooks are all you Need</a></p></li><li><p><a href=https://arxiv.org/pdf/2207.07528>A Conceptual Pipeline for Machine Learning</a></p></li><li><p><a href=https://arxiv.org/abs/2209.03430>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</a></p></li><li><p><a href=https://arxiv.org/abs/2005.10243>What Makes for Good Views for Contrastive Learning?</a></p></li><li><p><a href=https://link.springer.com/article/10.1007/s13735-019-00187-6>Characterization and classification of semantic image-text relations</a></p></li><li><p><a href=https://arxiv.org/abs/2210.01936>When and why vision-language models behave like bags-of-words, and what to do about it?</a></p></li><li><p><a href=https://arxiv.org/abs/2301.03728>Scaling Laws for Generative Mixed-Modal Models</a></p></li><li><p><a href=https://arxiv.org/abs/2404.07965>Not All Tokens Are What You Need for Pretraining</a></p></li><li><p><a href=https://arxiv.org/abs/2209.06794>PaLI: A Jointly-Scaled Multilingual Language-Image Model</a></p></li><li><p><a href=https://arxiv.org/abs/2405.17927>The Evolution of Multimodal Model Architectures</a></p></li><li><p><a href=https://arxiv.org/abs/2010.11929>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p></li><li><p><a href=https://arxiv.org/abs/2201.03545>A ConvNet for the 2020s</a></p></li><li><p><a href=https://arxiv.org/abs/1706.02216>Inductive Representation Learning on Large Graphs</a></p></li><li><p><a href=https://arxiv.org/abs/1811.01900>Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs</a></p></li><li><p><a href=https://dl.acm.org/doi/pdf/10.1145/319382.319398>Ten Myths of Multimodal Interaction</a></p></li><li><p><a href=https://www.sciencedirect.com/science/article/pii/S0167865513002584>Multimodal interaction: A review</a></p></li><li><p><a href=https://arxiv.org/abs/2302.12247>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</a></p></li><li><p><a href=https://aclanthology.org/2020.emnlp-main.62/>Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!</a></p></li><li><p><a href=https://arxiv.org/pdf/2405.07987>The Platonic Representation Hypothesis</a></p></li><li><p><a href=https://arxiv.org/abs/2005.10243>What Makes for Good Views for Contrastive Learning?</a></p></li><li><p><a href=https://arxiv.org/pdf/2502.16282>Understanding the Emergence of Multimodal Representation Alignment</a></p></li><li><p><a href=https://arxiv.org/abs/2410.23179>Does equivariance matter at scale?</a></p></li><li><p><a href=https://arxiv.org/pdf/2103.00020>Learning Transferable Visual Models From Natural Language Supervision?</a></p></li><li><p><a href=https://arxiv.org/pdf/2104.14294>Emerging Properties in Self-Supervised Vision Transformers</a></p></li><li><p><a href=https://arxiv.org/abs/2209.03430>Foundations & trends in multimodal machine learning - Principles, challenges, and open questions</a></p></li><li><p><a href=https://arxiv.org/abs/2303.00915v3>LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</a></p></li><li><p><a href=https://arxiv.org/abs/2309.11499>DreamLLM: Synergistic Multimodal Comprehension and Creation</a></p></li><li><p><a href=https://arxiv.org/abs/2303.03378>PaLM-E: An Embodied Multimodal Language Model</a></p></li><li><p><a href=https://www.sciencedirect.com/science/article/pii/S0167865513002584>Multimodal interaction: A review</a></p></li><li><p><a href=https://arxiv.org/abs/2302.12247>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</a></p></li><li><p><a href=https://aclanthology.org/2020.emnlp-main.62/>Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!</a></p></li><li><p><a href=https://arxiv.org/abs/2306.14824>Kosmos-2: Grounding Multimodal Large Language Models to the World</a></p></li><li><p><a href=https://arxiv.org/abs/2405.09818>Chameleon: Mixed-modal early-fusion foundation models</a></p></li><li><p><a href=https://link.springer.com/chapter/10.1007/978-3-031-73397-0_18>MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training</a></p></li><li><p><a href=https://arxiv.org/pdf/2407.21770>MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</a></p></li><li><p><a href=https://arxiv.org/abs/2203.15556>Training Compute-Optimal Large Language Models</a></p></li><li><p><a href=https://arxiv.org/abs/2204.07705>Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks</a></p></li><li><p><a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></p></li><li><p><a href=https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts>A Visual Guide to Mixture of Experts (MoE)</a></p></li><li><p><a href=https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization>A Visual Guide to Quantization</a></p></li><li><p><a href=https://arxiv.org/abs/2310.03744>Improved Baselines with Visual Instruction Tuning</a></p></li><li><p><a href=https://arxiv.org/abs/2302.12247>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</a></p></li><li><p><a href=https://arxiv.org/abs/1906.00295>Multimodal Transformer for Unaligned Multimodal Language Sequences</a></p></li><li><p><a href=https://arxiv.org/abs/2111.06377>Masked Autoencoders Are Scalable Vision Learners</a></p></li><li><p><a href=https://arxiv.org/abs/2504.07951>Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models</a></p></li><li><p><a href=https://arxiv.org/abs/2504.06256>Transfer between Modalities with MetaQueries</a></p></li><li><p><a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></p></li><li><p><a href=https://arxiv.org/abs/2312.06635>Gated Linear Attention Transformers with Hardware-Efficient Training</a></p></li><li><p><a href=https://arxiv.org/abs/2402.15018>Unintended Impacts of LLM Alignment on Global Representation</a></p></li><li><p><a href=https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization>A Visual Guide to Quantization</a></p></li><li><p><a href=https://arxiv.org/abs/2106.09685>Scaling Instruction-Finetuned Language Models</a></p></li><li><p><a href=https://arxiv.org/abs/2212.09748>Scalable Diffusion Models with Transformers</a></p></li><li><p><a href=https://arxiv.org/abs/2307.08698>Flow Matching in Latent Space</a></p></li><li><p><a href=https://arxiv.org/abs/2403.03206>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></p></li><li><p><a href=https://arxiv.org/abs/2410.13720>Movie Gen: A Cast of Media Foundation Models</a></p></li><li><p><a href=https://arxiv.org/pdf/2303.16199>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></p></li><li><p><a href=https://arxiv.org/pdf/2403.14520>Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference</a></p></li><li><p><a href=https://arxiv.org/abs/2401.06395>ModaVerse: Efficiently Transforming Modalities with LLMs</a></p></li><li><p><a href=https://arxiv.org/pdf/2411.09439>Spider: Any-to-Many Multimodal LLM</a></p></li><li><p><a href=https://arxiv.org/pdf/2402.05935>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</a></p></li><li><p><a href=https://arxiv.org/pdf/2404.16821>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</a></p></li><li><p><a href=https://arxiv.org/pdf/2309.05519>NExT-GPT: Any-to-Any Multimodal LLM</a></p></li><li><p><a href=https://arxiv.org/pdf/2404.08347>Learning to rebalance multi-modal optimization by adaptively masking subnetworks</a></p></li><li><p><a href=https://arxiv.org/pdf/2502.09992>Large Language Diffusion Models</a></p></li><li><p><a href=https://arxiv.org/pdf/2402.01103>Compositional Generative Modeling: A Single Model is Not All You Need</a></p></li><li><p><a href=https://arxiv.org/abs/2210.02747>Flow Matching for Generative Modeling</a></p></li><li><p><a href=https://arxiv.org/abs/2412.06264>Flow Matching Guide and Code</a></p></li><li><p><a href=https://arxiv.org/abs/2504.01338>FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-to-Motion Generation</a></p></li><li><p><a href=https://arxiv.org/abs/2504.13535>MusFlow: Multimodal Music Generation via Conditional Flow Matching</a></p></li><li><p><a href=https://arxiv.org/abs/2311.07625>Unraveling the Connections Between Flow Matching and Diffusion Probabilistic Models</a></p></li><li><p><a href=https://arxiv.org/abs/2412.11024>Exploring Diffusion and Flow Matching Under Generator Matching</a></p></li><li><p><a href=https://arxiv.org/abs/1706.03741>Deep reinforcement learning from human preferences</a></p></li><li><p><a href=https://arxiv.org/abs/2501.12948>Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning</a></p></li><li><p><a href=https://openai.com/index/faulty-reward-functions/>Faulty reward functions in the wild</a></p></li><li><p><a href=https://arxiv.org/abs/2305.18290>Direct preference optimization: Your language model is secretly a reward model</a></p></li><li><p><a href=https://arxiv.org/abs/2503.16434>Interactive Sketchpad: A Multimodal Tutoring System for Collaborative, Visual Problem-Solving</a></p></li><li><p><a href=https://arxiv.org/abs/2410.19100>VideoWebArena: Evaluating Multimodal Agents on Video Understanding Web Tasks</a></p></li><li><p><a href=https://arxiv.org/abs/2406.09246>OpenVLA: An Open-Source Vision-Language-Action Model</a></p></li><li><p><a href=https://arxiv.org/abs/2402.11753>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</a></p></li><li><p><a href=https://dl.acm.org/doi/abs/10.1145/3290605.3300233>Guidelines for Human-AI Interaction</a></p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://rosefinch-midsummer.github.io/zh/tags/mit/>MIT</a></li><li><a href=https://rosefinch-midsummer.github.io/zh/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://rosefinch-midsummer.github.io/zh/posts/pastime/%E9%9F%A9%E5%89%A7%E6%90%9E%E7%AC%91%E4%B8%80%E5%AE%B6%E4%BA%BA/><span class=title>¬´ ‰∏ä‰∏ÄÈ†Å</span><br><span>Èü©Ââß„ÄäÊêûÁ¨ë‰∏ÄÂÆ∂‰∫∫„Äã</span>
</a><a class=next href=https://rosefinch-midsummer.github.io/zh/posts/book/%E8%BF%9E%E6%8E%A5%E7%BB%84%E9%80%A0%E5%B0%B1%E7%8B%AC%E4%B8%80%E6%97%A0%E4%BA%8C%E7%9A%84%E4%BD%A0/><span class=title>‰∏ã‰∏ÄÈ†Å ¬ª</span><br><span>„ÄäËøûÊé•ÁªÑÔºöÈÄ†Â∞±Áã¨‰∏ÄÊó†‰∫åÁöÑ‰Ω†„Äã</span></a></nav></footer><div><div class=pagination__title><span class=pagination__title-h style=font-size:20px>ËØÑËÆ∫</span><br></div><div id=tcomment></div><script src=https://utteranc.es/client.js repo=Rosefinch-Midsummer/comments_of_blog issue-term=title theme=github-light crossorigin=anonymous async></script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.body.className.includes("dark")?"github-light":"photon-dark",t={type:"set-theme",theme:e},n=document.querySelector(".utterances-frame");n.contentWindow.postMessage(t,"https://utteranc.es")})</script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://rosefinch-midsummer.github.io/zh/>Â§©Êº¢Â∏ùÂúãÂæ©ËààÈåÑ</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>Êú¨Á´ôÊÄªËÆøÈóÆÈáè<span id=busuanzi_value_site_pv></span>Ê¨°
</span><span id=busuanzi_container_site_uv>Êú¨Á´ôËÆøÂÆ¢Êï∞<span id=busuanzi_value_site_uv></span>‰∫∫Ê¨°</span></div></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Ë§áË£Ω";function s(){t.innerHTML="Â∑≤Ë§áË£ΩÔºÅ",setTimeout(()=>{t.innerHTML="Ë§áË£Ω"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>